{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "![nlp](https://wrm5sysfkg-flywheel.netdna-ssl.com/wp-content/uploads/2019/01/NLP-Technology-in-Healthcare.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An area of computer science and artificial intelligence concerned with the interaction between computers and humans (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "To add context to the above it is fair to say that when performing analysis, a lot of data is numerical, measurements based and quantifiable. Computers are exceptional at dealing with this type of data. But, textual data is a whole other realm of meaning. Humans can tell there is a plethora of information inside of text documents but a computer needs specialised processing in order to 'understand' raw text data. Text data is highly unstructured and can be a mix of multiple languages.\n",
    "\n",
    "NLP is therefore a variety of techniques to create structure from text data. It is an area of active developments and advances in the field are constant. \n",
    "\n",
    "Typical use cases for NLP: \n",
    "- Email classification between spam and legitimate emails\n",
    "- Sentiment Analysis of movie reviews text\n",
    "- Analyzing trends from written feedback forms\n",
    "- Understanding text commands (Siri, Alexa, Google assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "Spacy is an open source NLP library for Python. It is an implementation of common algorithms for effectively and efficiently dealing with language processing problems. For many NLP tasks Spacy only has one implemented method, typically the most efficient algorithm currently available at the time of publishing the library. This means there is often one-way to do things with Spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NLTK, or natural language toolkit is another open source option that is hugely popular. This dates back to 2001 where as Spacy was first published in 2015. This library has a more comprehensive suite of options for achieving particular tasks but that means it also includes options which are not the not efficient implementation, or suboptimal approach to certain problem sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison notes\n",
    "- For many tasks Spacy is faster and more efficient than NLTK at the cost of having less freedom of choice in algorithmic implementations.\n",
    "- Spacy does not include pre-created models for some applications such as sentiment analysis, which is easier to achieve with NLTK.\n",
    "- Approach taken will be to default to Spacy where a use-case exists and fall back to NLTK for problem sets where this library is better or has additional resources and tooling that are not available in Spacy.\n",
    "\n",
    "[NLTK Vs Spacy speed comparison](https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2#:~:text=NLTK%20is%20a%20string%20processing,spaCy%20uses%20object%2Doriented%20approach.&text=As%20we%20can%20see%20below,sentence%20tokenization%2C%20NLTK%20outperforms%20spaCy)\n",
    "\n",
    "## Additional installation instructions\n",
    "- For conda envs: `conda install -c conda-forge spacy`\n",
    "- For language pack additions (English): `python -m spacy download en`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.0 - Spacy Basics\n",
    "\n",
    "- Loading the language libraries\n",
    "- Building a pipeline object\n",
    "- Using tokens\n",
    "- Parts-of-speech tagging\n",
    "- Understanding token attributes \n",
    "\n",
    "Spacy works with a `pipeline object` this takes raw text and automatically performs a series of operations to tag, parseand describe the text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the language pack (or model).\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the output of the assignation to doc above. Note that we have tokenised \n",
    "# each word. It has a POS value and we can see the full descriptor of the types of \n",
    "# words that is for each. \n",
    "\n",
    "def doc_analysis(doc):\n",
    "    print(f\"{'Token':<25}{'POS':>5} {'POS_desc':<8}{'Syntactic_Dependency':>25} {'tag':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<25}{token.pos:>5} {token.pos_:<8}{token.dep_:>25} {token.tag_:<25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                 POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Tesla                  96 PROPN                       nsubj NNP                      \n",
      "is                     87 AUX                           aux VBZ                      \n",
      "looking               100 VERB                         ROOT VBG                      \n",
      "at                     85 ADP                          prep IN                       \n",
      "buying                100 VERB                        pcomp VBG                      \n",
      "U.S.                   96 PROPN                    compound NNP                      \n",
      "startup                92 NOUN                         dobj NN                       \n",
      "for                    85 ADP                          prep IN                       \n",
      "$                      99 SYM                      quantmod $                        \n",
      "6                      93 NUM                      compound CD                       \n",
      "million                93 NUM                          pobj CD                       \n"
     ]
    }
   ],
   "source": [
    "# run the analysis on our test text \n",
    "doc_analysis(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fbcacb36dd0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fbcaca164b0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fbcad1a5d00>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describes the operations series that is performed on a submitted text. \n",
    "# tagger\n",
    "# parser\n",
    "# named entity recogniser\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Tesla isn't looking into startups anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                 POS POS_desc     Syntactic_Dependency\n",
      "---------------------------------------------------------------------------\n",
      "Tesla                  96 PROPN                       nsubj\n",
      "is                     87 AUX                           aux\n",
      "n't                    94 PART                          neg\n",
      "looking               100 VERB                         ROOT\n",
      "into                   85 ADP                          prep\n",
      "startups               92 NOUN                         pobj\n",
      "anymore                86 ADV                        advmod\n",
      ".                      97 PUNCT                       punct\n"
     ]
    }
   ],
   "source": [
    "doc_analysis(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow up on the syntactic dependency details with the official docs: https://spacy.io/usage/linguistic-features#dependency-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Life is what happens to us while we are making other plans\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "life_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# we can see the spacy is smart enough to note differences between types of input\n",
    "print(type(doc3))\n",
    "print(type(life_quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u\"This is the first sentence. This is another sentence. This is the last sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc4.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# traverse each token in the doc sample to test if the word is the \n",
    "# start of a new sentece. \n",
    "for token in doc4:\n",
    "    print(token.is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.0 - Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?** Tokenization is the splitting process of a body of text into its component parts. Spacy is intelligent enough to be able to split on:\n",
    "- `Whitespace` - Space between tokens / words\n",
    "- `prefix` - characters at the beginning\n",
    "- `infix` - characters in between \n",
    "- `exception` - special case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.\n",
    "- `suffix` - characters at the end\n",
    "\n",
    "Tokenization yields tokens that are part of the original text, we don't see conversions to word stems or lemmas. Named Entity Recognition will come later. Tokens are the building blocks of a Doc object - everything that helps us to understand the meaning of the text is derived from tokens and their relationships to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "\"                           97 PUNCT                       punct ``                       \n",
      "we                          95 PRON                        nsubj PRP                      \n",
      "'re                         87 AUX                           aux VBP                      \n",
      "moving                     100 VERB                         ROOT VBG                      \n",
      "to                          85 ADP                          prep IN                       \n",
      "L.A                         96 PROPN                        pobj NNP                      \n",
      "!                           97 PUNCT                       punct .                        \n",
      "\"                           97 PUNCT                       punct ''                       \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'\"we\\'re moving to L.A!\"')\n",
    "doc_analysis(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "We                          95 PRON                        nsubj PRP                      \n",
      "'re                         87 AUX                          ROOT VBP                      \n",
      "here                        86 ADV                        advmod RB                       \n",
      "to                          94 PART                          aux TO                       \n",
      "help                       100 VERB                        advcl VB                       \n",
      "!                           97 PUNCT                       punct .                        \n",
      "Send                       100 VERB                         ROOT VB                       \n",
      "Smail                       96 PROPN                    compound NNP                      \n",
      "-                           97 PUNCT                       punct HYPH                     \n",
      "mail                        92 NOUN                         dobj NN                       \n",
      ",                           97 PUNCT                       punct ,                        \n",
      "email                       92 NOUN                         conj NN                       \n",
      "support@oursite.com        101 X                            ROOT ADD                      \n",
      "or                          89 CCONJ                          cc CC                       \n",
      "visit                      100 VERB                         conj VB                       \n",
      "us                          95 PRON                         dobj PRP                      \n",
      "at                          85 ADP                          prep IN                       \n",
      "http://www.oursite.com     101 X                            pobj ADD                      \n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"We're here to help! Send Smail-mail, email support@oursite.com or visit us at http://www.oursite.com\")\n",
    "\n",
    "# demonstrate the library's capability with modern strings\n",
    "# such as web addresses or email addresses. \n",
    "# dot operators not identified as a punctuation token in \n",
    "# these instances. \n",
    "doc_analysis(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "A                           90 DET                           det DT                       \n",
      "5                           93 NUM                        nummod CD                       \n",
      "km                          92 NOUN                     compound NN                       \n",
      "NYC                         96 PROPN                    compound NNP                      \n",
      "cab                         92 NOUN                     compound NN                       \n",
      "ride                        92 NOUN                        nsubj NN                       \n",
      "costs                      100 VERB                         ROOT VBZ                      \n",
      "$                           99 SYM                          nmod $                        \n",
      "10.30                       93 NUM                          dobj CD                       \n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"A 5km NYC cab ride costs $10.30\")\n",
    "\n",
    "# demonstrate spacy being smart enough to keep \n",
    "# monetary amounts intact while separating the \n",
    "# amounts fro a distance indicator with 5km.\n",
    "doc_analysis(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Let                        100 VERB                         ROOT VB                       \n",
      "'s                          95 PRON                        nsubj PRP                      \n",
      "visit                      100 VERB                        ccomp VB                       \n",
      "St.                         96 PROPN                    compound NNP                      \n",
      "Louis                       96 PROPN                        dobj NNP                      \n",
      "in                          85 ADP                          prep IN                       \n",
      "the                         90 DET                           det DT                       \n",
      "U.S.                        96 PROPN                        pobj NNP                      \n",
      "next                        84 ADJ                          amod JJ                       \n",
      "year                        92 NOUN                     npadvmod NN                       \n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year\")\n",
    "doc_analysis(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better to give\n",
      "give receive\n"
     ]
    }
   ],
   "source": [
    "doc5 = nlp(u\"It is better to give than receive.\")\n",
    "print(doc5[2:5])\n",
    "print(doc5[4], doc5[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(doc5[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:** It's worth noting that spacy has derived a lot of information from the tokens of a doc, therefore `does not support inline reassignment of values to the doc or token` under analysis without performing the entire operation again. \n",
    "\n",
    "**What are ents?** we can also touch upon another type of token that may be derived. These can be accessed in the `.ents` method. These are the `named entities` meaning the library is smart enough to recognise some organisations names and place names as well as monetary units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Apple                       92 NOUN                        nsubj NN                       \n",
      "to                          94 PART                          aux TO                       \n",
      "build                      100 VERB                        relcl VB                       \n",
      "Scottish                    84 ADJ                          amod JJ                       \n",
      "factory                     92 NOUN                         dobj NN                       \n",
      "in                          85 ADP                          prep IN                       \n",
      "Glasgow                     96 PROPN                        pobj NNP                      \n",
      "creating                   100 VERB                         ROOT VBG                      \n",
      "400                         93 NUM                        nummod CD                       \n",
      "jobs                        92 NOUN                         dobj NNS                      \n",
      "at                          85 ADP                          prep IN                       \n",
      "a                           90 DET                           det DT                       \n",
      "cost                        92 NOUN                         pobj NN                       \n",
      "of                          85 ADP                          prep IN                       \n",
      "$                           99 SYM                      quantmod $                        \n",
      "290                         93 NUM                      compound CD                       \n",
      "million                     93 NUM                          pobj CD                       \n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(u\"Apple to build Scottish factory in Glasgow creating 400 jobs at a cost of $290 million\")\n",
    "\n",
    "doc_analysis(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple                 ORG 383 Companies, agencies, institutions, etc.\n",
      "Scottish             NORP 381 Nationalities or religious or political groups\n",
      "Glasgow               GPE 384 Countries, cities, states\n",
      "400              CARDINAL 397 Numerals that do not fall under another type\n",
      "$290 million        MONEY 394 Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# show the entity info\n",
    "for ent in doc6.ents:\n",
    "    entity = ent\n",
    "    lab = ent.label_\n",
    "    print(f\"{str(entity):<15}{str(lab):>10} {ent.label} {str(spacy.explain(ent.label_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Autonomous                  84 ADJ                          amod JJ                       \n",
      "cars                        92 NOUN                        nsubj NNS                      \n",
      "shify                      100 VERB                         ROOT VBP                      \n",
      "insurance                   92 NOUN                     compound NN                       \n",
      "liability                   92 NOUN                         dobj NN                       \n",
      "toward                      85 ADP                          prep IN                       \n",
      "manufacturers               92 NOUN                         pobj NNS                      \n",
      ".                           97 PUNCT                       punct .                        \n"
     ]
    }
   ],
   "source": [
    "doc7 = nlp(u\"Autonomous cars shify insurance liability toward manufacturers.\")\n",
    "doc_analysis(doc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "# demonstrate noun chunks\n",
    "\n",
    "for chunk in doc7.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
