{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "![nlp](https://wrm5sysfkg-flywheel.netdna-ssl.com/wp-content/uploads/2019/01/NLP-Technology-in-Healthcare.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An area of computer science and artificial intelligence concerned with the interaction between computers and humans (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "To add context to the above it is fair to say that when performing analysis, a lot of data is numerical, measurements based and quantifiable. Computers are exceptional at dealing with this type of data. But, textual data is a whole other realm of meaning. Humans can tell there is a plethora of information inside of text documents but a computer needs specialised processing in order to 'understand' raw text data. Text data is highly unstructured and can be a mix of multiple languages.\n",
    "\n",
    "NLP is therefore a variety of techniques to create structure from text data. It is an area of active developments and advances in the field are constant. \n",
    "\n",
    "Typical use cases for NLP: \n",
    "- Email classification between spam and legitimate emails\n",
    "- Sentiment Analysis of movie reviews text\n",
    "- Analyzing trends from written feedback forms\n",
    "- Understanding text commands (Siri, Alexa, Google assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "Spacy is an open source NLP library for Python. It is an implementation of common algorithms for effectively and efficiently dealing with language processing problems. For many NLP tasks Spacy only has one implemented method, typically the most efficient algorithm currently available at the time of publishing the library. This means there is often one-way to do things with Spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NLTK, or natural language toolkit is another open source option that is hugely popular. This dates back to 2001 where as Spacy was first published in 2015. This library has a more comprehensive suite of options for achieving particular tasks but that means it also includes options which are not the not efficient implementation, or suboptimal approach to certain problem sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison notes\n",
    "- For many tasks Spacy is faster and more efficient than NLTK at the cost of having less freedom of choice in algorithmic implementations.\n",
    "- Spacy does not include pre-created models for some applications such as sentiment analysis, which is easier to achieve with NLTK.\n",
    "- Approach taken will be to default to Spacy where a use-case exists and fall back to NLTK for problem sets where this library is better or has additional resources and tooling that are not available in Spacy.\n",
    "\n",
    "[NLTK Vs Spacy speed comparison](https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2#:~:text=NLTK%20is%20a%20string%20processing,spaCy%20uses%20object%2Doriented%20approach.&text=As%20we%20can%20see%20below,sentence%20tokenization%2C%20NLTK%20outperforms%20spaCy)\n",
    "\n",
    "## Additional installation instructions\n",
    "- For conda envs: `conda install -c conda-forge spacy`\n",
    "- For language pack additions (English): `python -m spacy download en`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.0 - Spacy Basics\n",
    "\n",
    "- Loading the language libraries\n",
    "- Building a pipeline object\n",
    "- Using tokens\n",
    "- Parts-of-speech tagging\n",
    "- Understanding token attributes \n",
    "\n",
    "Spacy works with a `pipeline object` this takes raw text and automatically performs a series of operations to tag, parseand describe the text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the language pack (or model).\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the output of the assignation to doc above. Note that we have tokenised \n",
    "# each word. It has a POS value and we can see the full descriptor of the types of \n",
    "# words that is for each. \n",
    "\n",
    "def doc_analysis(doc):\n",
    "    print(f\"{'Token':<20}{'POS':>5} {'POS_desc':<8}{'Syntactic_Dependency':>25} {'tag':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<20}{token.pos:>5} {token.pos_:<8}{token.dep_:>25} {token.tag_:<25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                 POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Tesla                  96 PROPN                       nsubj NNP                      \n",
      "is                     87 AUX                           aux VBZ                      \n",
      "looking               100 VERB                         ROOT VBG                      \n",
      "at                     85 ADP                          prep IN                       \n",
      "buying                100 VERB                        pcomp VBG                      \n",
      "U.S.                   96 PROPN                    compound NNP                      \n",
      "startup                92 NOUN                         dobj NN                       \n",
      "for                    85 ADP                          prep IN                       \n",
      "$                      99 SYM                      quantmod $                        \n",
      "6                      93 NUM                      compound CD                       \n",
      "million                93 NUM                          pobj CD                       \n"
     ]
    }
   ],
   "source": [
    "# run the analysis on our test text \n",
    "doc_analysis(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fbcacb36dd0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fbcaca164b0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fbcad1a5d00>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describes the operations series that is performed on a submitted text. \n",
    "# tagger\n",
    "# parser\n",
    "# named entity recogniser\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Tesla isn't looking into startups anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                 POS POS_desc     Syntactic_Dependency\n",
      "---------------------------------------------------------------------------\n",
      "Tesla                  96 PROPN                       nsubj\n",
      "is                     87 AUX                           aux\n",
      "n't                    94 PART                          neg\n",
      "looking               100 VERB                         ROOT\n",
      "into                   85 ADP                          prep\n",
      "startups               92 NOUN                         pobj\n",
      "anymore                86 ADV                        advmod\n",
      ".                      97 PUNCT                       punct\n"
     ]
    }
   ],
   "source": [
    "doc_analysis(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow up on the syntactic dependency details with the official docs: https://spacy.io/usage/linguistic-features#dependency-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Life is what happens to us while we are making other plans\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "life_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# we can see the spacy is smart enough to note differences between types of input\n",
    "print(type(doc3))\n",
    "print(type(life_quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u\"This is the first sentence. This is another sentence. This is the last sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc4.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# traverse each token in the doc sample to test if the word is the \n",
    "# start of a new sentece. \n",
    "for token in doc4:\n",
    "    print(token.is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.0 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
