{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics\n",
    "\n",
    "![nlp](https://wrm5sysfkg-flywheel.netdna-ssl.com/wp-content/uploads/2019/01/NLP-Technology-in-Healthcare.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An area of computer science and artificial intelligence concerned with the interaction between computers and humans (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "To add context to the above it is fair to say that when performing analysis, a lot of data is numerical, measurements based and quantifiable. Computers are exceptional at dealing with this type of data. But, textual data is a whole other realm of meaning. Humans can tell there is a plethora of information inside of text documents but a computer needs specialised processing in order to 'understand' raw text data. Text data is highly unstructured and can be a mix of multiple languages.\n",
    "\n",
    "NLP is therefore a variety of techniques to create structure from text data. It is an area of active developments and advances in the field are constant. \n",
    "\n",
    "Typical use cases for NLP: \n",
    "- Email classification between spam and legitimate emails\n",
    "- Sentiment Analysis of movie reviews text\n",
    "- Analyzing trends from written feedback forms\n",
    "- Understanding text commands (Siri, Alexa, Google assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "Spacy is an open source NLP library for Python. It is an implementation of common algorithms for effectively and efficiently dealing with language processing problems. For many NLP tasks Spacy only has one implemented method, typically the most efficient algorithm currently available at the time of publishing the library. This means there is often one-way to do things with Spacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NLTK, or natural language toolkit is another open source option that is hugely popular. This dates back to 2001 where as Spacy was first published in 2015. This library has a more comprehensive suite of options for achieving particular tasks but that means it also includes options which are not the not efficient implementation, or suboptimal approach to certain problem sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison notes\n",
    "- For many tasks Spacy is faster and more efficient than NLTK at the cost of having less freedom of choice in algorithmic implementations.\n",
    "- Spacy does not include pre-created models for some applications such as sentiment analysis, which is easier to achieve with NLTK.\n",
    "- Approach taken will be to default to Spacy where a use-case exists and fall back to NLTK for problem sets where this library is better or has additional resources and tooling that are not available in Spacy.\n",
    "\n",
    "[NLTK Vs Spacy speed comparison](https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2#:~:text=NLTK%20is%20a%20string%20processing,spaCy%20uses%20object%2Doriented%20approach.&text=As%20we%20can%20see%20below,sentence%20tokenization%2C%20NLTK%20outperforms%20spaCy)\n",
    "\n",
    "## Additional installation instructions\n",
    "- For conda envs: `conda install -c conda-forge spacy`\n",
    "- For language pack additions (English): `python -m spacy download en`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.0 - Spacy Basics\n",
    "\n",
    "- Loading the language libraries\n",
    "- Building a pipeline object\n",
    "- Using tokens\n",
    "- Parts-of-speech tagging\n",
    "- Understanding token attributes \n",
    "\n",
    "Spacy works with a `pipeline object` this takes raw text and automatically performs a series of operations to tag, parseand describe the text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the language pack (or model).\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the output of the assignation to doc above. Note that we have tokenised \n",
    "# each word. It has a POS value and we can see the full descriptor of the types of \n",
    "# words that is for each. \n",
    "\n",
    "def doc_analysis(doc):\n",
    "    print(f\"{'Token':<25}{'POS':>5} {'POS_desc':<8}{'Syntactic_Dependency':>25} {'tag':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<25}{token.pos:>5} {token.pos_:<8}{token.dep_:>25} {token.tag_:<25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Tesla                       96 PROPN                       nsubj NNP                      \n",
      "is                          87 AUX                           aux VBZ                      \n",
      "looking                    100 VERB                         ROOT VBG                      \n",
      "at                          85 ADP                          prep IN                       \n",
      "buying                     100 VERB                        pcomp VBG                      \n",
      "U.S.                        96 PROPN                    compound NNP                      \n",
      "startup                     92 NOUN                         dobj NN                       \n",
      "for                         85 ADP                          prep IN                       \n",
      "$                           99 SYM                      quantmod $                        \n",
      "6                           93 NUM                      compound CD                       \n",
      "million                     93 NUM                          pobj CD                       \n"
     ]
    }
   ],
   "source": [
    "# run the analysis on our test text \n",
    "doc_analysis(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fb5bde9af90>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fb5be46b8a0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fb5be52c520>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describes the operations series that is performed on a submitted text. \n",
    "# tagger\n",
    "# parser\n",
    "# named entity recogniser\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Tesla isn't looking into startups anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Tesla                       96 PROPN                       nsubj NNP                      \n",
      "is                          87 AUX                           aux VBZ                      \n",
      "n't                         94 PART                          neg RB                       \n",
      "looking                    100 VERB                         ROOT VBG                      \n",
      "into                        85 ADP                          prep IN                       \n",
      "startups                    92 NOUN                         pobj NNS                      \n",
      "anymore                     86 ADV                        advmod RB                       \n",
      ".                           97 PUNCT                       punct .                        \n"
     ]
    }
   ],
   "source": [
    "doc_analysis(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow up on the syntactic dependency details with the official docs: https://spacy.io/usage/linguistic-features#dependency-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Life is what happens to us while we are making other plans\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "life_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# we can see the spacy is smart enough to note differences between types of input\n",
    "print(type(doc3))\n",
    "print(type(life_quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u\"This is the first sentence. This is another sentence. This is the last sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc4.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "True\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# traverse each token in the doc sample to test if the word is the \n",
    "# start of a new sentece. \n",
    "for token in doc4:\n",
    "    print(token.is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.0 - Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?** Tokenization is the splitting process of a body of text into its component parts. Spacy is intelligent enough to be able to split on:\n",
    "- `Whitespace` - Space between tokens / words\n",
    "- `prefix` - characters at the beginning\n",
    "- `infix` - characters in between \n",
    "- `exception` - special case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.\n",
    "- `suffix` - characters at the end\n",
    "\n",
    "Tokenization yields tokens that are part of the original text, we don't see conversions to word stems or lemmas. Named Entity Recognition will come later. Tokens are the building blocks of a Doc object - everything that helps us to understand the meaning of the text is derived from tokens and their relationships to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "\"                           97 PUNCT                       punct ``                       \n",
      "we                          95 PRON                        nsubj PRP                      \n",
      "'re                         87 AUX                           aux VBP                      \n",
      "moving                     100 VERB                         ROOT VBG                      \n",
      "to                          85 ADP                          prep IN                       \n",
      "L.A                         96 PROPN                        pobj NNP                      \n",
      "!                           97 PUNCT                       punct .                        \n",
      "\"                           97 PUNCT                       punct ''                       \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'\"we\\'re moving to L.A!\"')\n",
    "doc_analysis(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "We                          95 PRON                        nsubj PRP                      \n",
      "'re                         87 AUX                          ROOT VBP                      \n",
      "here                        86 ADV                        advmod RB                       \n",
      "to                          94 PART                          aux TO                       \n",
      "help                       100 VERB                        advcl VB                       \n",
      "!                           97 PUNCT                       punct .                        \n",
      "Send                       100 VERB                         ROOT VB                       \n",
      "Smail                       96 PROPN                    compound NNP                      \n",
      "-                           97 PUNCT                       punct HYPH                     \n",
      "mail                        92 NOUN                         dobj NN                       \n",
      ",                           97 PUNCT                       punct ,                        \n",
      "email                       92 NOUN                         conj NN                       \n",
      "support@oursite.com        101 X                            ROOT ADD                      \n",
      "or                          89 CCONJ                          cc CC                       \n",
      "visit                      100 VERB                         conj VB                       \n",
      "us                          95 PRON                         dobj PRP                      \n",
      "at                          85 ADP                          prep IN                       \n",
      "http://www.oursite.com     101 X                            pobj ADD                      \n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"We're here to help! Send Smail-mail, email support@oursite.com or visit us at http://www.oursite.com\")\n",
    "\n",
    "# demonstrate the library's capability with modern strings\n",
    "# such as web addresses or email addresses. \n",
    "# dot operators not identified as a punctuation token in \n",
    "# these instances. \n",
    "doc_analysis(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "A                           90 DET                           det DT                       \n",
      "5                           93 NUM                        nummod CD                       \n",
      "km                          92 NOUN                     compound NN                       \n",
      "NYC                         96 PROPN                    compound NNP                      \n",
      "cab                         92 NOUN                     compound NN                       \n",
      "ride                        92 NOUN                        nsubj NN                       \n",
      "costs                      100 VERB                         ROOT VBZ                      \n",
      "$                           99 SYM                          nmod $                        \n",
      "10.30                       93 NUM                          dobj CD                       \n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"A 5km NYC cab ride costs $10.30\")\n",
    "\n",
    "# demonstrate spacy being smart enough to keep \n",
    "# monetary amounts intact while separating the \n",
    "# amounts fro a distance indicator with 5km.\n",
    "doc_analysis(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Let                        100 VERB                         ROOT VB                       \n",
      "'s                          95 PRON                        nsubj PRP                      \n",
      "visit                      100 VERB                        ccomp VB                       \n",
      "St.                         96 PROPN                    compound NNP                      \n",
      "Louis                       96 PROPN                        dobj NNP                      \n",
      "in                          85 ADP                          prep IN                       \n",
      "the                         90 DET                           det DT                       \n",
      "U.S.                        96 PROPN                        pobj NNP                      \n",
      "next                        84 ADJ                          amod JJ                       \n",
      "year                        92 NOUN                     npadvmod NN                       \n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year\")\n",
    "doc_analysis(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better to give\n",
      "give receive\n"
     ]
    }
   ],
   "source": [
    "doc5 = nlp(u\"It is better to give than receive.\")\n",
    "print(doc5[2:5])\n",
    "print(doc5[4], doc5[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(doc5[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:** It's worth noting that spacy has derived a lot of information from the tokens of a doc, therefore `does not support inline reassignment of values to the doc or token` under analysis without performing the entire operation again. \n",
    "\n",
    "**What are ents?** we can also touch upon another type of token that may be derived. These can be accessed in the `.ents` method. These are the `named entities` meaning the library is smart enough to recognise some organisations names and place names as well as monetary units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Apple                       92 NOUN                        nsubj NN                       \n",
      "to                          94 PART                          aux TO                       \n",
      "build                      100 VERB                        relcl VB                       \n",
      "Scottish                    84 ADJ                          amod JJ                       \n",
      "factory                     92 NOUN                         dobj NN                       \n",
      "in                          85 ADP                          prep IN                       \n",
      "Glasgow                     96 PROPN                        pobj NNP                      \n",
      "creating                   100 VERB                         ROOT VBG                      \n",
      "400                         93 NUM                        nummod CD                       \n",
      "jobs                        92 NOUN                         dobj NNS                      \n",
      "at                          85 ADP                          prep IN                       \n",
      "a                           90 DET                           det DT                       \n",
      "cost                        92 NOUN                         pobj NN                       \n",
      "of                          85 ADP                          prep IN                       \n",
      "$                           99 SYM                      quantmod $                        \n",
      "290                         93 NUM                      compound CD                       \n",
      "million                     93 NUM                          pobj CD                       \n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(u\"Apple to build Scottish factory in Glasgow creating 400 jobs at a cost of $290 million\")\n",
    "\n",
    "doc_analysis(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple                 ORG 383 Companies, agencies, institutions, etc.\n",
      "Scottish             NORP 381 Nationalities or religious or political groups\n",
      "Glasgow               GPE 384 Countries, cities, states\n",
      "400              CARDINAL 397 Numerals that do not fall under another type\n",
      "$290 million        MONEY 394 Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# show the entity info\n",
    "for ent in doc6.ents:\n",
    "    entity = ent\n",
    "    lab = ent.label_\n",
    "    print(f\"{str(entity):<15}{str(lab):>10} {ent.label} {str(spacy.explain(ent.label_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token                      POS POS_desc     Syntactic_Dependency tag                      \n",
      "---------------------------------------------------------------------------\n",
      "Autonomous                  84 ADJ                          amod JJ                       \n",
      "cars                        92 NOUN                        nsubj NNS                      \n",
      "shify                      100 VERB                         ROOT VBP                      \n",
      "insurance                   92 NOUN                     compound NN                       \n",
      "liability                   92 NOUN                         dobj NN                       \n",
      "toward                      85 ADP                          prep IN                       \n",
      "manufacturers               92 NOUN                         pobj NNS                      \n",
      ".                           97 PUNCT                       punct .                        \n"
     ]
    }
   ],
   "source": [
    "doc7 = nlp(u\"Autonomous cars shify insurance liability toward manufacturers.\")\n",
    "doc_analysis(doc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "# demonstrate noun chunks\n",
    "\n",
    "for chunk in doc7.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buit in visualiser \n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc8 = nlp(u\"Apple to build factory in Glasgow creating 500 jobs costing $210 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b9582bf7ddd444b29e637fb63a99c262-0\" class=\"displacy\" width=\"960\" height=\"277.0\" direction=\"ltr\" style=\"max-width: none; height: 277.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"120\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"120\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">build</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"260\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"260\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Glasgow</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">creating</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"540\">500</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"540\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">jobs</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">costing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">210</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">million</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-0\" stroke-width=\"2px\" d=\"M140,142.0 C140,107.0 175.0,107.0 175.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M140,144.0 L132,132.0 148,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-1\" stroke-width=\"2px\" d=\"M70,142.0 C70,72.0 180.0,72.0 180.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M180.0,144.0 L188.0,132.0 172.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-2\" stroke-width=\"2px\" d=\"M210,142.0 C210,107.0 245.0,107.0 245.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245.0,144.0 L253.0,132.0 237.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-3\" stroke-width=\"2px\" d=\"M210,142.0 C210,72.0 320.0,72.0 320.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M320.0,144.0 L328.0,132.0 312.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-4\" stroke-width=\"2px\" d=\"M350,142.0 C350,107.0 385.0,107.0 385.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,144.0 L393.0,132.0 377.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-5\" stroke-width=\"2px\" d=\"M210,142.0 C210,2.0 470.0,2.0 470.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470.0,144.0 L478.0,132.0 462.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-6\" stroke-width=\"2px\" d=\"M560,142.0 C560,107.0 595.0,107.0 595.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560,144.0 L552,132.0 568,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-7\" stroke-width=\"2px\" d=\"M490,142.0 C490,72.0 600.0,72.0 600.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M600.0,144.0 L608.0,132.0 592.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-8\" stroke-width=\"2px\" d=\"M630,142.0 C630,107.0 665.0,107.0 665.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M665.0,144.0 L673.0,132.0 657.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-9\" stroke-width=\"2px\" d=\"M770,142.0 C770,72.0 880.0,72.0 880.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,144.0 L762,132.0 778,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-10\" stroke-width=\"2px\" d=\"M840,142.0 C840,107.0 875.0,107.0 875.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M840,144.0 L832,132.0 848,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b9582bf7ddd444b29e637fb63a99c262-0-11\" stroke-width=\"2px\" d=\"M700,142.0 C700,37.0 885.0,37.0 885.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b9582bf7ddd444b29e637fb63a99c262-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M885.0,144.0 L893.0,132.0 877.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc8, style='dep', jupyter=True, options={'distance':70})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc9 = nlp(u\"Over the last quarter Apple sold nearly 6 hundred thousand iPad Pros for a profit of $312 million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold nearly \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6 hundred thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " iPad Pros for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $312 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc9, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc10 = nlp(u\"This is being served up to you now.\")\n",
    "#displacy.serve(doc10, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3136bb782c4f4de38f5a98202bdb9a4c-0\" class=\"displacy\" width=\"1450\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">being</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">served</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">up</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">now.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-6\" stroke-width=\"2px\" d=\"M595,352.0 C595,2.0 1275.0,2.0 1275.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3136bb782c4f4de38f5a98202bdb9a4c-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,354.0 L1283.0,342.0 1267.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc10, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.0 - Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's stemming?** Essentially stemming is a crude method of cataloging related words by chopping of letters at the end until a common stem is reached. It is generally accepted to work \"quite well\" but the English language has many quirks and imperfections in its construct - meaning there are many cases where a more sophisticated method is required. By example, if we search the word 'boat', we might return 'boats, boating, boater' so we can say that 'boat' is the `stem` for boat, boater, boating, boats.\n",
    "\n",
    "The `Spacy` library doesn't include a `stemmer` opting instead to rely on lemmatization. Stemming is a huge topic in NLP and the subject of many discussions and debates. It is helpful to know and understand the basics of stemming before moving on to lemmatization. \n",
    "\n",
    "For the purposes of covering stemming we're going to have a look at the `NLTK` library. We will look at:\n",
    "- Porter Stemming\n",
    "- Snowball Stemming\n",
    "\n",
    "[Porter's algorithm](https://tartarus.org/martin/PorterStemmer/def.txt) is considered one of the most common and most effective available. The algorithm employs five phases of word reduction, each with its own set of mapping rules.\n",
    "\n",
    "![](\"https://github.com/irisida/machine_learning/blob/master/image_resources/porterstemmer.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball stemming is also authored by Porter and is sometime referred to as Porters-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ------> run\n",
      "runner ------> runner\n",
      "ran ------> ran\n",
      "runs ------> run\n",
      "easily ------> easili\n",
      "fairly ------> fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + ' ------> ' + ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ------> run\n",
      "runner ------> runner\n",
      "ran ------> ran\n",
      "runs ------> run\n",
      "easily ------> easili\n",
      "fairly ------> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + ' ------> ' + ss.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ------> gener\n",
      "generously ------> gener\n",
      "generate ------> gener\n",
      "generation ------> gener\n",
      "generationally ------> gener\n"
     ]
    }
   ],
   "source": [
    "words = [\"generous\", \"generously\", \"generate\", \"generation\", \"generationally\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word + ' ------> ' + ps.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4.0 - Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?** In contrast to stemming, lemmatization looks beyond word reduction and considers a language's full vocabulary in order to apply a morphological analysis to words. The lemma of 'was' is 'be', the lemma of 'mice' is 'mouse', however, the lemma of 'meeting' could be 'meet', or could be 'meeting' depending on the context of the sentence.\n",
    "\n",
    "Lemmatization is typically seen as more informative than simply stemming words which is why the Spacy library has opted to only have lemmatization available and not to implement stemming.\n",
    "\n",
    "**how does it work?** lemmatization works by:\n",
    "- looking at surrounding text to determine a given words part of speech\n",
    "- it does not categorize words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run and have already ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token        Part of Speech  Lemma Type                         Lemma \n",
      " ---------------------------------------------------------------------------\n",
      "I                      PRON      -PRON-            561228191312463089\n",
      "am                      AUX          be          10382539506755952630\n",
      "a                       DET           a          11901859001352538922\n",
      "runner                 NOUN      runner          12640964157389618806\n",
      "running                VERB         run          12767647472892411841\n",
      "in                      ADP          in           3002984154512732771\n",
      "a                       DET           a          11901859001352538922\n",
      "race                   NOUN        race           8048469955494714898\n",
      "because               SCONJ     because          16950148841647037698\n",
      "I                      PRON      -PRON-            561228191312463089\n",
      "love                   VERB        love           3702023516439754181\n",
      "to                     PART          to           3791531372978436496\n",
      "run                    VERB         run          12767647472892411841\n",
      "and                   CCONJ         and           2283656566040971221\n",
      "have                    AUX        have          14692702688101715474\n",
      "already                 ADV     already          12647358837591399640\n",
      "ran                    VERB         run          12767647472892411841\n",
      "today                  NOUN       today          11042482332948150395\n"
     ]
    }
   ],
   "source": [
    "# print table headers\n",
    "print(f\"{'Token':<12}{'Part of Speech':>15}{'Lemma Type':>12}{'Lemma':>30}\",\"\\n\",\"-\" * 75)\n",
    "\n",
    "# iterate over the token values\n",
    "for token in doc1:\n",
    "    print(f\"{str(token):<12}{str(token.pos_):>15}{str(token.lemma_):>12}{str(token.lemma):>30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.0 - Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are stop words?** words such as 'a' and 'the' appear in texts so frequently that they don't require tagging as much as nouns, verbs and modifiers. known as `stop words`. they can be filtered from text that is due to be processed. The `spacy` library holds a list of stop words that is roughly 300 entries long. Typically they are filtered because they offer little to no additional information about the text being processed and can even hurt the meaning that can be drawn from a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thence', 'until', 'several', 'again', 'twenty', 'itself', 'somehow', 'meanwhile', 'call', 'less', 'somewhere', \"'ll\", 'sixty', 'n‘t', 'how', 'such', 'here', 'hundred', 'between', 'yourselves', 'five', 'toward', 'besides', 'mine', 'not', '‘s', '’re', 'often', 'mostly', 'part', 'front', 'much', 'really', 'ever', 'being', '’s', 'others', 'must', 'us', 'whereafter', 'no', 'however', 'forty', 'your', 'other', 'back', 'except', \"'m\", 'they', 'for', 'he', 'after', 'be', 'at', 'everything', 'name', 'four', 'had', 'about', 'her', 'throughout', 'whom', 'yet', 'thus', 'done', 'together', 'my', 'empty', 'you', 'towards', 'behind', '’d', 'anywhere', 'three', 'otherwise', 'themselves', 'are', 'rather', 'off', 'thereby', 'anyway', 'am', 'seemed', 'their', 'bottom', 'give', 'an', 'nevertheless', 'would', 'within', 'two', 'never', 'none', 'everywhere', 'from', 'this', 'during', '’ll', 'every', 'if', 'that', 'once', 'third', 'ten', 'own', 'can', 'just', 'below', 'cannot', 'twelve', 'therefore', 'see', 'same', 'across', 'top', 'although', 'before', 'whereas', 'will', 'without', 'make', 'seeming', 'please', '‘ll', 'might', 'ca', 'upon', '‘re', 'few', 'where', 'per', 'our', 'should', 'formerly', 'even', '’ve', 'though', 'to', 'which', 'neither', 'quite', 'on', 'thereupon', '‘m', 'we', 'nine', 'his', 'there', 'with', 'wherein', 'go', 'eight', 'through', 'became', 'why', 'latter', 'also', 'well', 'it', 'fifteen', 'each', 'one', 'both', 'when', 'so', 'another', 'has', 'latterly', 'perhaps', 'the', 'but', 'whose', 'along', 'alone', 'him', 'who', 'more', 'something', 'up', 'moreover', 'thereafter', 'nothing', 'sometimes', 'very', 'enough', 'nor', 'into', 'as', 'fifty', 'say', 'former', 'put', 'afterwards', 'nowhere', 'already', 'whatever', 'due', 'than', 'them', 'always', 'full', 'by', 'whenever', 'herself', 'yourself', 'hereupon', '’m', 'onto', 'were', 'serious', 'indeed', 'above', 'because', 'whereupon', 'among', 'least', '‘ve', 'hereby', 'some', 'unless', 'regarding', 'whither', 'what', 'herein', 'those', 'hence', 'me', 'show', \"'d\", 'further', 'in', 'anyone', 'six', 'last', 'becoming', 'namely', 'or', \"n't\", 'while', 'these', 'thru', 'first', 'been', 'does', 'btw', 'whereby', \"'re\", 'against', 'himself', \"'ve\", 'n’t', 'ours', 'using', 'becomes', 'become', 'via', 'i', 'too', 'elsewhere', 'doing', 'eleven', 'anything', 'whole', 'then', 'amount', 'now', 'keep', 'since', 'used', 'may', \"'s\", 'amongst', 'she', 'did', 'is', 'a', 'ourselves', 'almost', 'next', 'around', 'take', 'either', 'beforehand', 'have', 'therein', '‘d', 'all', 'and', 'nobody', 'myself', 'sometime', 'out', 'wherever', 'seem', 'seems', 'do', 'yours', 'many', 'still', 'hers', 'under', 'down', 'of', 'its', 'could', 'move', 'made', 'whence', 'beyond', 'hereafter', 'get', 'everyone', 'beside', 'any', 'whoever', 're', 'whether', 'various', 'most', 'side', 'else', 'over', 'only', 'someone', 'noone', 'was', 'anyhow'}\n"
     ]
    }
   ],
   "source": [
    "# to see the list of library specified stop words as \n",
    "# the defaults available.\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word can be passed to the `.is_stop` method when using a `vocab` lookup. This will have a boolean response to the word under tests status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking a words status as a stop word. \n",
    "nlp.vocab['you'].is_stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a word to the stop words list\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# removing a words from the defaults is possible too. \n",
    "print(nlp.vocab['btw'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#nlp.Defaults.stop_words.remove('btw')\n",
    "\n",
    "nlp.vocab['btw'].is_stop = False\n",
    "\n",
    "print(nlp.vocab['btw'].is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6.0 - Phrase matching and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how text is divided into tokens, how individual tokens are parsed and tagged with POS (parts of speech) dependencies and lemmas. We can also have programatically determined identifiers as types of labels and phrases that can be matched against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based matching is offered in Spacy with a tool called\n",
    "# matcher. This allows us to build a library of token paterns\n",
    "# then match against a doc object to return a list object of \n",
    "# found matches. Essentially it's quite similar to regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add patterns to the matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expects us to pass a list of dictionaries.\n",
    "\n",
    "# for this sample here we would like to be able to match\n",
    "# upon the following cases detected:\n",
    "\n",
    "# detect SolarPower\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "\n",
    "# detect Solar-power\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]\n",
    "\n",
    "# detect Solar power\n",
    "pattern3 = [{'LOWER': 'solar'},{'LOWER':'power'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to the matcher \n",
    "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"The Solar Power industry continues to flourish as solarpower increases. New Solar-Power technology is amazing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8656102463236116519, 1, 3),\n",
       " (8656102463236116519, 8, 9),\n",
       " (8656102463236116519, 12, 15)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 9 solarpower\n",
      "8656102463236116519 SolarPower 12 15 Solar-Power\n"
     ]
    }
   ],
   "source": [
    "for id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[id]\n",
    "    span = doc[start:end]\n",
    "    print(id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing patterns from the matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the matcher case we added above\n",
    "matcher.remove('SolarPower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new patterns with optional cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "\n",
    "# add a pattern that allows for zero or more matches of the hyphen punctuation\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True, 'OP': '*'}, {'LOWER':'power'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower', None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"The Solar Power industry can be called solarpower, or even Solar--Power, but seldom solar---power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 7 8 solarpower\n",
      "8656102463236116519 SolarPower 11 14 Solar--Power\n",
      "8656102463236116519 SolarPower 17 20 solar---power\n"
     ]
    }
   ],
   "source": [
    "for id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[id]\n",
    "    span = doc[start:end]\n",
    "    print(id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrase matching\n",
    "\n",
    "Above we're using token patterns to work some rule-based matching. An efficient alternative is  to match on a terminology list. This means we use a phrase matcher to create a document object with a list of phrases and this is what is then passed to the matcher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/reaganomics.txt',encoding= 'unicode_escape') as f:\n",
    "    doc3 = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_patterns = [nlp(text) for text in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('Econs', None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6801154258838967149, 41, 45),\n",
       " (6801154258838967149, 49, 53),\n",
       " (6801154258838967149, 54, 56),\n",
       " (6801154258838967149, 61, 65),\n",
       " (6801154258838967149, 673, 677),\n",
       " (6801154258838967149, 2986, 2990)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6801154258838967149 Econs 41 45 supply-side economics\n",
      "6801154258838967149 Econs 49 53 trickle-down economics\n",
      "6801154258838967149 Econs 54 56 voodoo economics\n",
      "6801154258838967149 Econs 61 65 free-market economics\n",
      "6801154258838967149 Econs 673 677 supply-side economics\n",
      "6801154258838967149 Econs 2986 2990 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "for id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[id]\n",
    "    span = doc3[start:end]\n",
    "    print(id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grab a better context of the matches you can update the convenience loop to add additional tokens from the point of match starting and ending. Let's say we want to grab 4 words at either side of a match to better grasp its content in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6801154258838967149 Econs 41 45 are commonly associated with supply-side economics, referred to as\n",
      "6801154258838967149 Econs 49 53 , referred to as trickle-down economics or voodoo economics by\n",
      "6801154258838967149 Econs 54 56 -down economics or voodoo economics by political opponents,\n",
      "6801154258838967149 Econs 61 65 political opponents, and free-market economics by political advocates.\n",
      "6801154258838967149 Econs 673 677 a following from the supply-side economics movement, which formed\n",
      "6801154258838967149 Econs 2986 2990 widely known as \"trickle-down economics\", due to\n"
     ]
    }
   ],
   "source": [
    "for id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[id]\n",
    "    span = doc3[start-4:end+4]\n",
    "    print(id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
