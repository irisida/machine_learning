{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "![nlp](https://wrm5sysfkg-flywheel.netdna-ssl.com/wp-content/uploads/2019/01/NLP-Technology-in-Healthcare.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Section goals:\n",
    "- Understand basic overview of Deep Learning\n",
    "- Understand basics of LSTM and RNN\n",
    "- Use LSTM to generate text from source corpus \n",
    "- Create QA Chatbots with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1.0 The basic Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neural networks or ANN's have a basis in biology. A Perceptron is the common term for an artificial neuron that mimics a biological neuron.\n",
    "\n",
    "The biological neuron is made up of component parts: \n",
    "- Dendrites\n",
    "- Body\n",
    "- Axon\n",
    "\n",
    "The artificial neuron is also multipart: \n",
    "- Inputs\n",
    "- Body\n",
    "- Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2.0 Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x_train = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8428fdaa50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_x_train, y_train, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.46506203e-03, 6.57006800e-01, 3.36528093e-01],\n",
       "       [9.71322954e-01, 2.84389425e-02, 2.38171298e-04],\n",
       "       [2.81957318e-06, 5.48888147e-02, 9.45108354e-01],\n",
       "       [4.88138339e-03, 6.34165525e-01, 3.60953093e-01],\n",
       "       [1.08065910e-03, 4.24147785e-01, 5.74771523e-01],\n",
       "       [9.55046833e-01, 4.44088355e-02, 5.44341747e-04],\n",
       "       [4.82193977e-02, 8.23449790e-01, 1.28330842e-01],\n",
       "       [5.31006153e-05, 1.68322966e-01, 8.31623852e-01],\n",
       "       [9.44099447e-04, 3.18148047e-01, 6.80907905e-01],\n",
       "       [2.09649596e-02, 7.73502707e-01, 2.05532342e-01],\n",
       "       [3.93887254e-04, 3.52640182e-01, 6.46965921e-01],\n",
       "       [9.63348567e-01, 3.60602252e-02, 5.91216958e-04],\n",
       "       [9.74746764e-01, 2.50020996e-02, 2.51196208e-04],\n",
       "       [9.66157794e-01, 3.33341695e-02, 5.07981807e-04],\n",
       "       [9.87617075e-01, 1.22843934e-02, 9.85338265e-05],\n",
       "       [3.89748579e-03, 6.60716176e-01, 3.35386276e-01],\n",
       "       [3.94649978e-05, 1.44074813e-01, 8.55885684e-01],\n",
       "       [2.58401204e-02, 7.68771946e-01, 2.05388010e-01],\n",
       "       [1.08118318e-02, 7.17714131e-01, 2.71474034e-01],\n",
       "       [4.67599857e-05, 1.40095577e-01, 8.59857678e-01],\n",
       "       [9.72084641e-01, 2.75204107e-02, 3.94957751e-04],\n",
       "       [1.22523634e-03, 4.64656562e-01, 5.34118176e-01],\n",
       "       [9.70420659e-01, 2.92196646e-02, 3.59670463e-04],\n",
       "       [5.89976335e-05, 1.51802063e-01, 8.48138869e-01],\n",
       "       [2.07461417e-05, 1.56888440e-01, 8.43090892e-01],\n",
       "       [4.98558256e-05, 1.57735810e-01, 8.42214286e-01],\n",
       "       [5.22744085e-05, 1.28298119e-01, 8.71649563e-01],\n",
       "       [2.28807985e-05, 1.26816332e-01, 8.73160779e-01],\n",
       "       [9.58042741e-01, 4.12452109e-02, 7.11955829e-04],\n",
       "       [9.64021027e-01, 3.54171544e-02, 5.61889436e-04],\n",
       "       [9.88629222e-01, 1.12846224e-02, 8.61179869e-05],\n",
       "       [9.91966486e-01, 7.99886510e-03, 3.46576271e-05],\n",
       "       [3.94367939e-03, 6.36070788e-01, 3.59985471e-01],\n",
       "       [9.79480028e-01, 2.02802382e-02, 2.39714020e-04],\n",
       "       [9.79118645e-01, 2.06094719e-02, 2.71818135e-04],\n",
       "       [1.80627874e-04, 2.02401295e-01, 7.97418177e-01],\n",
       "       [4.79980465e-03, 6.73968256e-01, 3.21231902e-01],\n",
       "       [9.77890849e-01, 2.18799338e-02, 2.29159719e-04],\n",
       "       [9.85660076e-01, 1.42061040e-02, 1.33839596e-04],\n",
       "       [9.92699444e-01, 7.26202317e-03, 3.85917119e-05],\n",
       "       [6.08720293e-04, 3.30877572e-01, 6.68513775e-01],\n",
       "       [1.22999875e-02, 7.61036098e-01, 2.26663962e-01],\n",
       "       [1.99466408e-03, 5.45110941e-01, 4.52894330e-01],\n",
       "       [9.87212002e-01, 1.26976594e-02, 9.04247281e-05],\n",
       "       [9.82121050e-01, 1.77269150e-02, 1.52030610e-04],\n",
       "       [4.11694273e-02, 7.90449381e-01, 1.68381184e-01],\n",
       "       [1.13956747e-03, 4.29407179e-01, 5.69453239e-01],\n",
       "       [4.00535209e-04, 3.42726052e-01, 6.56873405e-01],\n",
       "       [3.77465435e-03, 6.16135240e-01, 3.80090028e-01],\n",
       "       [1.10836136e-05, 1.12812735e-01, 8.87176156e-01]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(scaled_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 2, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(scaled_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(scaled_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0, 13,  2],\n",
       "       [ 0,  0, 16]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.87      0.93        15\n",
      "           2       0.89      1.00      0.94        16\n",
      "\n",
      "    accuracy                           0.96        50\n",
      "   macro avg       0.96      0.96      0.96        50\n",
      "weighted avg       0.96      0.96      0.96        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test.argmax(axis=1), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('iris_Classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3.0 - RNNs - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are typically applied to sequence based data. By example:\n",
    "- Time series Data (sales)\n",
    "- Sentences (stream of words)\n",
    "- Audio (stream of digital sounds)\n",
    "- Car trajectories (stream of GPS points)\n",
    "- Music (stream of sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a \"normal\" neuron in a feed-forward network the the inputs are weighted and those calculations feed into an activation function and onward to an output. In an RNN setting the out is sent back to the neuron itself as the input, allowing for the unrolling through time. \n",
    "We should note that in an RNN the neuron is receiving as inputs: \n",
    "1. The outputs of the previous time step. \n",
    "2. The inputs of the current time step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4.0 - LSTM & GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue that RNNs face is that after a while the network will begin to \"forget\" the first, early inputs as information is lost in each step going through the RNN. Therefore we need some long-term-memory for our network. This is where, or why, the LSTM was created.\n",
    "\n",
    "Let's discuss how an LSTM cell works. \n",
    "\n",
    "1. forget gate layer - What info do we want to forget or discard? This receives $H_{t-1}$ and $X_{t}$ as inputs, this is transformed with weights and biases and passed to a sigmoid function. Return is between 0-1, forget or keep. \n",
    "2. Now we decide what information we are going to store in the cell state. There is two parts \n",
    "    1. A sigmoid layer - input gate later or $I_{t}$ receives the same inputs as step 1\n",
    "    2. A hyperbolic tangent layer - receives the same inputs as step 1 also, creates a vector of new candidate values. $\\tilde{C}_{t}$\n",
    "3. It's time to update the old cell state $C_{t -1}$ to $\\tilde{C}_{t}$ and pass to the $C_{t+1}$\n",
    "4. Take the inputs of step and pass through the hyperbolic tangent\n",
    "\n",
    "See: \n",
    "- [Peepholes](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "- [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5.0 Text Generation with Python and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "- Process the text\n",
    "- Clean the text\n",
    "- Tokenize the text and create sequences with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy already loaded with english language\n"
     ]
    }
   ],
   "source": [
    "# load the english lib, \n",
    "# disable the lib segments that are not \n",
    "# required for this project. \n",
    "\n",
    "if not nlp:\n",
    "    nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "    print(f\"Loaded english language and disabled the parser, tagger & named entity recognition (ner)\")\n",
    "else:\n",
    "    print(f\"spacy already loaded with english language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the read_file utility function\n"
     ]
    }
   ],
   "source": [
    "# utility function to read a file in as a single string of\n",
    "# text we can then process or slice as required. \n",
    "\n",
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text\n",
    "\n",
    "print(f\"Created the read_file utility function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set big enough to work with entire moby dick text\n",
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created the separate_punctuation utility function\n"
     ]
    }
   ],
   "source": [
    "def separate_punctuation(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n\"-#$%;:.`{}[]\\t\\n' ]\n",
    "\n",
    "print(f\"created the separate_punctuation utility function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable d was already loaded.\n"
     ]
    }
   ],
   "source": [
    "if not d:\n",
    "    d = read_file('./resources/moby_dick_four_chapters.txt')\n",
    "    print(f\"loaded text into variable d\")\n",
    "else:\n",
    "    print(f\"variable d was already loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens already created as d minus the punctuation\n"
     ]
    }
   ],
   "source": [
    "if not tokens:\n",
    "    tokens = separate_punctuation(d)\n",
    "    print(f\"tokens created from d minus the punctuation\")\n",
    "else:\n",
    "    print(f\"tokens already created as d minus the punctuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12454"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_sequences were already created\n"
     ]
    }
   ],
   "source": [
    "# 25 words -> network predict word 26\n",
    "train_len = 25 + 1\n",
    "\n",
    "if not text_sequences:\n",
    "    # create the holding pen\n",
    "    text_sequences = []\n",
    "\n",
    "    # perform the sequeces creation\n",
    "    for i in range(train_len, len(tokens)):\n",
    "        seq = tokens[i-train_len:i]\n",
    "        text_sequences.append(seq)\n",
    "\n",
    "else: \n",
    "    print(f\"text_sequences were already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2723"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabularly_size: 2723\n"
     ]
    }
   ],
   "source": [
    "if not vocabulary_size:\n",
    "    vocabulary_size = len(tokenizer.word_counts)\n",
    "    \n",
    "print(f\"vocabularly_size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 962,   15,  269, ...,  154,  265,    7],\n",
       "       [  15,  269,   55, ...,  265,    7,  963],\n",
       "       [ 269,   55,  267, ...,    7,  963,   15],\n",
       "       ...,\n",
       "       [2718,    1,    4, ...,  268,   57,    3],\n",
       "       [   1,    4,   11, ...,   57,    3, 2723],\n",
       "       [   4,   11, 2719, ...,    3, 2723,   28]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the features labels split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocabulary_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len=x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 25, 25)            68100     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 25, 50)            15200     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2724)              138924    \n",
      "=================================================================\n",
      "Total params: 244,974\n",
      "Trainable params: 244,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/pykit/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12428/12428 [==============================] - 21s 2ms/step - loss: 6.8046 - accuracy: 0.0504\n",
      "Epoch 2/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 6.1670 - accuracy: 0.0748\n",
      "Epoch 3/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 6.1373 - accuracy: 0.0748\n",
      "Epoch 4/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 6.0674 - accuracy: 0.0748\n",
      "Epoch 5/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 5.9612 - accuracy: 0.0748\n",
      "Epoch 6/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.9055 - accuracy: 0.0748\n",
      "Epoch 7/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.8655 - accuracy: 0.0748\n",
      "Epoch 8/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.8076 - accuracy: 0.0790\n",
      "Epoch 9/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.7391 - accuracy: 0.0880\n",
      "Epoch 10/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.6657 - accuracy: 0.0923\n",
      "Epoch 11/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.6115 - accuracy: 0.0943\n",
      "Epoch 12/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.5710 - accuracy: 0.0951\n",
      "Epoch 13/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.5298 - accuracy: 0.0962\n",
      "Epoch 14/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.4935 - accuracy: 0.0977\n",
      "Epoch 15/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.4612 - accuracy: 0.0987\n",
      "Epoch 16/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 5.4328 - accuracy: 0.0989\n",
      "Epoch 17/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.4049 - accuracy: 0.0998\n",
      "Epoch 18/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.3778 - accuracy: 0.1009\n",
      "Epoch 19/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.3544 - accuracy: 0.1030\n",
      "Epoch 20/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.3254 - accuracy: 0.1056\n",
      "Epoch 21/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.2986 - accuracy: 0.1107\n",
      "Epoch 22/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.2671 - accuracy: 0.1126\n",
      "Epoch 23/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.2409 - accuracy: 0.1125\n",
      "Epoch 24/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.2128 - accuracy: 0.1153\n",
      "Epoch 25/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.1869 - accuracy: 0.1159\n",
      "Epoch 26/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 5.1637 - accuracy: 0.1175\n",
      "Epoch 27/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 5.1403 - accuracy: 0.1217\n",
      "Epoch 28/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 5.1206 - accuracy: 0.1202\n",
      "Epoch 29/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.0965 - accuracy: 0.1200\n",
      "Epoch 30/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.0769 - accuracy: 0.1215\n",
      "Epoch 31/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.0552 - accuracy: 0.1261\n",
      "Epoch 32/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 5.0322 - accuracy: 0.1280\n",
      "Epoch 33/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 5.0115 - accuracy: 0.1272\n",
      "Epoch 34/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.9881 - accuracy: 0.1286\n",
      "Epoch 35/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.9596 - accuracy: 0.1313\n",
      "Epoch 36/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.9365 - accuracy: 0.1286\n",
      "Epoch 37/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.9103 - accuracy: 0.1329\n",
      "Epoch 38/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.8824 - accuracy: 0.1358\n",
      "Epoch 39/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.8597 - accuracy: 0.1370\n",
      "Epoch 40/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.8306 - accuracy: 0.1404\n",
      "Epoch 41/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.8017 - accuracy: 0.1438\n",
      "Epoch 42/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.7744 - accuracy: 0.1437\n",
      "Epoch 43/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.7478 - accuracy: 0.1477\n",
      "Epoch 44/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.7192 - accuracy: 0.1495\n",
      "Epoch 45/50\n",
      "12428/12428 [==============================] - 19s 2ms/step - loss: 4.6897 - accuracy: 0.1501\n",
      "Epoch 46/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.6586 - accuracy: 0.1552\n",
      "Epoch 47/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.6335 - accuracy: 0.1556\n",
      "Epoch 48/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.6010 - accuracy: 0.1583\n",
      "Epoch 49/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.5749 - accuracy: 0.1594\n",
      "Epoch 50/50\n",
      "12428/12428 [==============================] - 20s 2ms/step - loss: 4.5445 - accuracy: 0.1610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8412516ed0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, batch_size=128, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
