{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Fundamentals\n",
    "\n",
    "#### Leading points\n",
    "- Scientific study estimates the amount of neurons in an adult brain to be more than 100 billion. \n",
    "- All these neurons are connected and interconnected. \n",
    "- Information flows between the neurons via these information-link connections which go to explaining human capabilities such as walking, reading, typing, understanding, questioning and so on. \n",
    "- These connections control communications, emotions, creativity etc.. \n",
    "\n",
    "This leads to defining a neural network as `a network of neurons that exchange information`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic components of a Neuron\n",
    "\n",
    "![neuron](https://www.simplilearn.com/ice9/free_resources_article_thumb/diagram-of-a-biological-neuron.jpg)\n",
    "\n",
    "- Dendrites\n",
    "    - receive data from other neurons.\n",
    "- Cell body \n",
    "    - processes the data received in an information transfer. The information is the flow of electrical signals and its transfer is called `synapse`. The `synapse` is the journey from the Dendrites to the point of continued transfer from the Axon via the terminals. After the process of `synapse` biological chemicals enter the Dendrites for the purpose of increasing/decreasing the electrical potential of the cell body.\n",
    "    - The electrical flow in a biological neuron is what gives the potential of the cell which will lead to decision making. \n",
    "    - Therefore, we can say that new connections (and new learning) is formed from these potentials.  \n",
    "- Axon\n",
    "    - transmits signals to other neurons using the Axon terminals.\n",
    "- Axon terminals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Artificial Neuron\n",
    "\n",
    "The artificial neuron mimics the biological structure. We have the equivalence of `Dendrites`, `Cell bodies` & `Axom terminals` in the artificial setting. \n",
    "\n",
    "- It is entirely possible for an an indefinite amount of inputs and outputs to an artificial neuron. \n",
    "- The inputs are information, data or datums from environment.\n",
    "- The outputs are the final response of the perceptron such as a decision or prediction.\n",
    "\n",
    "#### Example consideration\n",
    "\n",
    "In order to predict a persons salary we might reasonably expect that to be based on two key attributes:\n",
    "- age\n",
    "- Educational Background\n",
    "\n",
    "The perceptron receives the age as an input to the equivalent of the Dendrites, this will typically be represented by a figure, followed by another number for years of study or numerical indicator of depth of education. This is processed and the output will also be a number that indicates/predicts the salary of the profile based on the inputs.\n",
    "\n",
    "There is a `black box` around the `Cell body` of a neuron because it is not that easy to interpret what happens during this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important disclaimers**\n",
    "- it is not known truly how the human brain works but there are significant insights which form the opinions on which the work of all artificial neural technology is based. \n",
    "- Artificial neural networks are merely an abstraction of what is known/accepted in this field of study. \n",
    "- They are nothing more than a simulation of a brain, or thought process.\n",
    "- We can depict the artificial neuron as follows: \n",
    "![neuron](https://www.researchgate.net/profile/Mike_Riley/publication/299490278/figure/fig1/AS:626481235517442@1526376174991/Artificial-Neuron-Structure.png)\n",
    "\n",
    "#### key takeaways:\n",
    "- Inputs are of an indeterminate number\n",
    "- each input is weighted, weighting dictates importance/credence factors.\n",
    "- We then have the `sum function` and the `activation function` which equates to the `black box` of above. \n",
    "- It works on the basis of $sum = \\sum\\limits_{i=1}^n xi \\cdot wi$\n",
    "- In an example with 4 inputs this means that what is passed to the sum function would be: $x1 \\cdot w1 + x2 \\cdot w2 + x3 \\cdot w3 + x4 \\cdot w4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "The Perceptron is the combination of the inputs & weights passed to the sum function and the activation function. Above we seen that the sum function has the job of taking each input & multiplying it by the associated weight for that input, adding to the results of other $input \\cdot weight$ calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example case 1\n",
    "If we take a two-input example case for age and education, we could have the following attributes:\n",
    "- age: input=35, weight=0.8\n",
    "- education: input=25, weight=0.1\n",
    "\n",
    "#### First simplification\n",
    "The sum function is now: \n",
    "- $sum = (35 \\cdot 0.8) + (25 \\cdot 0.1)$\n",
    "\n",
    "#### Second simplification\n",
    "The first simplification is: \n",
    "- $sum = (28) + (2.5)$ = $30.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "We can now apply the activation function. This indicates whether a neuron was `fired or not` or `activated or not`. This `synapse` will change the electrical potential in the biological example but we have no electrical signal in the artificial example so how we represent that in the simplest terms is a `step function` that makes a simple fork decision:\n",
    "- Greater or equal to 1 = 1 (neuron activated)\n",
    "- Otherwise = 0\n",
    "\n",
    "In the sample above we have: \n",
    "$(35 \\cdot 0.8) + (25 \\cdot 0.1) \\sum f = 1$\n",
    "\n",
    "In this simple example that firing is decision tree and in our example analysis we can say a: \n",
    "- `1` indicates the person might receive a salary increase. \n",
    "- `0` indicates not. \n",
    "\n",
    "You can see in this example our step/activation function is trivial and in real cases we will have a more complex set of decision forks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example case 2\n",
    "For example case two we will the same structure of a two-input perceptron, we will re-use the age & education factors and only change the weights. We now have the following attributes:\n",
    "- age: input=35, weight=-0.8\n",
    "- education: input=25, weight 0.1\n",
    "\n",
    "#### Simplification 1\n",
    "$sum = (35 \\cdot -0.8) = (25 \\cdot 0.1) = (-28 +2.5) = -25.5$\n",
    "\n",
    "#### Summary\n",
    "Meaning that under the same `step function` decision fork the nueron will have the negative value and be aggregated to a zero (unfired) in our case and indicate that person may not receive a salary increase in the current scoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We can see that the graph of our step function has only a window between zero and one. At both zero and one we have straight lines of cut-off meaning the values above or below do not matter. Depending on the application we can define the step function thresholds in order to create categorised returns. \n",
    "\n",
    "#### Complimentary theoretical definitions of a Perceptron\n",
    "- Positive weight indicates an exciting synapse (electrical increase of the cell body, or greater likelihood of activation)\n",
    "- Negative weight indicates an inhibitory synpase. Lessening the chances of activation.\n",
    "- Weights are considered synapses\n",
    "- Weights amplify or reduce the input signal. _(see differentiation between value in `ex1 & ex2` purely based on weighting)_\n",
    "- The knowledge of a neural network _is_ the weights. _(The goal of a neural net is to learn the best set of weights that fits a given dataset)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Single Layer Perceptron - Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance to the lessons above we need to define and implement the step and sum functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum function. \n",
    "def sum(inputs, weights):\n",
    "    # checks length f params is good\n",
    "    if len(inputs) == len(weights):\n",
    "        s = 0\n",
    "        for i in range(2):\n",
    "            s += inputs[i] * weights[i]\n",
    "        return s\n",
    "    else:\n",
    "        print(f\"ERROR: inputs length={len(inputs)} : weights length={len(weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function\n",
    "def step_function(sum):\n",
    "    if sum >= 1 : return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of the input scores for \n",
    "# age, education respectively \n",
    "\n",
    "inputs = [35,25]\n",
    "\n",
    "# create a list of the weightings to \n",
    "# apply to each input \n",
    "\n",
    "weights = [0.8, 0.1]\n",
    "\n",
    "# call the step function passing in the result\n",
    "# of the sum function call that takes in the \n",
    "# lists for inputs and weights. \n",
    "\n",
    "step_function(sum(inputs, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example here we return a `1`. This means the neuron `is fired`. In the definition of our example the employee would have qualified for a salary increase based on the decision forks implemented and the parameters passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution example 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep same params for age, education.\n",
    "inputs = [35,25]\n",
    "\n",
    "# change the weights to a negative on age\n",
    "weights = [-0.8, 0.1]\n",
    "\n",
    "# call the step with updated weights.\n",
    "step_function(sum(inputs, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are returning a `0`. The neuron is `not fired`. In this case the employee would not qualify for a salary increase based on the decision forks and parameters passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Single Layer Perceptron - Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In version 1 we use a standard python loop and standard python lists and for a trivial example like this one this wold be fine, but neural nets will often run on high volumes of data ad therefore performance is a key aspect to remember a every stage otherwise our solution may not be credible or usable in a production setting. We are going to rewrite the single layer example using numpy and taking otimisations into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(inputs, weights):\n",
    "    # use the np builtin on ndarrays\n",
    "    # to return the product\n",
    "    return inputs.dot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(sum):\n",
    "    if sum >= 1: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the positive weights v2 edition\n",
    "# creating inputs and weights as np arrays\n",
    "inputs = np.array([35,25])\n",
    "weights = np.array([0.8, 0.1])\n",
    "\n",
    "# execute v2 \n",
    "step_function(sum(inputs, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the negative weights v2 edition\n",
    "# creating inputs and weights as np arrays\n",
    "inputs = np.array([35,25])\n",
    "weights = np.array([-0.8, 0.1])\n",
    "\n",
    "# execute v2 \n",
    "step_function(sum(inputs, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was stated earlier, the goal of a neural network is to determine the best set of weights to apply in order to classify some data. \n",
    "\n",
    "Using the age, education example above the `nn` needs to find the best weights. To make it easier we will use the `binary and` operator to determine values from a tabular example. \n",
    "\n",
    "| input value 1 | input value 2 | Class |\n",
    "| -- |--- |-------|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 0 |\n",
    "| 1  | 0  | 0 |\n",
    "| 1  | 1  | 1 |\n",
    "\n",
    "To process this table with our Perceptron we need to 'give' it each of our values in the table. \n",
    "- $(0 \\cdot 0) + (0 \\cdot 0) = 0$ - This outcome matches the result table \n",
    "- $(0 \\cdot 0) + (1 \\cdot 0) = 0$ - This outcome matches the result table \n",
    "- $(1 \\cdot 0) + (0 \\cdot 0) = 0$ - This outcome matches the result table \n",
    "- $(1 \\cdot 0) + (1 \\cdot 0) = 0$ - This outcome does not match the result table and we have an error. \n",
    "\n",
    "Simple division shows that our model for this example is currently 75% correct because we achieved 3 of 4 expected results, but the correct way to calculate a models accuracy is to table the expected and actual with error: \n",
    "\n",
    "| class | prediction | error |\n",
    "| -- |--- |-------|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 0  | 0 |\n",
    "| 1  | 0  | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error calculation formula\n",
    "- error = correct - prediction\n",
    "\n",
    "#### Reducing the error\n",
    "Our goal is to reduce the errors and achieve a higher accuracy for out Perceptron. We update our model by changing the weights and using the formula: \n",
    "- weight(n + 1) = weight(n) + (learning_rate * input * error)\n",
    "\n",
    "The `learning rate` of neural networks is typically a fixed value of low increment. eg. 0.1, 0.01, 0.001 etc. This parameter indicates the speed at which the network will learn how much the value of the weights will be changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Learning Rate\n",
    "\n",
    "keeping the same example going we will use the sample from our previous test which was wrong. \n",
    "- $(1 \\cdot 0) + (1 \\cdot 0) = 0$ \n",
    "\n",
    "To update the weights in line with the learning rate `(0.1)` we need to apply the formula to both sides of sample, $x1, x2$. \n",
    "- $x1$ weight$(n+1) = 0 + (0.1 \\cdot 1 \\cdot 1)$\n",
    "- $x1$ weight$(n+1) = 0.1$\n",
    "- $x2$ weight$(n+1) = 0 + (0.1 \\cdot 1 \\cdot 1)$\n",
    "- $x2$ weight$(n+1) = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**important note** the weight shifts for _all_ examples, not just the sample that are in error, so our example above has changed the entire calculations applied across all 4 samples in our dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new weight is `0.1`. This is applied and a new result is calculated and passed to the step function. \n",
    "```python\n",
    "def step_function(sum):\n",
    "    if sum >= 1: return 1\n",
    "    return 0\n",
    "```\n",
    "\n",
    "- $0 \\cdot 0.1 + 0 \\cdot 0.1$  or $0 + 0 = 0$ _(<1 = 0)_\n",
    "\n",
    "- $0 \\cdot 0.1 + 1 \\cdot 0.1$  or $0 + 0.1 = 0.1$ _(<1 = 0)_\n",
    "\n",
    "- $1 \\cdot 0.1 + 0 \\cdot 0.1$  or $0.1 + 0 = 0.1$ _(<1 = 0)_\n",
    "\n",
    "- $1 \\cdot 0.1 + 1 \\cdot 0.1$  or $0.1 + 0.1 = 0.2$ _(<1 = 0)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an epoch?\n",
    "We have incremented the weights, but still achieved the same result and still have a 75% accuracy. Each iteration of this weight shifting process is called an **epoch**.\n",
    "\n",
    "### Jumping forward to _n_th epoch\n",
    "Let's jump forward to after **5 epochs** for our example. \n",
    "- $0 \\cdot 0.5 + 0 \\cdot 0.5$  or $0 + 0 = 0$ _(<1 = 0)_\n",
    "\n",
    "- $0 \\cdot 0.5 + 1 \\cdot 0.5$  or $0 + 0.5 = 0.5$ _(<1 = 0)_\n",
    "\n",
    "- $1 \\cdot 0.5 + 0 \\cdot 0.5$  or $0.5 + 0 = 0.5$ _(<1 = 0)_\n",
    "\n",
    "- $1 \\cdot 0.5 + 1 \\cdot 0.5$  or $0.5 + 0.5 = 1.0$ _(1 = 1) the desired answer_\n",
    "\n",
    "### Updating the Error table \n",
    "\n",
    "| class | prediction | error |\n",
    "| -- |--- |-------|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 0  | 0 |\n",
    "| 1  | 1  | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary notes\n",
    "\n",
    "We can say that the intention of the Perceptron is to find weights that are shared between all instances in the dataset in order to correctly classify all instances (_or most instances_). Given we have a tiny dataset in this example we have found a way to get to 100% correct classification but that is not always possible and the goal is to find the best weights where perfect ones cannot be identified.  \n",
    "\n",
    "_As an aside... It's probably worth noting the scale of the problem here, we have a simple Perceptron with two inputs, which we have admitted is tiny. In a commercial application setting the size of an input selection will be significantly larger. For example imagine we are doing image classification on an image of `800 x 600` pixels. Each pixel would be an input and this means for that one image we have `480,000` inputs alone. Now we are applying the training weight shifts across all of those._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "# Implementing the Learning Rate with `binary and` operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement the learning rates weight adjustments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the inputs we will use a matrix format instead of a vector format. \n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the inputs \n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check outputs shape \n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the weights. Given we have a two input perceptron\n",
    "# we need a two-weight vector. \n",
    "weights = np.array([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the learning rate, ie the parameter that dictates \n",
    "# the learning speed increments. \n",
    "learning_rate = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the step function to determine\n",
    "# an artificail being fired or not. \n",
    "def step_function(sum):\n",
    "    if sum >= 1: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the calculate output functions\n",
    "def calculate_output(instance):\n",
    "    s = instance.dot(weights)\n",
    "    return step_function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([[0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training steps (weight updates)\n",
    "def train():\n",
    "    total_error = 1\n",
    "    while total_error != 0:\n",
    "        total_error = 0\n",
    "        for i in range(len(outputs)):\n",
    "            prediction = calculate_output(inputs[i])\n",
    "            error = abs(outputs[i] - prediction)\n",
    "            total_error += error\n",
    "            \n",
    "            if error > 0:\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] = weights[j] + (learning_rate * inputs[i][j] * error)\n",
    "                    print(\"Weighting updated: \" + str(weights[j]))\n",
    "        print('Total error: ' + str(total_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting updated: 0.1\n",
      "Weighting updated: 0.1\n",
      "Total error: 1\n",
      "Weighting updated: 0.2\n",
      "Weighting updated: 0.2\n",
      "Total error: 1\n",
      "Weighting updated: 0.30000000000000004\n",
      "Weighting updated: 0.30000000000000004\n",
      "Total error: 1\n",
      "Weighting updated: 0.4\n",
      "Weighting updated: 0.4\n",
      "Total error: 1\n",
      "Weighting updated: 0.5\n",
      "Weighting updated: 0.5\n",
      "Total error: 1\n",
      "Total error: 0\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "\n",
    "# Implementing the Learning Rate with `binary or` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the inputs we will again use a matrix \n",
    "# format instead of a vector format. \n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using a binary or operator in this example\n",
    "# that means the check will return a positive if\n",
    "# either input of the peceptron is positive \n",
    "outputs = np.array([0,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialised as zeros once more\n",
    "weights = np.array([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "learning_rate = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(sum):\n",
    "    if sum >= 1: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the calculate output functions (same as for binary and)\n",
    "def calculate_output(instance):\n",
    "    s = instance.dot(weights)\n",
    "    return step_function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training steps (weight updates, same as for binary and)\n",
    "def train():\n",
    "    total_error = 1\n",
    "    while total_error != 0:\n",
    "        total_error = 0\n",
    "        for i in range(len(outputs)):\n",
    "            prediction = calculate_output(inputs[i])\n",
    "            error = abs(outputs[i] - prediction)\n",
    "            total_error += error\n",
    "            \n",
    "            if error > 0:\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] = weights[j] + (learning_rate * inputs[i][j] * error)\n",
    "                    print(\"Weighting updated: \" + str(weights[j]))\n",
    "        print('Total error: ' + str(total_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting updated: 0.0\n",
      "Weighting updated: 0.1\n",
      "Weighting updated: 0.1\n",
      "Weighting updated: 0.1\n",
      "Weighting updated: 0.2\n",
      "Weighting updated: 0.2\n",
      "Total error: 3\n",
      "Weighting updated: 0.2\n",
      "Weighting updated: 0.30000000000000004\n",
      "Weighting updated: 0.30000000000000004\n",
      "Weighting updated: 0.30000000000000004\n",
      "Weighting updated: 0.4\n",
      "Weighting updated: 0.4\n",
      "Total error: 3\n",
      "Weighting updated: 0.4\n",
      "Weighting updated: 0.5\n",
      "Weighting updated: 0.5\n",
      "Weighting updated: 0.5\n",
      "Total error: 2\n",
      "Weighting updated: 0.5\n",
      "Weighting updated: 0.6\n",
      "Weighting updated: 0.6\n",
      "Weighting updated: 0.6\n",
      "Total error: 2\n",
      "Weighting updated: 0.6\n",
      "Weighting updated: 0.7\n",
      "Weighting updated: 0.7\n",
      "Weighting updated: 0.7\n",
      "Total error: 2\n",
      "Weighting updated: 0.7\n",
      "Weighting updated: 0.7999999999999999\n",
      "Weighting updated: 0.7999999999999999\n",
      "Weighting updated: 0.7999999999999999\n",
      "Total error: 2\n",
      "Weighting updated: 0.7999999999999999\n",
      "Weighting updated: 0.8999999999999999\n",
      "Weighting updated: 0.8999999999999999\n",
      "Weighting updated: 0.8999999999999999\n",
      "Total error: 2\n",
      "Weighting updated: 0.8999999999999999\n",
      "Weighting updated: 0.9999999999999999\n",
      "Weighting updated: 0.9999999999999999\n",
      "Weighting updated: 0.9999999999999999\n",
      "Total error: 2\n",
      "Weighting updated: 0.9999999999999999\n",
      "Weighting updated: 1.0999999999999999\n",
      "Weighting updated: 1.0999999999999999\n",
      "Weighting updated: 1.0999999999999999\n",
      "Total error: 2\n",
      "Total error: 0\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1, 1.1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the weights, post training \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_output(np.array([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example results\n",
    "Taking out weights we can see: \n",
    "- inputs (0,0) = $(0 \\cdot1.1) + (0 \\cdot1.1) = (0 + 0) = 0$\n",
    "- inputs (0,1) = $(0 \\cdot1.1) + (1 \\cdot1.1) = (0 + 1.1) = (1.1 > 1) = 1$\n",
    "- inputs (1,0) = $(1 \\cdot1.1) + (0 \\cdot1.1) = (1.1 + 0) = (1.1 > 1) = 1$\n",
    "- inputs (1,1) = $(1 \\cdot1.1) + (1 \\cdot1.1) = (1.1 + 1.1) = (2.2 > 1) = 1$\n",
    "\n",
    "\n",
    "#### Summary notes \n",
    "Although a very simple, trivial neural network we have already managed to make some interesting classifications. We can highlight that different datasets can require different weights. In the comparison of examples between the `binary and` & `binary or` operators we can see the weights of `0.5` and `1.1` respectively. What we can also highlight is that in our examples we have matched weights between inputs 1 and 2. This is not typical in real world (more complex datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3\n",
    "\n",
    "# Implementing Learning Rates with the `binary xor` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "\n",
    "# we are using a binary xor operator in this example\n",
    "# that means the check will return a positive if\n",
    "# either input of the peceptron is positive BUT NOT\n",
    "# if both are positive \n",
    "outputs = np.array([0,1,1,0])\n",
    "\n",
    "# Weights initialised as zeros once more\n",
    "weights = np.array([0.0, 0.0])\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(sum):\n",
    "    if sum >= 1: return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the calculate output functions (same as for binary and)\n",
    "def calculate_output(instance):\n",
    "    s = instance.dot(weights)\n",
    "    return step_function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the implementation of epochs control as simply creating the training \n",
    "# steps with (weight updates, same as for binary and and binary or) as we did\n",
    "# in exaples 1 & 2 will set off an infinite unresolved loop whic will eventually\n",
    "# segment fault, and crash the machine as the memory os depleted. \n",
    "def train():\n",
    "    total_error = 1\n",
    "    while total_error != 0:\n",
    "        total_error = 0\n",
    "        for i in range(len(outputs)):\n",
    "            prediction = calculate_output(inputs[i])\n",
    "            error = abs(outputs[i] - prediction)\n",
    "            total_error += error\n",
    "            \n",
    "            if error > 0:\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] = weights[j] + (learning_rate * inputs[i][j] * error)\n",
    "                    print(\"Weighting updated: \" + str(weights[j]))\n",
    "        print('Total error: ' + str(total_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see in the fault above is that for `binary and` and `binary or` they are **_linearly separable_** problems. The `binary xor` is not - it is a **_non-linearly separable_** problem. This is a drawback of the single layer perceptron neural network, it is capable of working on linearly separable problems only. This typically means simple problems with a strong correlation between the inputs and the weights. \n",
    "\n",
    "In order to solve the `binary xor` non-linearly separable problem we are required to use more complex networks. This is basically a `multi-layer perceptron`, or `many layers perceptron`. This will be the subject of the next lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional reading resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [\"A logical calculus of the ideias immanent in nervous activity\"](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)\n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
