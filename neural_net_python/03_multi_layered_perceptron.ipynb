{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layered Perceptron\n",
    "\n",
    "![Multi layered perceptron](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - What is a multilayer Neural Network\n",
    "- 3.1.1 - An introduction to Multilayer Neural Networks\n",
    "- 3.1.2 - The Hidden-Layer (An example using the binary `xor` operator)\n",
    "\n",
    "\n",
    "### Part 2 - Let's make a prediction\n",
    "- 3.2.1 - Make a prediction step by step \n",
    "    - 3.2.1.1 - Define the values fed forward as `inputs`.\n",
    "    - 3.2.1.2 - Define the multiplier factors as `weights0` and `weights1`\n",
    "    - 3.2.1.3 - Calculate the product $(inputs \\cdot weights0)$ as `sum_synapse0`\n",
    "    - 3.2.1.4 - Calculate the `sigmoid(sum_synapse0)` as the `hidden_layer`\n",
    "    - 3.2.1.5 - Calculate the hidden_layer * weights1 as the `sum_synpase1`\n",
    "    - 3.2.1.6 - Calculate the predictions of `sigmoid(sum_synpase1)` as the `output_layer`\n",
    "    \n",
    "\n",
    "### Recap\n",
    "**Checkpoint reached** Let's have a manual walkthrough \n",
    "\n",
    "### Part 3\n",
    "- 3.3.1 - Improving on predictions\n",
    "    - 3.3.1.0 - Theory of cost, deltas, Weights adjustment with gradient descent and backpropagation\n",
    "    - 3.3.1.1 - Calculate the `error_output_layer`(error, cost & loss functions)\n",
    "    - 3.3.1.2 - Calculate the partial derivatives (`sigmoid_derivative` functions)\n",
    "    - 3.3.1.3 - Calculate the `delta_output`\n",
    "    - 3.3.1.4 - Transpose `weights1` to enable a `delta_output_weight_multiplier`\n",
    "    - 3.3.1.5 - Calculate the `delta_output_weights_multiplier`\n",
    "    - 3.3.1.6 - Calculate the `hidden_layer_deltas` # 3.3.1 - Improving on predictions\n",
    "\n",
    "### Part 4 - Backpropagation of new weights values\n",
    "- 3.4.1 - Application of Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 - An introduction to Multilayer Neural Networks\n",
    "\n",
    "#### Q: What is a Multi-layer Neural network?\n",
    "**A:** In the depiction at the top of the workbook we can see the main concepts of a multilayer perceptron. The most basic principles are: \n",
    "\n",
    "- **There is at least 1 hidden layer**. \n",
    "- Each neuron in the hidden layer should have its own `sum function` & `activation function`. \n",
    "- The elements are all connected! Which means in the case of the first hidden layer that each neuron is connected to each of the inputs. In the case of subsequent hidden layers each neuron is connected to each neuron of the preceding layer, again each having its own `sum function` and `activation function`.  \n",
    "- Our structure will conclude with an output layer. This is fed the results of each of the neurons in the final hidden layer and **this result is the prediction** of our neural network. \n",
    "- This `prediction` is evaluated and we adjust the weights to see if we determine a better outcome. Each iteration of this cycle is called an `epoch`. \n",
    "\n",
    "#### Q: What is an epoch and why do we set an epochs limit?\n",
    "**A:** We set the epochs limit to control the amount of times we'll allow the algorithm to run. Epoch thresholds are used to prevent infinite loops in search of a perfect prediction because in a lot of cases in machine learning we will not achieve a 100% perfect algorithm. I guess we can take some clues from the fact we use ML to make `predictions` of outcomes rather than determine factual outcomes.\n",
    "\n",
    "#### Q: What is a `sum function`?\n",
    "**A:** The `sum function` is the result of multiplying an input value by the associated weight. In a single layer perceptron that means, a single sum and activation function for the inputs. In a multilayer perceptron where we can have `_n_ layers` that means the sum can be made of the multiplier by the inputs and the weights in the case of the first hidden layer and for subsequent layers it can be the sum of the neuron in that preceding layer multiplied by another weight between hidden layers.   \n",
    "#### Q: What is an `activation function`?\n",
    "**A:** The `activation function` is the decision fork of evaluating a sum and deciding if that neuron is fired or not. In the single layer perceptron we seen a `step function` type of activation function. A step can have values of `0` or `1`. Another type of activation function is the `sigmoid function`. What is different here is that the result or activation can be between `0` and `1` and not stepped. That ability to touch all points between `0` and `1` means we need to work out exactly where on the line that value belongs. \n",
    "  \n",
    "#### Q: What if I need to return negative values?  \n",
    "**A:** If we need to return negative values we can use the `hyperbolic tangent function` which looks like this: $y = \\frac{e^{x} - e^{-x} }{e^{x} + e^{-x}}$ evaluating the equation asks to replace the `x` with the value under evaluation and the return will be graded between `-1` & `1`. \n",
    "\n",
    "#### Q: Any additional information worth remembering?\n",
    "**A:** The irrational number `e` is also known as `Eulerâ€™s number`. It is approximately 2.718281, and is the base of the natural logarithm, ln (this means that, if $x = l_n y = \\log_e y$, then $e^x = y$. In a `sigmoid function` We apply the following equation: $y = \\frac{1}{1 + e^{-x}}$ to determine:\n",
    "- if `x` is high, the value lies closer to, or equal to 1. \n",
    "- if `x` is low , the value lies closer to, or equal to 0.  \n",
    "\n",
    "# 3.1.2 - The Hidden-Layer (An example using the binary `xor` operator)\n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/coa/images/logic-gates5.png)\n",
    "\n",
    "We will use the 'XOR' operator as our case for the multi-layer study. The following truth table used as reference. We will focus on the `feed-forward` process from the input layer to the hidden layer. We can declare the following upfront: \n",
    "- We have 2 inputs (x,y) as we have had all along.\n",
    "- We have 3 neurons in our hidden layer, with individual sum and activation functions, of course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "For additional information on the `sigmoid` function see [here](https://en.wikipedia.org/wiki/Sigmoid_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 - Make a prediction step by step\n",
    "\n",
    "### Determining a process\n",
    "\n",
    "- 3.2.1.0 - Imports and Setup\n",
    "- 3.2.1.1 - Define the inputs, outputs (feed forward values)\n",
    "- 3.2.1.2 - Define the multiplier factors as `weights0` & `weights1`\n",
    "- 3.2.1.3 - Calculate the product $(inputs \\cdot weights0)$ as `sum_synapse0`\n",
    "- 3.2.1.4 - Calculate the `sigmoid(sum_synapse0)` as the `hidden_layer` \n",
    "- 3.2.1.5 - Calculate the hidden_layer * weights1 as the `sum_synpase1`\n",
    "- 3.2.1.6 - Calculate the predictions of `sigmoid(sum_synpase1)` as the `output_layer` or `prediction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.0 - Imports, Setup & Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the imports \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of a sigmoid function. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the learning rate\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.1 - Define the inputs, outputs (feed forward values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the inputs data \n",
    "input_layer = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "input_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_outputs = np.array([[0], [1], [1], [0]])\n",
    "expected_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.2 - Define the multiplier factors as `weights0` and `weights1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights0 \n",
    "# These are the input-X weights & input-Y weights respectively \n",
    "# they are the weights used in the sum function to generate a \n",
    "# product value. \n",
    "weights0 = np.array([[-0.424, -0.740, -0.961], \n",
    "                     [0.358, -0.577, -0.469]])\n",
    "\n",
    "\n",
    "# weights1 \n",
    "# these are hardcoded in the class lecture of this example. \n",
    "# They are the weights we see used between the hidden layer\n",
    "# and the output layer at the end of our operation. The \n",
    "# figures used are for demonstration purposes so don't get \n",
    "# hung up on what these particular numbers mean. They are \n",
    "# just demo weights. \n",
    "weights1 = np.array([[-0.017], \n",
    "                     [-0.893], \n",
    "                     [0.148]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epochs limit.\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.3 - Calculate the product $(inputs \\cdot weights0)$ as `sum_synapse0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.358, -0.577, -0.469],\n",
       "       [-0.424, -0.74 , -0.961],\n",
       "       [-0.066, -1.317, -1.43 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starts the sum of the communication between the input layer and the \n",
    "# hidden layer. This is basically a matrix multiplication exercise. \n",
    "# Creating the results of: \n",
    "#     \"for each input_layer * each weights0\"  \n",
    "# Note: using the np.dot is more optimised than using a for loop.\n",
    "\n",
    "# sum_synapse0 is what we called product above the name sum_synapse0 \n",
    "# is more descriptive because it is the sum of the synapse at level 0\n",
    "# which is the first layer, or the input to hidden layer. \n",
    "sum_synapse0 = np.dot(input_layer, weights0)\n",
    "sum_synapse0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.4 - Calculate the `sigmoid(sum_synapse0)` as the `hidden_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the hidden layer values, these are the values\n",
    "# returned from the sigmoid of the sum_synapse0\n",
    "\n",
    "hidden_layer = sigmoid(sum_synapse0)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.5 - Calculate the hidden_layer * weights1 as the `sum_synpase1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.381     ],\n",
       "       [-0.27419072],\n",
       "       [-0.25421887],\n",
       "       [-0.16834784]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the sum_synapse1 values. These are the values that \n",
    "# are generated from the hidden layer to the output layer \n",
    "# and are considered as the final results of the neural \n",
    "# network for each of the items in our dataset. \n",
    "sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "sum_synapse1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1.6 - Calculate the predictions of `sigmoid(sum_synpase1)` as the `output_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net Prediction\t\tRounded\n",
      "----------------------------------------\n",
      "0.40588573188433286\t\t0\n",
      "0.43187856951314224\t\t0\n",
      "0.43678536461116163\t\t0\n",
      "0.4580121591884929\t\t0\n"
     ]
    }
   ],
   "source": [
    "# We create the output layer or as it's often called the prediction of the\n",
    "# neural network by applying the sigmoid to the sum_synapse1 value. \n",
    "output_layer  = sigmoid(sum_synapse1)\n",
    "\n",
    "print(\"Neural net Prediction\\t\\tRounded\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(len(output_layer)):\n",
    "    print(f\"{float(output_layer[i])}\\t\\t{round(float(output_layer[i]))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results of the `sigmoid` from applying the inputs and weights we apply the `sum function` and `activation function` again to achieve the `sum_synapse1` or outputs and the `sigmoid(output)` will give us the neural network's prediction. Let's work through a manual calculation process to increase our comprehension before adding more complexity to our python implementation.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1wfrCqtzef-qBymeaeIApeI_9VlxlbV0E\" alt=\"I predict\" width=\"600\" height=\"400\"></center>\n",
    "<br>\n",
    "\n",
    "\n",
    "<h1><center><i>We made a prediction!</i></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET'S HAVE A RECAP\n",
    "\n",
    "#### Manual calculation of output results\n",
    "\n",
    "There's a lot to this process so let's see a timeline of the steps up until now, from input to prediction. This is effectively one iteration of our network's capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_step(stepname):\n",
    "    print(f\"\\n[Step Definition] {stepname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionised the descriptor of the process up until this point.\n",
    "# func accepts the index of the instance\n",
    "def processing_description(i_idx):\n",
    "\n",
    "    define_step(\"Inputs Passing\")\n",
    "    input1 = input_layer[i_idx][0]\n",
    "    input2 = input_layer[i_idx][1]\n",
    "    print(f\"\\t[001.001] Input1: {input1}\")\n",
    "    print(f\"\\t[001.002] Input2: {input2}\")\n",
    "\n",
    "    define_step(\"weights0 application\")\n",
    "    for n_idx in range(3):\n",
    "        in1_calc = f\"({input1} * {weights0[:,n_idx][0]})\"\n",
    "        in2_calc = f\"({input2} * {weights0[:,n_idx][1]})\"\n",
    "        calc = sum_synapse0[i_idx][n_idx]\n",
    "        print(f\"\\t[002.00{n_idx}] sum function for neuron{n_idx}: {in1_calc} + {in2_calc} = {calc}\")\n",
    "\n",
    "    define_step(\"Apply sigmoid to get activation function values\")\n",
    "    for n_idx in range(3):\n",
    "        sum_desc = f\"sigmoid({sum_synapse0[i_idx][n_idx]})\"\n",
    "        sum_val = hidden_layer[i_idx][n_idx]\n",
    "        print(f\"\\t[003.00{n_idx}] Activation value neuron{n_idx}: {sum_desc} = {sum_val}\")\n",
    "\n",
    "    define_step(\"Perform neuron calculations (activation * weight1) to generate sum_synapse1\")\n",
    "    for n_idx in range(3):\n",
    "        sum_desc = f\"{hidden_layer[i_idx][n_idx]} * {float(weights1[n_idx])}\"\n",
    "        sum_val = hidden_layer[i_idx][n_idx] * float(weights1[n_idx])\n",
    "        print(f\"\\t[004.001] sum_synpase1 for neuron{n_idx}: {sum_desc} = {sum_val}\")\n",
    "\n",
    "    define_step(\"Get output as sum of sum_synpase1\")\n",
    "    calc_0 = f\"({hidden_layer[i_idx][0]} * {float(weights1[0])})\"\n",
    "    calc_1 = f\"({hidden_layer[i_idx][1]} * {float(weights1[1])})\"\n",
    "    calc_2 = f\"({hidden_layer[i_idx][2]} * {float(weights1[2])})\"\n",
    "    desc = f\"{calc_0} + {calc_1} + {calc_2}\"\n",
    "    ss1 = float((hidden_layer[i_idx][0] * weights1[0]) + (hidden_layer[i_idx][1] * weights1[1]) + (hidden_layer[i_idx][2] * weights1[2]))\n",
    "    print(f\"\\t[005.001] describe ouput calculation: {desc}\")\n",
    "    print(f\"\\t[005.002] Sum the sum_synpase1 as Output: {ss1}\")\n",
    "\n",
    "    define_step(\"Apply sigmoid to output totals to create prediction\")\n",
    "    print(f\"\\t[006.001] sigmoid(output): {ss1}\")\n",
    "    print(f\"\\t[006.002] Neural network prediction: {float(sigmoid(ss1))}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step Definition] Inputs Passing\n",
      "\t[001.001] Input1: 0\n",
      "\t[001.002] Input2: 0\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "\t[002.000] sum function for neuron0: (0 * -0.4240241433554733) + (0 * 0.35797265188542493) = 0.0\n",
      "\t[002.001] sum function for neuron1: (0 * -0.7430403770354519) + (0 * -0.5805236465810291) = 0.0\n",
      "\t[002.002] sum function for neuron2: (0 * -0.9605554161304843) + (0 * -0.4683213369656219) = 0.0\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "\t[003.000] Activation value neuron0: sigmoid(0.0) = 0.5\n",
      "\t[003.001] Activation value neuron1: sigmoid(0.0) = 0.5\n",
      "\t[003.002] Activation value neuron2: sigmoid(0.0) = 0.5\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "\t[004.001] sum_synpase1 for neuron0: 0.5 * -0.017 = -0.0085\n",
      "\t[004.001] sum_synpase1 for neuron1: 0.5 * -0.893 = -0.4465\n",
      "\t[004.001] sum_synpase1 for neuron2: 0.5 * 0.148 = 0.074\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "\t[005.001] describe ouput calculation: (0.5 * -0.017) + (0.5 * -0.893) + (0.5 * 0.148)\n",
      "\t[005.002] Sum the sum_synpase1 as Output: -0.381\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "\t[006.001] sigmoid(output): -0.381\n",
      "\t[006.002] Neural network prediction: 0.40588573188433286\n"
     ]
    }
   ],
   "source": [
    "processing_description(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step Definition] Inputs Passing\n",
      "\t[001.001] Input1: 0\n",
      "\t[001.002] Input2: 1\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "\t[002.000] sum function for neuron0: (0 * -0.4240241433554733) + (1 * 0.35797265188542493) = 0.358\n",
      "\t[002.001] sum function for neuron1: (0 * -0.7430403770354519) + (1 * -0.5805236465810291) = -0.577\n",
      "\t[002.002] sum function for neuron2: (0 * -0.9605554161304843) + (1 * -0.4683213369656219) = -0.469\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "\t[003.000] Activation value neuron0: sigmoid(0.358) = 0.5885562043858291\n",
      "\t[003.001] Activation value neuron1: sigmoid(-0.577) = 0.3596231853677901\n",
      "\t[003.002] Activation value neuron2: sigmoid(-0.469) = 0.38485295749078957\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "\t[004.001] sum_synpase1 for neuron0: 0.5885562043858291 * -0.017 = -0.010005455474559095\n",
      "\t[004.001] sum_synpase1 for neuron1: 0.3596231853677901 * -0.893 = -0.32114350453343654\n",
      "\t[004.001] sum_synpase1 for neuron2: 0.38485295749078957 * 0.148 = 0.05695823770863685\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "\t[005.001] describe ouput calculation: (0.5885562043858291 * -0.017) + (0.3596231853677901 * -0.893) + (0.38485295749078957 * 0.148)\n",
      "\t[005.002] Sum the sum_synpase1 as Output: -0.2741907222993588\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "\t[006.001] sigmoid(output): -0.2741907222993588\n",
      "\t[006.002] Neural network prediction: 0.43187856951314224\n"
     ]
    }
   ],
   "source": [
    "processing_description(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "        [001.001] Input1: 1\n",
      "        [001.002] Input2: 0\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "        [002.000] sum function for neuron0: (1 * -0.424) + (0 * 0.358) = -0.424\n",
      "        [002.001] sum function for neuron1: (1 * -0.74) + (0 * -0.577) = -0.74\n",
      "        [002.002] sum function for neuron2: (1 * -0.961) + (0 * -0.469) = -0.961\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "        [003.000] Activation value neuron0: sigmoid(-0.424) = 0.39555998258063735\n",
      "        [003.001] Activation value neuron1: sigmoid(-0.74) = 0.323004143761477\n",
      "        [003.002] Activation value neuron2: sigmoid(-0.961) = 0.2766780228949468\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "        [004.001] sum_synpase1 for neuron0: 0.39555998258063735 * -0.017 = -0.006724519703870836\n",
      "        [004.001] sum_synpase1 for neuron1: 0.323004143761477 * -0.893 = -0.288442700378999\n",
      "        [004.001] sum_synpase1 for neuron2: 0.2766780228949468 * 0.148 = 0.04094834738845213\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "        [005.001] describe ouput calculation: (0.39555998258063735 * -0.017) + (0.323004143761477 * -0.893) + (0.2766780228949468 * 0.148)\n",
      "        [005.002] Sum the sum_synpase1 as Output: -0.2542188726944177\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "        [006.001] sigmoid(output): -0.2542188726944177\n",
      "        [006.002] Neural network prediction: 0.43678536461116163\n"
     ]
    }
   ],
   "source": [
    "processing_description(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "        [001.001] Input1: 1\n",
      "        [001.002] Input2: 1\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "        [002.000] sum function for neuron0: (1 * -0.424) + (1 * 0.358) = -0.066\n",
      "        [002.001] sum function for neuron1: (1 * -0.74) + (1 * -0.577) = -1.317\n",
      "        [002.002] sum function for neuron2: (1 * -0.961) + (1 * -0.469) = -1.43\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "        [003.000] Activation value neuron0: sigmoid(-0.066) = 0.4835059868921233\n",
      "        [003.001] Activation value neuron1: sigmoid(-1.317) = 0.21131784831127748\n",
      "        [003.002] Activation value neuron2: sigmoid(-1.43) = 0.19309868423321644\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "        [004.001] sum_synpase1 for neuron0: 0.4835059868921233 * -0.017 = -0.008219601777166097\n",
      "        [004.001] sum_synpase1 for neuron1: 0.21131784831127748 * -0.893 = -0.1887068385419708\n",
      "        [004.001] sum_synpase1 for neuron2: 0.19309868423321644 * 0.148 = 0.02857860526651603\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "        [005.001] describe ouput calculation: (0.4835059868921233 * -0.017) + (0.21131784831127748 * -0.893) + (0.19309868423321644 * 0.148)\n",
      "        [005.002] Sum the sum_synpase1 as Output: -0.16834783505262085\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "        [006.001] sigmoid(output): -0.16834783505262085\n",
      "        [006.002] Neural network prediction: 0.4580121591884929\n"
     ]
    }
   ],
   "source": [
    "processing_description(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO: Summary Table of the `inputs`, `network output`, `prediction value`, rounded(`prediction`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs\tInstance Index\tInstance Output Calculation\tPrediction\tRounded_prediction\n",
      "------------------------------------------------------------------------------------------\n",
      "(0,0)\t0\t\t-0.381\t\t\t\t0.40588573\t0\n",
      "(0,1)\t1\t\t-0.274\t\t\t\t0.43187857\t0\n",
      "(1,0)\t2\t\t-0.254\t\t\t\t0.43678537\t0\n",
      "(1,1)\t3\t\t-0.168\t\t\t\t0.45801216\t0\n"
     ]
    }
   ],
   "source": [
    "# for all the arrays out hidden_layer values show a summary of above \n",
    "# processes to have the neuron calculations (or outputs) and the \n",
    "# predictions handy for reference. \n",
    "arrs = np.array([\n",
    "        [0.5       , 0.5       , 0.5       ],\n",
    "        [0.5885562 , 0.35962319, 0.38485296],\n",
    "        [0.39555998, 0.32300414, 0.27667802],\n",
    "        [0.48350599, 0.21131785, 0.19309868]])\n",
    "\n",
    "print(\"Inputs\\tInstance Index\\tInstance Output Calculation\\tPrediction\\tRounded_prediction\")\n",
    "print(\"-\" * 90)\n",
    "for i in range(len(arrs)):\n",
    "    x,y = input_layer[i]\n",
    "    arr = arrs[i]\n",
    "    output_calculations = float(arr.dot(weights1))\n",
    "    prediction = sigmoid(output_calculations)\n",
    "\n",
    "    print(f\"({x},{y})\\t{i}\\t\\t{round(output_calculations, 3)}\\t\\t\\t\\t{round(prediction, 8)}\\t{round(prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.1 - Improving on predictions\n",
    "\n",
    " \n",
    "### Theory of `Cost`, `Deltas`, `Weights` and `Gradient Descent`. \n",
    "\n",
    "Improving on our predictions requires us to run further epochs and update our weights to get a better result. In order to optimise for best results here we need to look at `error functions`, then the `gradient descent`, the `calculation of deltas` and `backpropogation`. Loads of jazzy words there so lets get about it. \n",
    "\n",
    "#### Q: What is the error function, or cost/loss function?\n",
    "**A:** we calculate the error (_cost or loss_) by comparing the results of the predictions with the outputs of the dataset. The simplest formula to calculate the `error` is `error = correct - prediction`.This is one we will use. For additional info on error functions see [here](https://en.wikipedia.org/wiki/Error_function)\n",
    "\n",
    "\n",
    "#### Q: What is Gradient Descent?\n",
    "**A:** The idea of gradient descent is to manage our cost function `(loss function, error function)` to get to the **smallest possible error** in the adjustment of the weights. The directional control of how a weight set should be adjusted is done by calculating the partial derivative as a means of determining the direction of a gradient. \n",
    "\n",
    "For the **Partial Derivative** Assuming that `y` = 0.1 we get $d = 0.1 \\cdot (1 -0.1)$\n",
    "\n",
    "```python\n",
    " def sigmoid_derivative(sigmoid_value):\n",
    "        return sigmoid_value * (1 - sigmoid_value)\n",
    "```\n",
    "- reminder of the sigmoid function formula: $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- calculating the partial derivative formula: $d = y \\cdot (1 -y)$\n",
    "\n",
    "You can read more about gradient descent [here](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "#### Q: What is the Delta (of the output layer)?\n",
    "**A:** Deriving the delta is basically a sequenced order of events.\n",
    "\n",
    "- activation function (sigmoid) $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- Derivative $d = y \\cdot (1 -y)$\n",
    "\n",
    "- Delta $delta _{output} = error \\cdot sigmoid _{derivative}$\n",
    "\n",
    "- Gradient\n",
    "\n",
    "\n",
    "#### Q: What is Backpropagation?\n",
    "**A:** TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.1 - Calculate the `error_output_layer`(error, cost & loss functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the errors (error = correct - preditions) as detailed above \n",
    "error_output_layer = expected_outputs - output_layer\n",
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error average: 0.49880848923713045\n",
      "Error average (rounded) : 0.499\n"
     ]
    }
   ],
   "source": [
    "# In order to get the average error we need to heed a reminder: \n",
    "#    ** we need to use the absolute values **\n",
    "# of the errors. If this is overlooked we will skew the results. \n",
    "\n",
    "error_avg = np.mean(abs(error_output_layer))\n",
    "print(f\"Error average: {error_avg}\")\n",
    "print(f\"Error average (rounded) : {round(error_avg, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.2 - Calculate the partial derivatives (`sigmoid_derivative` functions)\n",
    "\n",
    "To calculate the `derivative_output` we need to get the result of the `sigmoid_derivative(output_layer)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to calculate the partial derivative \n",
    "def sigmoid_derivative(sigmoid_value):\n",
    "    return sigmoid_value * (1 - sigmoid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2411425 ],\n",
       "       [0.24535947],\n",
       "       [0.24600391],\n",
       "       [0.24823702]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the partial derivatives\n",
    "derivative_output = sigmoid_derivative(output_layer)\n",
    "derivative_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.3 - Calculate the `Delta_output`\n",
    "\n",
    "As a reminder: \n",
    "- The delta is used to determine the direction of the gradient in order to update weights. \n",
    "- To calculate the delta output we need to take the `error_output_layer` and multiply by the `derivative_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the deltas \n",
    "delta_output = error_output_layer * derivative_output\n",
    "delta_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$delta _{hidden} = sigmoid _{derivative} \\cdot weight \\cdot delta _{output}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.4 - Transpose the `weights1` to enable `delta_output_weights_multiplier`\n",
    "\n",
    "- The `T` denotes `transposed`, where columns become rows and rows become columns. \n",
    "- This is basically a total reshaping allowing us to continue with the weights array as a usable multiplier because if we attempt to calc the `delta_output.dot(weights1)` we will have a shape mismatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017, -0.893,  0.148]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1T = weights1.T\n",
    "weights1T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights1     : (3, 1)\n",
      "Shape of weights1T    : (1, 3)\n",
      "Shape of delta_output : (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# check shapes \n",
    "print(f\"Shape of weights1     : {weights1.shape}\")\n",
    "print(f\"Shape of weights1T    : {weights1T.shape}\")\n",
    "print(f\"Shape of delta_output : {delta_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.5 - Create the `delta_output_weight_multiplier` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_output\t* -0.017\t* -0.893\t* 0.143\n",
      "------------------------------------------------------------\n",
      "[-0.0978763]\t[0.0016639]\t[0.08740354]\t[-0.01448569]\n",
      "[0.13939397]\t[-0.0023697]\t[-0.12447882]\t[0.02063031]\n",
      "[0.138553]\t[-0.0023554]\t[-0.12372783]\t[0.02050584]\n",
      "[-0.11369557]\t[0.00193282]\t[0.10153015]\t[-0.01682694]\n"
     ]
    }
   ],
   "source": [
    "# we will use the numpy library to make the calculation more efficient\n",
    "# but this is the matrix we will create and demontrates the purpose of\n",
    "# transposing the weights above. \n",
    "# we can sucessfuly perform matric multiplication on out values now to\n",
    "# generate the delta_output * weights multiplier. \n",
    "\n",
    "\n",
    "print(f\"delta_output\\t* -0.017\\t* -0.893\\t* 0.143\")\n",
    "print(\"-\" * 60)\n",
    "for i in delta_output:\n",
    "    print(f\"{i}\\t{i * -0.017}\\t{i * -0.893}\\t{i * 0.148}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0016639 ,  0.08740354, -0.01448569],\n",
       "       [-0.0023697 , -0.12447882,  0.02063031],\n",
       "       [-0.0023554 , -0.12372783,  0.02050584],\n",
       "       [ 0.00193282,  0.10153015, -0.01682694]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making the calcultion using numpy\n",
    "delta_output_weight_multiplier = delta_output.dot(weights1T)\n",
    "delta_output_weight_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.6 - Calculate the `hidden_layer_deltas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00041597,  0.02185088, -0.00362142],\n",
       "       [-0.00057384, -0.02866677,  0.00488404],\n",
       "       [-0.00056316, -0.02705587,  0.00410378],\n",
       "       [ 0.00048268,  0.01692128, -0.00262183]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_deltas = delta_output_weight_multiplier * sigmoid_derivative(hidden_layer)\n",
    "hidden_layer_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Backpropagation of new weights values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 - Application of Backpropagation (adjusting the weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have worked on the `feed forward` principle by applying weights to the input layer that lead to hidden layer calculations. Then feeding those results forward until we have an output layer total and `prediction`. Overall the process works from `left to right`. Backproagation, on the other hand, is the reverse of this flow in the sense that we will recalculate the weights updates based on results, weights and activation_function results as the input. Then apply them from `right to left`. We will use the formula: $weight_{n + 1} = weight_{n} + (input \\cdot delta \\cdot learning\\_rate)$\n",
    "\n",
    "When creating a neural network the user, or programmer, determines the value of the learning rate. This rate defines the speed of the algorithm or how fast it will learn. Where the learning rate is: \n",
    "- **High** : convergence is fast but the risk is to lose the global minimum. \n",
    "- **Low** : convergence is slow but the risk of losing the global minimum is greatly reduced. \n",
    "\n",
    "**Note:** convergence means the neural network has reached the best result, or global minimum. To maximise a neural network's capability with its efficiency and avoiding setting the learning rate so high that it loses the global minimum many libraries will implement a dynamic learning rate that reduces the rate as the number of epochs increases, so a learning rate starts off higher and reduces as it nears the global minimum which is deemed the best of both worlds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1.1 - Transpose the `hidden_layer` to enable the `input_delta1_multiplier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5885562 , 0.39555998, 0.48350599],\n",
       "       [0.5       , 0.35962319, 0.32300414, 0.21131785],\n",
       "       [0.5       , 0.38485296, 0.27667802, 0.19309868]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose the hidden layer so we can \n",
    "# work by neuron index across all instances\n",
    "# with the multiplier. \n",
    "hidden_layerT = hidden_layer.T\n",
    "hidden_layerT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1.2 - Calculate the `input_delta1_multiplier`\n",
    "\n",
    "We need these deltas to enable the updated weight value calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03293657],\n",
       "       [0.02191844],\n",
       "       [0.02108814]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the deltas needed for the updated weights \n",
    "# calculations to be done. \n",
    "input_delta1_multiplier = hidden_layerT.dot(delta_output)\n",
    "input_delta1_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1.3 - Calculate the new `weights1` values\n",
    "\n",
    "Here we will calculate the new weights to be applied between the `hidden_layer` & `output_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00711903],\n",
       "        [-0.88642447],\n",
       "        [ 0.15432644]]),\n",
       " array([[-0.017],\n",
       "        [-0.893],\n",
       "        [ 0.148]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the weights from the \n",
    "# hidden layer to the output layer \n",
    "weights2 = weights1 + (input_delta1_multiplier * learning_rate) \n",
    "weights2, weights1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1.4 - Calculate new `weights0` values\n",
    "\n",
    "Here we calculate the new weights values that are applied between the `input_layer` & `hidden_layer`\n",
    "\n",
    "Again we are using the formula: $delta _{hidden} = sigmoid _{derivative} \\cdot weight \\cdot delta _{output}$  but here the delta is the gradient or the `partial derivative` of the `cost` value. It's an important part to remember we are _not_ using the activation values of the neurons but the delta values instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00041597,  0.02185088, -0.00362142],\n",
       "       [-0.00057384, -0.02866677,  0.00488404],\n",
       "       [-0.00056316, -0.02705587,  0.00410378],\n",
       "       [ 0.00048268,  0.01692128, -0.00262183]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1],\n",
       "       [0, 1, 0, 1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose the inputs to allow for the multiplier. \n",
    "input_layerT = input_layer.T\n",
    "input_layerT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.04778516e-05, -1.01345901e-02,  1.48194623e-03],\n",
       "       [-9.11603819e-05, -1.17454886e-02,  2.26221011e-03]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the multiplier layer matrix\n",
    "input_delta0_multiplier = input_layerT.dot(hidden_layer_deltas)\n",
    "input_delta0_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42402414, -0.74304038, -0.96055542],\n",
       "       [ 0.35797265, -0.58052365, -0.46832134]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the new weights0 value and apply \n",
    "weights0 = weights0 + (input_delta0_multiplier * learning_rate)\n",
    "weights0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>We have completed a whole epoch</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
