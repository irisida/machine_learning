{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi layered perceptron](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layered Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to Multi-layer networks\n",
    "2. Activation functions (introducing sigmoids)\n",
    "3. Sigmoid function Theory\n",
    "4. Hidden layer activation\n",
    "5. Sigmoid function implementation\n",
    "6. Error Functions\n",
    "7. Multilayered Perceptron basic Algorithm\n",
    "8. Gradient Descent\n",
    "9. Output layerDelta\n",
    "10. Backpropagation\n",
    "11. Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Multi-layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the depiction at the top of the workbook the main concepts of a multilayer perceptron. \n",
    "- The idea of a hidden layer. \n",
    "- The confirmation that each neuron of that layer should its own: \n",
    "    - sum function \n",
    "    - activation function. \n",
    "- We see that for each input and weight, there is a connection to each neuron in the hidden layer. This means that a great many individual connection lines may be seen. \n",
    "- We see that each neuron in the hidden layer is connected to the last neuron in the output layer and passes the weights along these connections. \n",
    "- The output layer neuron will have a sum function & activation function. This leads to a final output from our networks evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions (Introducing sigmoids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Step functions` : can have values of zero or 1, ie values are stepped. This is the example we seen in the single layer perceptron. \n",
    "\n",
    "- `Sigmoid functions` : can have a value in the **_range of 0 to 1_**, the function has the ability to touch all the points between 0 & 1. To get the value we can apply the following equation: $y = \\frac{1}{1 + e^{-x}}$. This works by determining where on the line a value belongs.\n",
    "    - if `x` is high, the value lies closer to, or equal to 1. \n",
    "    - if `x` is low , the value lies closer to, or equal to 0. \n",
    "    \n",
    "If we need to return negative values we can use the `hyperbolic tangent function`:\n",
    "- $y = \\frac{e^{x} - e^{-x} }{e^{x} + e^{-x}}$\n",
    "- evaluating the equation asks to replace the `x` with the value under evaluation and the return will be graded between `-1` & `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function Theory\n",
    "\n",
    "The irrational number e is also known as `Euler’s number`. It is approximately 2.718281, and is the base of the natural logarithm, ln (this means that, if $x = \\ln y = \\log_e y$, then $e^x = y$. \n",
    "\n",
    "**Note: For real input, exp(x) is always positive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See above for Euler’s number details. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our sigmoid in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-1)\t is:  0.2689414213699951\n",
      "sigmoid(0)\t is:  0.5\n",
      "sigmoid(1)\t is:  0.7310585786300049\n",
      "sigmoid(3)\t is:  0.9525741268224334\n",
      "sigmoid(5)\t is:  0.9933071490757153\n",
      "sigmoid(30.5)\t is:  0.9999999999999432\n",
      "sigmoid(-25.5)\t is:  8.423463754397692e-12\n"
     ]
    }
   ],
   "source": [
    "# sample testing the sigmoid function with a range of values. \n",
    "values = [-1, 0, 1, 3, 5, 30.5, -25.5]\n",
    "\n",
    "for val in values:\n",
    "    print(f\"sigmoid({val})\\t is:  {sigmoid(val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Layer activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 'XOR' operator as our case for the multi-layer study. The following truth table used as reference. We will focus on the `feed-forward` process from the input layer to the hidden layer. \n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/coa/images/logic-gates5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two inputs, three neurons hidden layer\n",
    "In the theory for this example we will have an example with: \n",
    "- two inputs (x,y) as we have had all along\n",
    "- three neurons. (with individual sum and activation functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcomes \n",
    "For the data sample `x=0, y=0, class=0` we will have the following scenario: \n",
    "\n",
    "| | Input 1 value | Input 1 weight | Input 2 value | Input 2 weight | Result |\n",
    "|-|---------|----------|---------|----------|--------|\n",
    "|1|0|-0.424|0|0.358| 0 * (-0.424) + 0 * 0.358 = 0.000|\n",
    "|2|0|-0.740|0|-0.577| 0 * (-0.740) + 0 * (-0.577) = 0.000|\n",
    "|3|0|-0.961|0|-0.469| 0 * (-0.961) + 0 * (-0.469) = 0.000|\n",
    "\n",
    "| | Input 1 value | Input 1 weight | Input 2 value | Input 2 weight | Result |\n",
    "|-|---------|----------|---------|----------|--------|\n",
    "|4|0|-0.424|1|0.358| 0 * (-0.424) + 1 * 0.358 = 0.358|\n",
    "|5|0|-0.740|1|-0.577| 0 * (-0.740) + 1 * (-0.577) = -0.577|\n",
    "|6|0|-0.961|1|-0.469| 0 * (-0.961) + 1 * (-0.469) = -0.469|\n",
    "\n",
    "\n",
    "| | Input 1 value | Input 1 weight | Input 2 value | Input 2 weight | Result |\n",
    "|-|---------|----------|---------|----------|--------|\n",
    "|7|1|-0.424|0|0.358| 1 * (-0.424) + 0 * 0.358 = -0.424|\n",
    "|8|1|-0.740|0|-0.577| 1 * (-0.740) + 0 * (-0.577) = -0.740|\n",
    "|9|1|-0.961|0|-0.469| 1 * (-0.961) + 0 * (-0.469) = -0.961|\n",
    "\n",
    "\n",
    "| | Input 1 value | Input 1 weight | Input 2 value | Input 2 weight | Result |\n",
    "|-|---------|----------|---------|----------|--------|\n",
    "|10|1|-0.424|1|0.358| 1 * (-0.424) + 1 * 0.358 = -0.066|\n",
    "|11|1|-0.740|1|-0.577| 1 * (-0.740) + 1 * (-0.577) = -1.317|\n",
    "|12|1|-0.961|1|-0.469| 1 * (-0.961) + 1 * (-0.469) = -1.430|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling sigmoid() on the neuron results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 0.5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0), sigmoid(0), sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6044400174193626, 0.323004143761477, 0.2766780228949468)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0.358), sigmoid(-0.577), sigmoid(-0.469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6044400174193626, 0.323004143761477, 0.2766780228949468)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0.424), sigmoid(-0.740), sigmoid(-0.961)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5164940131078767, 0.21131784831127748, 0.19309868423321644)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0.066), sigmoid(-1.317), sigmoid(-1.430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron Implementation \n",
    "\n",
    "We have covered the theory of passing from the inputs to the hidden layer. We can now move to present that in python code.\n",
    "\n",
    "1. create the inputs \n",
    "2. create the outputs \n",
    "3. create the np.array(weights_for_each_input)\n",
    "4. create the np.array(weights_for_each_hidden_layer_to_output)\n",
    "5. create the epochs threshold. \n",
    "6. create the sum_synapse0 as np.dot(inputs, weights_for_each_input)\n",
    "7. create the hidden_layer results as sigmoid(sum_synapse0) _see Euler's number_ \n",
    "8. apply the sum function to each of the hidden layer results (sigmoid) and activation application as sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "9. define the error_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the data \n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.array([[0], [1], [1], [0]])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights0 are inputX weights, inputY weghts respectively \n",
    "weights0 = np.array([[-0.424, -0.740, -0.961], \n",
    "                     [0.358, -0.577, -0.469]])\n",
    "\n",
    "\n",
    "# weights1 are hardcoded in the class lecture of this example. \n",
    "weights1 = np.array([[-0.017], \n",
    "                     [-0.893], \n",
    "                     [0.148]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the epochs limit to control the amount of times we'll allow the algorithm to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epochs limit.\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start the sum of the communication between the input layer and the hidden layer. This is basically a matrix multiplication exercise.Creating the results of: `for each input_layer * each weights0` except we are using the `np.dot()` is more highly optimised than using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.358, -0.577, -0.469],\n",
       "       [-0.424, -0.74 , -0.961],\n",
       "       [-0.066, -1.317, -1.43 ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = inputs\n",
    "\n",
    "# starts the sum of the communication between the input layer and the hidden layer\n",
    "# basically a matrix multiplication exercise. Creating the results of: \n",
    "# for each input_layer * each weights0.  Important: using the np.dot is more highly \n",
    "# optimised than using a for loop.\n",
    "\n",
    "sum_synapse0 = np.dot(input_layer, weights0)\n",
    "sum_synapse0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the hidden layer values, these are the values\n",
    "# returned from the sigmoid of the sum_synapse0\n",
    "\n",
    "hidden_layer = sigmoid(sum_synapse0)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results of a the sigmoid from applying the inputs and weights we apply the sum funtion and activation function again to achieve the final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40588573188433286"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 0 in our dataset (0,0) we can see the results are 0.5, 0.5, 0.5 \n",
    "# lets take the results and multiply by the weights \n",
    "arr = [0.5, 0.5, 0.5]\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "result = sigmoid(calc_result)\n",
    "\n",
    "calc_result, result\n",
    "\n",
    "# in our case it's 0.40588573188433286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.27419072598999994, 0.43187856860760854)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 1 in the dataset [0.5885562 , 0.35962319, 0.38485296]\n",
    "arr = [0.5885562 , 0.35962319, 0.38485296]\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "\n",
    "result = sigmoid(calc_result)\n",
    "\n",
    "calc_result, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.25421886972, 0.43678536534288)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0.39555998, 0.32300414, 0.27667802],\n",
    "arr = [0.39555998, 0.32300414, 0.27667802]\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "\n",
    "result = sigmoid(calc_result)\n",
    "\n",
    "calc_result, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.16834783724, 0.4580121586455045)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0.48350599, 0.21131785, 0.19309868]\n",
    "arr = [0.48350599, 0.21131785, 0.19309868]\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "\n",
    "result = sigmoid(calc_result)\n",
    "\n",
    "calc_result, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.381     ],\n",
       "       [-0.27419072],\n",
       "       [-0.25421887],\n",
       "       [-0.16834784]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the sum_synapse1 values which are the \n",
    "# final results of the neural network for \n",
    "# each of the items in our dataset. \n",
    "sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "sum_synapse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer  = sigmoid(sum_synapse1)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Functions (loss function, cost function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the error by comparing the results of the predictions with the outputs of the dataset. The error function is often referred to as the loss function in ML terminology. the simplest formula is `error = correct - prediction`\n",
    "\n",
    "|x1|x2|Class|Prediction|Error|\n",
    "|--|--|-----|----------|-----|\n",
    "|0|0|0|0.405|-0.405|\n",
    "|0|1|1|0.431|0.569|\n",
    "|1|0|1|0.436|0.564|\n",
    "|1|1|0|0.458|-0.458|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.499"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the average error by taking the absolute values by instances \n",
    "res = (0.405 + 0.569 + 0.564 + 0.458) / 4\n",
    "round(res, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the outputs defined earlier (outputs are the correct results)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get out output layer (predictions)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the errors (error = correct - preditions)\n",
    "error_output_layer = outputs - output_layer\n",
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error average: 0.49880848923713045\n",
      "Error average (rounded) : 0.499\n"
     ]
    }
   ],
   "source": [
    "# get the average error. As a reminder we need to use \n",
    "# the absolute vaues of the errors. If this is overlooked\n",
    "# we will skew the results. \n",
    "error_avg = np.mean(abs(error_output_layer))\n",
    "print(f\"Error average: {error_avg}\")\n",
    "print(f\"Error average (rounded) : {round(error_avg, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayered Perceptron Basic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1JqnDqu0T9k9-g87C_6dEHLoWItFK8D7J)\n",
    "\n",
    "1. Cost function (loss function)\n",
    "2. Gradient descent\n",
    "3. Derivative \n",
    "4. Delta\n",
    "5. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)\n",
    "\n",
    "\n",
    "The idea of gradient descent is to manage out cost function (loss function, error function) to get to the smallest possible error in the adjustment of the weights. The directional control of how a weight set should be adjusted is done by calculating the partial derivative as a means of determining the direction of a gradient. \n",
    "\n",
    "If you imagine a x,y axis graph with a curve, the x-axis is the weight and the y-axi is the error value, we are trying to achieve the lowest point of the curve, which may never be zero by the way, in a multi-dip curve we may have a local minimum and a global minimum across the span of measurements (number of epochs). So the purpose is to calculate the slope of a curve based on the partial derivatives.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Yong_Ma15/publication/267820876/figure/fig1/AS:669428953923612@1536615708709/Schematic-of-the-local-minima-problem-in-FWI-The-data-misfit-has-spurious-local-minima.png)\n",
    "\n",
    "- reminder of the sigmoid function: $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- calculating the partial derivative: $d = y \\cdot (1 -y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hypothetical example \n",
    "\n",
    "Assuming that `y` = 0.1 \n",
    "\n",
    "- calculating the partial derivative: $d = 0.1 \\cdot (1 -0.1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid_derivative(sigmoid):\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sigmoid(0.5)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2350037122015945"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sigmoid_derivative(s)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output layer Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of sequence is: \n",
    "\n",
    "- activation function (sigmoid) $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- Derivative $d = y \\cdot (1 -y)$\n",
    "\n",
    "- Delta $delta _{output} = error \\cdot sigmoid _{derivative}$\n",
    "- Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough for dataset example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.5, 0.5, 0.5]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.0085] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.4465] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.074] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.381\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.381 :  0.40588573188433286\n",
      "[Step 04.00] Error = [0] - 0.40588573188433286 :  [-0.40588573]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.40588573188433286)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [-0.40588573] * 0.24114250453705233 :  [-0.0978763]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = [0.5, 0.5, 0.5]\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[0] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[0]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough for dataset example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.5885562  0.35962319 0.38485296]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.01000546] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.3211435] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.05695824] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.2741907222993588\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.2741907222993588 :  0.43187856951314224\n",
      "[Step 04.00] Error = [1] - 0.43187856951314224 :  [0.56812143]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.43187856951314224)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [0.56812143] * 0.24114250453705233 :  [0.13699822]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = np.array([sigmoid(0.358), sigmoid(-0.577), sigmoid(-0.469)])\n",
    "\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[1] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[1]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough for dataset example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.60444002 0.32300414 0.27667802]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.01027548] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.2884427] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.04094835] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.257769833286676\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.257769833286676 :  0.4359120113833003\n",
      "[Step 04.00] Error = [1] - 0.4359120113833003 :  [0.56408799]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.4359120113833003)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [0.56408799] * 0.24114250453705233 :  [0.13602559]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = np.array([sigmoid(0.424), sigmoid(-0.740), sigmoid(-0.961)])\n",
    "\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[2] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[2]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.245904"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(0.436)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough for dataset example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.51649401 0.21131785 0.19309868]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.0087804] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.18870684] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.02857861] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.16890863149828866\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.16890863149828866 :  0.45787295203081535\n",
      "[Step 04.00] Error = [0] - 0.45787295203081535 :  [-0.45787295]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.45787295203081535)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [-0.45787295] * 0.24114250453705233 :  [-0.11041263]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = np.array([sigmoid(0.066), sigmoid(-1.317), sigmoid(-1.430)])\n",
    "\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[3] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[3]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
