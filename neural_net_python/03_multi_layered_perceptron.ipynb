{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi layered perceptron](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layered Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.01 - Introduction to Multi-layer networks\n",
    "- 3.02 - Hidden layer activation\n",
    "- 3.03 - Multilayer Perceptron Implementation steps\n",
    "- 3.04 - Error Functions\n",
    "- 3.05 - Multilayered Perceptron basic Algorithm\n",
    "- 3.06 - Gradient Descent\n",
    "- 3.07 - Output Layer Delta\n",
    "- 3.08 - Delta implementation in Python\n",
    "- 3.09 - Backpropagation\n",
    "- 3.10 - Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.01 - Introduction to Multi-layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the depiction at the top of the workbook the main concepts of a multilayer perceptron. The basic principal is that there is at least 1 hidden layer. Each neuron in the hidden layer should have its own `sum function` & `activation function`. The elements are all connected! which means in the case of the first hidden layer that each neuron is connected to each of the inputs. In the case of subsequent hidden layers each neuron is connected to each neuron of the preceding layer, again each having its own `sum` and `activation` functions.  \n",
    "\n",
    "Finally our structure will conclude and this is called the output layer. This is fed the results of each of the neurons in the final hidden layer and this result is `the prediction` of our neural network.\n",
    "\n",
    "**key point$^1$**: a `sum function` is the result of multiplying an input value by the associated weight. In a single layer perceptron that means, a single sum and activation function for the inputs. In a multilayer perceptron where we can have `_n_ layers` that means the sum can be made of the multiplier by the inputs and the weights in the case of the first hidden layer and for subsequent layers it can be the sum of the neuron in that preceding layer multiplied by another weight between hidden layers.   \n",
    "\n",
    "**key point$^2$**: an `activation function` is a decision fork of evaluating a sum and deciding of that neuron is fired or not. In the single layer perceptron we seen a `step function` type of activation function. A step can have values of `0` or `1`. Another type of activation function is the `sigmoid function`. What is different here is that the result or activation can be between `0` and `1` and not stepped. That ability to touch all points between `0` and `1` means we need to work out exactly where on the line that value belongs. \n",
    "  \n",
    "**key point$^3$**: If we need to return negative values we can use the `hyperbolic tangent function`:\n",
    "- $y = \\frac{e^{x} - e^{-x} }{e^{x} + e^{-x}}$\n",
    "- evaluating the equation asks to replace the `x` with the value under evaluation and the return will be graded between `-1` & `1`. \n",
    "\n",
    "**Theory to remember**: The irrational number `e` is also known as `Eulerâ€™s number`. It is approximately 2.718281, and is the base of the natural logarithm, ln (this means that, if $x = \\ln y = \\log_e y$, then $e^x = y$. In a `sigmoid function` We apply the following equation: $y = \\frac{1}{1 + e^{-x}}$ to determine:\n",
    "- if `x` is high, the value lies closer to, or equal to 1. \n",
    "- if `x` is low , the value lies closer to, or equal to 0.  \n",
    "\n",
    "**let's see what this looks like in python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-1)\t is:  0.2689414213699951\n",
      "sigmoid(0)\t is:  0.5\n",
      "sigmoid(1)\t is:  0.7310585786300049\n",
      "sigmoid(3)\t is:  0.9525741268224334\n",
      "sigmoid(5)\t is:  0.9933071490757153\n",
      "sigmoid(30.5)\t is:  0.9999999999999432\n",
      "sigmoid(-25.5)\t is:  8.423463754397692e-12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# definition of a sigmoid function. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))\n",
    "\n",
    "\n",
    "# sample testing the sigmoid function with a range of values. \n",
    "values = [-1, 0, 1, 3, 5, 30.5, -25.5]\n",
    "\n",
    "for val in values:\n",
    "    print(f\"sigmoid({val})\\t is:  {sigmoid(val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.02 - Hidden Layer activation (using binary xor operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static.javatpoint.com/tutorial/coa/images/logic-gates5.png)\n",
    "\n",
    "We will use the 'XOR' operator as our case for the multi-layer study. The following truth table used as reference. We will focus on the `feed-forward` process from the input layer to the hidden layer. We can declare the following upfront: \n",
    "- We have 2 inputs (x,y) as we have had all along.\n",
    "- We have 3 neurons in our hidden layer, with individual sum and activation functions, of course. \n",
    "\n",
    "\n",
    "**Let's see that in simple python code**\n",
    "\n",
    "- task 1 - define the inputs of the xor truth table \n",
    "- task 2 - define some weights (demonstration purposes here)\n",
    "- task 3 - perform the calculation loop to find:\n",
    "    - product of _(input * weight)_\n",
    "    - sigmoid of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "x1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "----------------------------------------------------------------------\n",
      "0\t0\t-0.424\t0.358\t\t0.0\t0.5 \n",
      "0\t0\t-0.74\t-0.577\t\t-0.0\t0.5 \n",
      "0\t0\t-0.961\t-0.469\t\t-0.0\t0.5 \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "x1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "----------------------------------------------------------------------\n",
      "0\t1\t-0.424\t0.358\t\t0.358\t0.5885562043858291 \n",
      "0\t1\t-0.74\t-0.577\t\t-0.577\t0.3596231853677901 \n",
      "0\t1\t-0.961\t-0.469\t\t-0.469\t0.38485295749078957 \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "x1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "----------------------------------------------------------------------\n",
      "1\t0\t-0.424\t0.358\t\t-0.424\t0.39555998258063735 \n",
      "1\t0\t-0.74\t-0.577\t\t-0.74\t0.323004143761477 \n",
      "1\t0\t-0.961\t-0.469\t\t-0.961\t0.2766780228949468 \n",
      "\n",
      "----------------------------------------------------------------------\n",
      "x1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "----------------------------------------------------------------------\n",
      "1\t1\t-0.424\t0.358\t\t-0.066\t0.4835059868921233 \n",
      "1\t1\t-0.74\t-0.577\t\t-1.317\t0.21131784831127748 \n",
      "1\t1\t-0.961\t-0.469\t\t-1.43\t0.19309868423321644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To show a table of inputs, weights and products\n",
    "def table_border():\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# declare the inputs for a xor truth table     \n",
    "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
    "\n",
    "# define some weights. These are random numbers for demo purposes. \n",
    "weights0 = [(-0.424, 0.358), (-0.740, -0.577), (-0.961, -0.469)]\n",
    "\n",
    "# create a table of outcomes \n",
    "for idx, i in enumerate(inputs):\n",
    "    x1,x2 = i\n",
    "    table_border()\n",
    "    print(f\"x1\\tx2\\tw1\\tw2\\t\\tProduct\\tSigmoid \")\n",
    "    table_border()\n",
    "    for w1,w2 in weights0:\n",
    "        product = (x1 * w1) + (x2 * w2)\n",
    "        sig = sigmoid(product)\n",
    "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t\\t{product}\\t{sig} \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the results, or `products` of the `inputs` * `weights` above, we call the sigmoid function to get the result. In the cell below we can see a summary around the generation of the sigmoid values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid values of inputs * weights or 'products'\n",
      "example0 : (0.5, 0.5, 0.5)\n",
      "example1 : (0.5885562043858291, 0.3596231853677901, 0.38485295749078957)\n",
      "example2 : (0.6044400174193626, 0.323004143761477, 0.2766780228949468)\n",
      "example3 : (0.5164940131078767, 0.21131784831127748, 0.19309868423321644)\n"
     ]
    }
   ],
   "source": [
    "# example 0 \n",
    "example0 = sigmoid(0), sigmoid(0), sigmoid(0)\n",
    "\n",
    "# example1\n",
    "example1 = sigmoid(0.358), sigmoid(-0.577), sigmoid(-0.469)\n",
    "\n",
    "# example 2\n",
    "example2 = sigmoid(0.424), sigmoid(-0.740), sigmoid(-0.961)\n",
    "\n",
    "# example 3\n",
    "example3 = sigmoid(0.066), sigmoid(-1.317), sigmoid(-1.430)\n",
    "\n",
    "print(\"Sigmoid values of inputs * weights or 'products'\")\n",
    "print(f\"example0 : {example0}\")\n",
    "print(f\"example1 : {example1}\")\n",
    "print(f\"example2 : {example2}\")\n",
    "print(f\"example3 : {example3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.03 - Multilayer Perceptron Implementation steps \n",
    "\n",
    "We have covered the theory of passing from the inputs to the hidden layer, the calculation of the inputs and weights to generate a sum and the sigmoid to get an activation value. We can now move to present that in slightly more robust python code and utilizing the `numpy` library because it's much more performant, an industry standard and well, pretty awesome too. Here are the tasks.\n",
    "\n",
    "1. create the inputs \n",
    "2. create the outputs \n",
    "3. create the np.array(weights_for_each_input)\n",
    "4. create the np.array(weights_for_each_hidden_layer_to_output)\n",
    "5. create the epochs threshold. \n",
    "6. create the sum_synapse0 as np.dot(inputs, weights_for_each_input)\n",
    "7. create the hidden_layer results as sigmoid(sum_synapse0) _see Euler's number_ \n",
    "8. apply the sum function to each of the hidden layer results (sigmoid) and activation application as sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "9. define the error_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the inputs data \n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.array([[0], [1], [1], [0]])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights0 are inputX weights, inputY weghts respectively \n",
    "# these are the weights that are used in the sum function to \n",
    "# geberate a product value. \n",
    "weights0 = np.array([[-0.424, -0.740, -0.961], \n",
    "                     [0.358, -0.577, -0.469]])\n",
    "\n",
    "\n",
    "# weights1 are hardcoded in the class lecture of this example. \n",
    "# These are the weights we see used between the hidden layer\n",
    "# and the output layer at the end of our opoeration. The figures\n",
    "# are for demonstration purposes so don't get hung up on what \n",
    "# these particular numbers mean. They're just demo weights. \n",
    "weights1 = np.array([[-0.017], \n",
    "                     [-0.893], \n",
    "                     [0.148]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the epochs limit to control the amount of times we'll allow the algorithm to run. Epoch thresholds are used to prevent infinite loops in search of a perfect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epochs limit.\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start the sum of the communication between the input layer and the hidden layer. This is basically a matrix multiplication exercise. \n",
    "\n",
    "In the proof of concept above in the intro section we are doing this with `for loops` and gather in the values of `x1`, `x2`, `w1` & `w2` which creates a results of: _for each input_layer * each weights0_. Here we use the numpy method `np.dot()` as it is far more highly optimised than using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.358, -0.577, -0.469],\n",
       "       [-0.424, -0.74 , -0.961],\n",
       "       [-0.066, -1.317, -1.43 ]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = inputs\n",
    "\n",
    "# starts the sum of the communication between the input\n",
    "# layer and the hidden layer. This is basically a matrix\n",
    "# multiplication exercise. Creating the results of: \n",
    "#     \"for each input_layer * each weights0\"  \n",
    "# Important note: using the np.dot is more highly \n",
    "# optimised than using a for loop.\n",
    "\n",
    "# sum_synapse0 is what we called product above\n",
    "# the name sum_synapse0 is more descriptive \n",
    "# because it is the sum of the synapse at level 0\n",
    "# which is the first layer, or the input to hidden\n",
    "# layer. \n",
    "sum_synapse0 = np.dot(input_layer, weights0)\n",
    "sum_synapse0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the hidden layer values, these are the values\n",
    "# returned from the sigmoid of the sum_synapse0\n",
    "\n",
    "hidden_layer = sigmoid(sum_synapse0)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results of a the sigmoid from applying the inputs and weights we apply the sum funtion and activation function again to achieve the final outputs. Let's work through a manual calculation process to increase our comprehension before adding more complexity to our python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.424, -0.74 , -0.961],\n",
       "       [ 0.358, -0.577, -0.469]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual calculation of output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.381, 0.40588573188433286)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 0 in our dataset \n",
    "# inputs are (0,0) \n",
    "# sum_synapse0 = inputs * weights0 = (0, 0, 0)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.5, 0.5, 0.5)\n",
    "\n",
    "arr = np.array([0.5, 0.5, 0.5])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction\n",
    "\n",
    "# in our case it's 0.40588573188433286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.27419072598999994, 0.43187856860760854)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 1 in our dataset \n",
    "# inputs are (0,1) \n",
    "# sum_synapse0 = inputs * weights0 = (0.358, -0.577, -0.469)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.5885562 , 0.35962319, 0.38485296)\n",
    "\n",
    "\n",
    "arr = np.array([0.5885562 , 0.35962319, 0.38485296])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,1) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.25421886972, 0.43678536534288)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 2 in our dataset \n",
    "# inputs are (1,0) \n",
    "# sum_synapse0 = inputs * weights0 = (-0.424, -0.74 , -0.96)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.39555998, 0.32300414, 0.27667802)\n",
    "\n",
    "arr = np.array([0.39555998, 0.32300414, 0.27667802])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (1,0) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.16834783724, 0.4580121586455045)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 3 in our dataset \n",
    "# inputs are (1,1) \n",
    "# sum_synapse0 = inputs * weights0 = (-0.066, -1.317, -1.43)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.48350599, 0.21131785, 0.19309868)\n",
    "\n",
    "arr = np.array([0.48350599, 0.21131785, 0.19309868])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (1,1) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create the sum_synapse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.381     ],\n",
       "       [-0.27419072],\n",
       "       [-0.25421887],\n",
       "       [-0.16834784]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the sum_synapse1 values. \n",
    "# These are the values that are generated\n",
    "# from the hidden layer to the output\n",
    "# layer and are considered as the final \n",
    "# results of the neural network for \n",
    "# each of the items in our dataset. \n",
    "sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "sum_synapse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create the output layer or as it's \n",
    "# often called the prediction of the \n",
    "# neural network.\n",
    "output_layer  = sigmoid(sum_synapse1)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.04 - Error Functions (_loss function, cost function_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the error by comparing the results of the predictions with the outputs of the dataset. The error function is often referred to as the loss function in ML terminology. the simplest formula is `error = correct - prediction`\n",
    "\n",
    "|x1|x2|Class|Prediction|Error|\n",
    "|--|--|-----|----------|-----|\n",
    "|0|0|0|0.405|-0.405|\n",
    "|0|1|1|0.431|0.569|\n",
    "|1|0|1|0.436|0.564|\n",
    "|1|1|0|0.458|-0.458|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.499"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the average error by taking the absolute values by instances \n",
    "res = (0.405 + 0.569 + 0.564 + 0.458) / 4\n",
    "round(res, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the outputs defined earlier (outputs are the correct results)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get out output layer (predictions)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the errors (error = correct - preditions)\n",
    "error_output_layer = outputs - output_layer\n",
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error average: 0.49880848923713045\n",
      "Error average (rounded) : 0.499\n"
     ]
    }
   ],
   "source": [
    "# get the average error. As a reminder we need to use \n",
    "# the absolute vaues of the errors. If this is overlooked\n",
    "# we will skew the results. \n",
    "error_avg = np.mean(abs(error_output_layer))\n",
    "print(f\"Error average: {error_avg}\")\n",
    "print(f\"Error average (rounded) : {round(error_avg, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.05 - Multilayered Perceptron Basic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1JqnDqu0T9k9-g87C_6dEHLoWItFK8D7J)\n",
    "\n",
    "1. Cost function (loss function)\n",
    "2. Gradient descent\n",
    "3. Derivative \n",
    "4. Delta\n",
    "5. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.06 - Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)\n",
    "\n",
    "\n",
    "The idea of gradient descent is to manage out cost function (loss function, error function) to get to the smallest possible error in the adjustment of the weights. The directional control of how a weight set should be adjusted is done by calculating the partial derivative as a means of determining the direction of a gradient. \n",
    "\n",
    "If you imagine a x,y axis graph with a curve, the x-axis is the weight and the y-axi is the error value, we are trying to achieve the lowest point of the curve, which may never be zero by the way, in a multi-dip curve we may have a local minimum and a global minimum across the span of measurements (number of epochs). So the purpose is to calculate the slope of a curve based on the partial derivatives.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Yong_Ma15/publication/267820876/figure/fig1/AS:669428953923612@1536615708709/Schematic-of-the-local-minima-problem-in-FWI-The-data-misfit-has-spurious-local-minima.png)\n",
    "\n",
    "- reminder of the sigmoid function: $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- calculating the partial derivative: $d = y \\cdot (1 -y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hypothetical example \n",
    "\n",
    "Assuming that `y` = 0.1 \n",
    "\n",
    "- calculating the partial derivative: $d = 0.1 \\cdot (1 -0.1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid_derivative(sigmoid):\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sigmoid(0.5)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2350037122015945"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sigmoid_derivative(s)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.07 - Output layer Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of sequence is: \n",
    "\n",
    "- activation function (sigmoid) $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- Derivative $d = y \\cdot (1 -y)$\n",
    "\n",
    "- Delta $delta _{output} = error \\cdot sigmoid _{derivative}$\n",
    "- Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10.01 - Walkthrough for dataset example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.5, 0.5, 0.5]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.0085] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.4465] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.074] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.381\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.381 :  0.40588573188433286\n",
      "[Step 04.00] Error = [0] - 0.40588573188433286 :  [-0.40588573]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.40588573188433286)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [-0.40588573] * 0.24114250453705233 :  [-0.0978763]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = [0.5, 0.5, 0.5]\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[0] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[0]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10.02 - Walkthrough for dataset example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.5885562  0.35962319 0.38485296]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.01000546] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.3211435] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.05695824] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.2741907222993588\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.2741907222993588 :  0.43187856951314224\n",
      "[Step 04.00] Error = [1] - 0.43187856951314224 :  [0.56812143]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.43187856951314224)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [0.56812143] * 0.24114250453705233 :  [0.13699822]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = np.array([sigmoid(0.358), sigmoid(-0.577), sigmoid(-0.469)])\n",
    "\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[1] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[1]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10.03 - Walkthrough for dataset example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.60444002 0.32300414 0.27667802]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.01027548] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.2884427] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.04094835] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.257769833286676\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.257769833286676 :  0.4359120113833003\n",
      "[Step 04.00] Error = [1] - 0.4359120113833003 :  [0.56408799]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.4359120113833003)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [0.56408799] * 0.24114250453705233 :  [0.13602559]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = np.array([sigmoid(0.424), sigmoid(-0.740), sigmoid(-0.961)])\n",
    "\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[2] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[2]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.245904"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(0.436)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10.04 - Walkthrough for dataset example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] np.dot(input_layer, weights0) = hidden layer : [0.51649401 0.21131785 0.19309868]\n",
      "[Step 02.01] arr[0] * weights1[0] : [-0.0087804] \n",
      "[Step 02.02] arr[1] * weights1[1] : [-0.18870684] \n",
      "[Step 02.03] arr[2] * weights1[2] : [0.02857861] \n",
      "[step 02.04] Apply sum to calculated figures :  -0.16890863149828866\n",
      "[Step 03.00] The prediction - gets sigmoid of sum -0.16890863149828866 :  0.45787295203081535\n",
      "[Step 04.00] Error = [0] - 0.45787295203081535 :  [-0.45787295]\n",
      "[Step 05.00] Derivative - sigmoid_derivative(sigmoid(0.45787295203081535)) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [-0.45787295] * 0.24114250453705233 :  [-0.11041263]\n"
     ]
    }
   ],
   "source": [
    "# as a reminder we will take the first calculation from the inputs example 0.  \n",
    "# lets take the results and multiply by the weights \n",
    "arr = np.array([sigmoid(0.066), sigmoid(-1.317), sigmoid(-1.430)])\n",
    "\n",
    "print(\"[Step 01.00] np.dot(input_layer, weights0) = hidden layer :\", arr)\n",
    "\n",
    "# start output layer sequence, multiply by the weights between hidden layer \n",
    "# and the output layer with final result (prediction)\n",
    "calc_result = arr[0] * (-0.017) + arr[1] * (-0.893) + arr[2] * (0.148)\n",
    "print(f\"[Step 02.01] arr[0] * weights1[0] : {arr[0] * weights1[0]} \")\n",
    "print(f\"[Step 02.02] arr[1] * weights1[1] : {arr[1] * weights1[1]} \")\n",
    "print(f\"[Step 02.03] arr[2] * weights1[2] : {arr[2] * weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", calc_result)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(calc_result)\n",
    "print(f\"[Step 03.00] The prediction - gets sigmoid of sum {calc_result} : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[3] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Error = {outputs[3]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Derivative - sigmoid_derivative(sigmoid({sigmoid_sum})) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.08 - Delta implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the delta is used to determine the direction of the gradient in order to update weights. The process takes each instance of a case's dataset and returns a delta value:\n",
    "\n",
    "- To calculate the `derivative_output` we need to get the result of the `sigmoid_derivative(output_layer)`\n",
    "- to calculate the delta output we need to take the `error_output_layer` and multiply by the `derivative_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2411425 ],\n",
       "       [0.24535947],\n",
       "       [0.24600391],\n",
       "       [0.24823702]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_output = sigmoid_derivative(output_layer)\n",
    "derivative_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output = error_output_layer * derivative_output\n",
    "delta_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$delta _{hidden} = sigmoid _{derivavtive} \\cdot weight \\cdot delta _{output} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0][0] : activation: 0.5 : derivative: 0.25 * lb: -0.017 * delta : -0.098 = 0.00042\n",
      "[0][1] : activation: 0.5 : derivative: 0.25 * lb: -0.893 * delta : -0.098 = 0.02188\n",
      "[0][2] : activation: 0.5 : derivative: 0.25 * lb: 0.148 * delta : -0.098 = -0.00363\n",
      "\n",
      "[1][0] : activation: 0.589 : derivative: 0.242 * lb: -0.017 * delta : 0.139 = -0.00057\n",
      "[1][1] : activation: 0.36 : derivative: 0.23 * lb: -0.893 * delta : 0.139 = -0.02855\n",
      "[1][2] : activation: 0.385 : derivative: 0.237 * lb: 0.148 * delta : 0.139 = 0.00488\n",
      "\n",
      "[2][0] : activation: 0.396 : derivative: 0.239 * lb: -0.017 * delta : 0.139 = -0.00056\n",
      "[2][1] : activation: 0.323 : derivative: 0.219 * lb: -0.893 * delta : 0.139 = -0.02718\n",
      "[2][2] : activation: 0.277 : derivative: 0.2 * lb: 0.148 * delta : 0.139 = 0.00411\n",
      "\n",
      "[3][0] : activation: 0.484 : derivative: 0.25 * lb: -0.017 * delta : -0.114 = 0.00048\n",
      "[3][1] : activation: 0.211 : derivative: 0.167 * lb: -0.893 * delta : -0.114 = 0.017\n",
      "[3][2] : activation: 0.193 : derivative: 0.156 * lb: 0.148 * delta : -0.114 = -0.00263\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hidden_layer)):\n",
    "    arr = np.array(hidden_layer[i])\n",
    "    print(\"\")\n",
    "    for j in range(len(arr)):  \n",
    "        synapse = round(hidden_layer[i][j], 3)\n",
    "        sd_val = round(sigmoid_derivative(hidden_layer[i][j]),3)\n",
    "        weight = float(weights1[j])\n",
    "        do = round(float(delta_output[i]), 3)\n",
    "        print(f\"[{i}][{j}] : activation: {synapse} : derivative: {sd_val} * lb: {weight} * delta : {do} = {round(float(sd_val * weight * do),5)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the delta_output by weights1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017],\n",
       "       [-0.893],\n",
       "       [ 0.148]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delta_output_weight_multiplier = delta_output.dot(weights1)\n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017, -0.893,  0.148]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1T = weights1.T\n",
    "weights1T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (1, 3), (4, 1))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes \n",
    "weights1.shape, weights1T.shape, delta_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0016639 ,  0.08740354, -0.01448569],\n",
       "       [-0.0023697 , -0.12447882,  0.02063031],\n",
       "       [-0.0023554 , -0.12372783,  0.02050584],\n",
       "       [ 0.00193282,  0.10153015, -0.01682694]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output_weight_multiplier = delta_output.dot(weights1T)\n",
    "delta_output_weight_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.09 - Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.10 - Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
