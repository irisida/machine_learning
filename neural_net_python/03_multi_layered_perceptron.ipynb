{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi layered perceptron](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layered Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "- 3.01 - Introduction to Multi-layer networks\n",
    "- 3.02 - Hidden layer activation\n",
    "- 3.03 - Multilayer Perceptron Implementation steps\n",
    "- 3.04 - Manual calculation and process checkpoint summary\n",
    "\n",
    "#### Part 2\n",
    "- 3.05 - Multilayered Perceptron basic Algorithm\n",
    "    - 3.05.01 - Error Functions (cost function, loss function)\n",
    "    - 3.05.02 - Gradient Descent\n",
    "    - 3.05.03 - Output Layer Delta\n",
    "    - 3.05.04 - Delta implementation in Python\n",
    "    - 3.05.05 - Backpropagation\n",
    "- 3.06 - Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.01 - Introduction to Multi-layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the depiction at the top of the workbook we can see the main concepts of a multilayer perceptron. The most basic principles are: \n",
    "\n",
    "- **There is at least 1 hidden layer**. \n",
    "- Each neuron in the hidden layer should have its own `sum function` & `activation function`. \n",
    "- The elements are all connected! Which means in the case of the first hidden layer that each neuron is connected to each of the inputs. In the case of subsequent hidden layers each neuron is connected to each neuron of the preceding layer, again each having its own `sum` and `activation` functions.  \n",
    "- Our structure will conclude with an output layer. This is fed the results of each of the neurons in the final hidden layer and **this result is the prediction** of our neural network.\n",
    "\n",
    "#### Q: What is a `sum function`?\n",
    "**A:** The `sum function` is the result of multiplying an input value by the associated weight. In a single layer perceptron that means, a single sum and activation function for the inputs. In a multilayer perceptron where we can have `_n_ layers` that means the sum can be made of the multiplier by the inputs and the weights in the case of the first hidden layer and for subsequent layers it can be the sum of the neuron in that preceding layer multiplied by another weight between hidden layers.   \n",
    "#### Q: What is an `activation function`?\n",
    "**A:** The `activation function` is the decision fork of evaluating a sum and deciding if that neuron is fired or not. In the single layer perceptron we seen a `step function` type of activation function. A step can have values of `0` or `1`. Another type of activation function is the `sigmoid function`. What is different here is that the result or activation can be between `0` and `1` and not stepped. That ability to touch all points between `0` and `1` means we need to work out exactly where on the line that value belongs. \n",
    "  \n",
    "#### Q: What if I need to return negative values?  \n",
    "**A:** If we need to return negative values we can use the `hyperbolic tangent function` which looks like this: $y = \\frac{e^{x} - e^{-x} }{e^{x} + e^{-x}}$ evaluating the equation asks to replace the `x` with the value under evaluation and the return will be graded between `-1` & `1`. \n",
    "\n",
    "#### Q: Anything else worth remembering? \n",
    "**A:** The irrational number `e` is also known as `Eulerâ€™s number`. It is approximately 2.718281, and is the base of the natural logarithm, ln (this means that, if $x = l_n y = \\log_e y$, then $e^x = y$. In a `sigmoid function` We apply the following equation: $y = \\frac{1}{1 + e^{-x}}$ to determine:\n",
    "- if `x` is high, the value lies closer to, or equal to 1. \n",
    "- if `x` is low , the value lies closer to, or equal to 0.  \n",
    "\n",
    "**Let's see what this looks like in python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hanlde the imports \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of a sigmoid function. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample testing the sigmoid function with a range\n",
    "# of values for demonstration purposes. \n",
    "values = [-1, 0, 1, 3, 5, 30.5, -25.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\t\tSigmoid\t Rounded_sigmoid\n",
      "----------------------------------------\n",
      "sigmoid(-1)\t0.26894\t 0\n",
      "sigmoid(0)\t0.5\t 0\n",
      "sigmoid(1)\t0.73106\t 1\n",
      "sigmoid(3)\t0.95257\t 1\n",
      "sigmoid(5)\t0.99331\t 1\n",
      "sigmoid(30.5)\t1.0\t 1\n",
      "sigmoid(-25.5)\t0.0\t 0\n"
     ]
    }
   ],
   "source": [
    "# Show a table out test values and the \n",
    "# generated output, also demonstrate the \n",
    "# rounding to display how a step function\n",
    "# would be interpreted. \n",
    "print(\"Test\\t\\tSigmoid\\t Rounded_sigmoid\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# loop over the vals \n",
    "for val in values:\n",
    "    print(f\"sigmoid({val})\\t{round(sigmoid(val),5)}\\t {round(sigmoid(val))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.02 - Hidden Layer activation (using binary `xor` operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static.javatpoint.com/tutorial/coa/images/logic-gates5.png)\n",
    "\n",
    "We will use the 'XOR' operator as our case for the multi-layer study. The following truth table used as reference. We will focus on the `feed-forward` process from the input layer to the hidden layer. We can declare the following upfront: \n",
    "- We have 2 inputs (x,y) as we have had all along.\n",
    "- We have 3 neurons in our hidden layer, with individual sum and activation functions, of course. \n",
    "\n",
    "\n",
    "**Let's see that in simple python code**\n",
    "\n",
    "- task 1 - define the inputs of the `xor` truth table \n",
    "- task 2 - define some weights (demonstration purposes here)\n",
    "- task 3 - perform the calculation loop to find:\n",
    "    - product of _(input * weight)_\n",
    "    - sigmoid of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Processing Instance: 0 - Inputs are: (0,0)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "x1\tw1\tx2\tw2\tmultiplier\t\t\tProduct\tSigmoid of Product \n",
      "----------------------------------------------------------------------------------------------------\n",
      "0\t-0.424\t0\t0.358\t(0 * -0.424) + (0 * 0.358)\t0.0\t0.5 \n",
      "0\t-0.74\t0\t-0.577\t(0 * -0.74) + (0 * -0.577)\t-0.0\t0.5 \n",
      "0\t-0.961\t0\t-0.469\t(0 * -0.961) + (0 * -0.469)\t-0.0\t0.5 \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing Instance: 1 - Inputs are: (0,1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "x1\tw1\tx2\tw2\tmultiplier\t\t\tProduct\tSigmoid of Product \n",
      "----------------------------------------------------------------------------------------------------\n",
      "0\t-0.424\t1\t0.358\t(0 * -0.424) + (1 * 0.358)\t0.358\t0.5885562043858291 \n",
      "0\t-0.74\t1\t-0.577\t(0 * -0.74) + (1 * -0.577)\t-0.577\t0.3596231853677901 \n",
      "0\t-0.961\t1\t-0.469\t(0 * -0.961) + (1 * -0.469)\t-0.469\t0.38485295749078957 \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing Instance: 2 - Inputs are: (1,0)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "x1\tw1\tx2\tw2\tmultiplier\t\t\tProduct\tSigmoid of Product \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1\t-0.424\t0\t0.358\t(1 * -0.424) + (0 * 0.358)\t-0.424\t0.39555998258063735 \n",
      "1\t-0.74\t0\t-0.577\t(1 * -0.74) + (0 * -0.577)\t-0.74\t0.323004143761477 \n",
      "1\t-0.961\t0\t-0.469\t(1 * -0.961) + (0 * -0.469)\t-0.961\t0.2766780228949468 \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing Instance: 3 - Inputs are: (1,1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "x1\tw1\tx2\tw2\tmultiplier\t\t\tProduct\tSigmoid of Product \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1\t-0.424\t1\t0.358\t(1 * -0.424) + (1 * 0.358)\t-0.066\t0.4835059868921233 \n",
      "1\t-0.74\t1\t-0.577\t(1 * -0.74) + (1 * -0.577)\t-1.317\t0.21131784831127748 \n",
      "1\t-0.961\t1\t-0.469\t(1 * -0.961) + (1 * -0.469)\t-1.43\t0.19309868423321644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To show a table of inputs, weights and products\n",
    "def table_border():\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# declare the inputs for a xor truth table     \n",
    "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
    "\n",
    "# define some weights. These are random numbers for demo purposes. \n",
    "weights0 = [(-0.424, 0.358), (-0.740, -0.577), (-0.961, -0.469)]\n",
    "\n",
    "# create a table of outcomes \n",
    "for idx, i in enumerate(inputs):\n",
    "    x1,x2 = i\n",
    "    table_border()\n",
    "    print(f\"Processing Instance: {idx} - Inputs are: ({x1},{x2})\")\n",
    "    table_border()\n",
    "    print(f\"x1\\tw1\\tx2\\tw2\\tmultiplier\\t\\t\\tProduct\\tSigmoid of Product \")\n",
    "    table_border()\n",
    "    for w1,w2 in weights0:\n",
    "        product = (x1 * w1) + (x2 * w2)\n",
    "        sig = sigmoid(product)\n",
    "        print(f\"{x1}\\t{w1}\\t{x2}\\t{w2}\\t({x1} * {w1}) + ({x2} * {w2})\\t{product}\\t{sig} \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the results, or `products` of the `inputs` * `weights` above, we call the sigmoid function to get the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.03 - Multilayer Perceptron Implementation steps \n",
    "\n",
    "We have covered the theory of passing from the inputs to the hidden layer, the calculation of the inputs and weights to generate a sum and the sigmoid to get an activation value. We can now move to present that in slightly more robust python code and utilizing the `numpy` library because it's much more performant, an industry standard and well, pretty awesome too. Here are the tasks.\n",
    "\n",
    "1. create the inputs \n",
    "2. create the outputs \n",
    "3. create the np.array(weights_for_each_input)\n",
    "4. create the np.array(weights_for_each_hidden_layer_to_output)\n",
    "5. create the epochs threshold. \n",
    "6. create the sum_synapse0 as np.dot(inputs, weights_for_each_input)\n",
    "7. create the hidden_layer results as sigmoid(sum_synapse0) _see Euler's number_ \n",
    "8. apply the sum function to each of the hidden layer results (sigmoid) and activation application as sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "9. define the error_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the inputs data \n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.array([[0], [1], [1], [0]])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights0 \n",
    "# These are the input-X weights & input-Y weights respectively \n",
    "# they are the weights used in the sum function to generate a \n",
    "# product value. \n",
    "weights0 = np.array([[-0.424, -0.740, -0.961], \n",
    "                     [0.358, -0.577, -0.469]])\n",
    "\n",
    "\n",
    "# weights1 \n",
    "# these are hardcoded in the class lecture of this example. \n",
    "# They are the weights we see used between the hidden layer\n",
    "# and the output layer at the end of our operation. The \n",
    "# figures used are for demonstration purposes so don't get \n",
    "# hung up on what these particular numbers mean. They are \n",
    "# just demo weights. \n",
    "weights1 = np.array([[-0.017], \n",
    "                     [-0.893], \n",
    "                     [0.148]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Why do we set an epochs limit?\n",
    "We set the epochs limit to control the amount of times we'll allow the algorithm to run. Epoch thresholds are used to prevent infinite loops in search of a perfect prediction because in a lot of cases in machine learning we will not achieve a 100% perfect algorithm. I guess we can take some clues from the fact we use ML to make `predictions` of outcomes rather than determine factual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epochs limit.\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start the sum of the communication between the input layer and the hidden layer. This is basically a matrix multiplication exercise. \n",
    "\n",
    "In the proof of concept above in the intro section we are doing this with `for loops` and gather in the values of `x1`, `x2`, `w1` & `w2` which creates a results of: _for each input_layer * each weights0 value_. Here we use the numpy method `np.dot()` as it is far more highly optimised than using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.358, -0.577, -0.469],\n",
       "       [-0.424, -0.74 , -0.961],\n",
       "       [-0.066, -1.317, -1.43 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = inputs\n",
    "\n",
    "# starts the sum of the communication between the input layer and the \n",
    "# hidden layer. This is basically a matrix multiplication exercise. \n",
    "# Creating the results of: \n",
    "#     \"for each input_layer * each weights0\"  \n",
    "# Note: using the np.dot is more optimised than using a for loop.\n",
    "\n",
    "# sum_synapse0 is what we called product above the name sum_synapse0 \n",
    "# is more descriptive because it is the sum of the synapse at level 0\n",
    "# which is the first layer, or the input to hidden layer. \n",
    "sum_synapse0 = np.dot(input_layer, weights0)\n",
    "sum_synapse0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the hidden layer values, these are the values\n",
    "# returned from the sigmoid of the sum_synapse0\n",
    "\n",
    "hidden_layer = sigmoid(sum_synapse0)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.381     ],\n",
       "       [-0.27419072],\n",
       "       [-0.25421887],\n",
       "       [-0.16834784]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the sum_synapse1 values. These are the values that \n",
    "# are generated from the hidden layer to the output layer \n",
    "# and are considered as the final results of the neural \n",
    "# network for each of the items in our dataset. \n",
    "sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "sum_synapse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net Prediction\tRounded\n",
      "----------------------------------------\n",
      "0.40588573188433286\t0\n",
      "0.43187856951314224\t0\n",
      "0.43678536461116163\t0\n",
      "0.4580121591884929\t0\n"
     ]
    }
   ],
   "source": [
    "# We create the output layer or as it's often called the prediction of the\n",
    "# neural network by applying the sigmoid to the sum_synapse1 value. \n",
    "output_layer  = sigmoid(sum_synapse1)\n",
    "\n",
    "print(\"Neural net Prediction\\tRounded\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(len(output_layer)):\n",
    "    print(f\"{float(output_layer[i])}\\t{round(float(output_layer[i]))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1wfrCqtzef-qBymeaeIApeI_9VlxlbV0E)\n",
    "\n",
    "Now we have the results of the `sigmoid` from applying the inputs and weights we apply the `sum function` and `activation function` again to achieve the `sum_synapse1` or outputs and the `sigmoid(output)` will give us the neural network's prediction. Let's work through a manual calculation process to increase our comprehension before adding more complexity to our python implementation.\n",
    "\n",
    "\n",
    "\n",
    "<h1><center><i>We made a prediction!</i></center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual calculation of output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00711903"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(float(weights1[0]), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionised the descriptor of the process up until this point.\n",
    "# func accepts the index of the instance\n",
    "def processing_description(i_idx):\n",
    "\n",
    "    print(f\"[Step Definition] Inputs Passing\")\n",
    "    input1 = inputs[i_idx][0]\n",
    "    input2 = inputs[i_idx][1]\n",
    "    print(f\"    [001.001] Input1: {input1}\")\n",
    "    print(f\"    [001.002] Input2: {input2}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] weights0 application\")\n",
    "    for n_idx in range(3):\n",
    "        print(f\"    [002.00{n_idx}] sum function for neuron{n_idx}: ({input1} * {weights0[:,n_idx][0]}) + ({input2} * {weights0[:,n_idx][1]}) = {sum_synapse0[i_idx][n_idx]}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Apply sigmoid to get activation function values\")\n",
    "    for n_idx in range(3):\n",
    "        print(f\"    [003.00{n_idx}] Activation value neuron{n_idx}: sigmoid({sum_synapse0[i_idx][n_idx]}) = {hidden_layer[i_idx][n_idx]}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\")\n",
    "    for n_idx in range(3):\n",
    "        print(f\"    [004.001] sum_synpase1 for neuron{n_idx}: {hidden_layer[i_idx][n_idx]} * {float(weights1[n_idx])} = {hidden_layer[i_idx][n_idx] * float(weights1[n_idx])}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Get output as sum of sum_synpase1\")\n",
    "    desc = f\"({hidden_layer[i_idx][0]} * {float(weights1[0])}) + ({hidden_layer[i_idx][1]} * {float(weights1[1])}) + ({hidden_layer[i_idx][2]} * {float(weights1[2])})\"\n",
    "    ss1 = float((hidden_layer[i_idx][0] * weights1[0]) + (hidden_layer[i_idx][1] * weights1[1]) + (hidden_layer[i_idx][2] * weights1[2]))\n",
    "    print(f\"    [005.001] describe ouput calculation: {desc}\")\n",
    "    print(f\"    [005.002] Sum the sum_synpase1 as Output: {ss1}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Apply sigmoid to output totals to create prediction\")\n",
    "    print(f\"    [006.001] sigmoid(output): {ss1}\")\n",
    "    print(f\"    [006.002] Neural network prediction: {float(sigmoid(ss1))}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 0\n",
      "    [001.002] Input2: 0\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (0 * -0.424) + (0 * 0.358) = 0.0\n",
      "    [002.001] sum function for neuron1: (0 * -0.74) + (0 * -0.577) = 0.0\n",
      "    [002.002] sum function for neuron2: (0 * -0.961) + (0 * -0.469) = 0.0\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(0.0) = 0.5\n",
      "    [003.001] Activation value neuron1: sigmoid(0.0) = 0.5\n",
      "    [003.002] Activation value neuron2: sigmoid(0.0) = 0.5\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.5 * -0.007119029167563903 = -0.0035595145837819513\n",
      "    [004.001] sum_synpase1 for neuron1: 0.5 * -0.8864244669130955 = -0.44321223345654776\n",
      "    [004.001] sum_synpase1 for neuron2: 0.5 * 0.1543264410978047 = 0.07716322054890234\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.5 * -0.007119029167563903) + (0.5 * -0.8864244669130955) + (0.5 * 0.1543264410978047)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.36960852749142736\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "    [006.001] sigmoid(output): -0.36960852749142736\n",
      "    [006.002] Neural network prediction: 0.408635618512468\n"
     ]
    }
   ],
   "source": [
    "processing_description(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 0\n",
      "    [001.002] Input2: 1\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (0 * -0.424) + (1 * 0.358) = 0.358\n",
      "    [002.001] sum function for neuron1: (0 * -0.74) + (1 * -0.577) = -0.577\n",
      "    [002.002] sum function for neuron2: (0 * -0.961) + (1 * -0.469) = -0.469\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(0.358) = 0.5885562043858291\n",
      "    [003.001] Activation value neuron1: sigmoid(-0.577) = 0.3596231853677901\n",
      "    [003.002] Activation value neuron2: sigmoid(-0.469) = 0.38485295749078957\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.5885562043858291 * -0.007119029167563903 = -0.004189948785773419\n",
      "    [004.001] sum_synpase1 for neuron1: 0.3596231853677901 * -0.8864244669130955 = -0.3187787903792327\n",
      "    [004.001] sum_synpase1 for neuron2: 0.38485295749078957 * 0.1543264410978047 = 0.05939298727551827\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.5885562043858291 * -0.007119029167563903) + (0.3596231853677901 * -0.8864244669130955) + (0.38485295749078957 * 0.1543264410978047)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.26357575188948784\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "    [006.001] sigmoid(output): -0.26357575188948784\n",
      "    [006.002] Neural network prediction: 0.4344849132267969\n"
     ]
    }
   ],
   "source": [
    "processing_description(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 1\n",
      "    [001.002] Input2: 0\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (1 * -0.424) + (0 * 0.358) = -0.424\n",
      "    [002.001] sum function for neuron1: (1 * -0.74) + (0 * -0.577) = -0.74\n",
      "    [002.002] sum function for neuron2: (1 * -0.961) + (0 * -0.469) = -0.961\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(-0.424) = 0.39555998258063735\n",
      "    [003.001] Activation value neuron1: sigmoid(-0.74) = 0.323004143761477\n",
      "    [003.002] Activation value neuron2: sigmoid(-0.961) = 0.2766780228949468\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.39555998258063735 * -0.007119029167563903 = -0.0028160030535126267\n",
      "    [004.001] sum_synpase1 for neuron1: 0.323004143761477 * -0.8864244669130955 = -0.2863187759444881\n",
      "    [004.001] sum_synpase1 for neuron2: 0.2766780228949468 * 0.1543264410978047 = 0.04269873460335406\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.39555998258063735 * -0.007119029167563903) + (0.323004143761477 * -0.8864244669130955) + (0.2766780228949468 * 0.1543264410978047)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.24643604439464672\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "    [006.001] sigmoid(output): -0.24643604439464672\n",
      "    [006.002] Neural network prediction: 0.4387009035571403\n"
     ]
    }
   ],
   "source": [
    "processing_description(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 1\n",
      "    [001.002] Input2: 1\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (1 * -0.424) + (1 * 0.358) = -0.066\n",
      "    [002.001] sum function for neuron1: (1 * -0.74) + (1 * -0.577) = -1.317\n",
      "    [002.002] sum function for neuron2: (1 * -0.961) + (1 * -0.469) = -1.43\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(-0.066) = 0.4835059868921233\n",
      "    [003.001] Activation value neuron1: sigmoid(-1.317) = 0.21131784831127748\n",
      "    [003.002] Activation value neuron2: sigmoid(-1.43) = 0.19309868423321644\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.4835059868921233 * -0.007119029167563903 = -0.003442093223376796\n",
      "    [004.001] sum_synpase1 for neuron1: 0.21131784831127748 * -0.8864244669130955 = -0.1873173110385465\n",
      "    [004.001] sum_synpase1 for neuron2: 0.19309868423321644 * 0.1543264410978047 = 0.029800232718381062\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.4835059868921233 * -0.007119029167563903) + (0.21131784831127748 * -0.8864244669130955) + (0.19309868423321644 * 0.1543264410978047)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.16095917154354225\n",
      "\n",
      "[Step Definition] Apply sigmoid to output totals to create prediction\n",
      "    [006.001] sigmoid(output): -0.16095917154354225\n",
      "    [006.002] Neural network prediction: 0.459846859848677\n"
     ]
    }
   ],
   "source": [
    "processing_description(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Index\tNeuron Calculation\tPrediction\n",
      "--------------------------------------------------\n",
      "0\t\t-0.36960853\t\t\t0.40863562\n",
      "1\t\t-0.26357576\t\t\t0.43448491\n",
      "2\t\t-0.24643604\t\t\t0.4387009\n",
      "3\t\t-0.16095917\t\t\t0.45984686\n"
     ]
    }
   ],
   "source": [
    "# for all the arrays out hidden_layer values show a summary of above \n",
    "# processes to have the neuron calculations (or outputs) and the \n",
    "# predictions handy for reference. \n",
    "arrs = np.array([\n",
    "        [0.5       , 0.5       , 0.5       ],\n",
    "        [0.5885562 , 0.35962319, 0.38485296],\n",
    "        [0.39555998, 0.32300414, 0.27667802],\n",
    "        [0.48350599, 0.21131785, 0.19309868]])\n",
    "\n",
    "print(\"Neuron Index\\tNeuron Calculation\\tPrediction\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(arrs)):\n",
    "    arr = arrs[i]\n",
    "    neuron_calculations = float(arr.dot(weights1))\n",
    "    prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "    print(f\"{i}\\t\\t{round(neuron_calculations, 8)}\\t\\t\\t{round(prediction, 8)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.05 - Multi-layered Perceptron Basic Algorithm\n",
    "\n",
    "- 3.05.01. Error function (`Cost function` or `Loss function`)\n",
    "- 3.05.02. Gradient descent\n",
    "- 3.05.03. Derivative \n",
    "- 3.05.04. Delta\n",
    "- 3.05.05. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.05.01 - Error Functions (`Cost function` or `Loss function`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the error (_cost or loss_) by comparing the results of the predictions with the outputs of the dataset. The simplest formula is `error = correct - prediction`\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=19KkCREOLUUGuFcm5SsP074mEtDjE3H3-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1\tInput2\tExpected\tPrediction\tError\n",
      "------------------------------------------------------------\n",
      "0\t0\t[0]\t\t[0.40588573]\t[-0.40588573]\n",
      "0\t1\t[1]\t\t[0.43187857]\t[0.56812143]\n",
      "1\t0\t[1]\t\t[0.43678536]\t[0.56321464]\n",
      "1\t1\t[0]\t\t[0.45801216]\t[-0.45801216]\n"
     ]
    }
   ],
   "source": [
    "# create a list to store the alculated error rate of our predictions \n",
    "# whilst remembering that the formula: error = (correct - prediction)\n",
    "# is applied to get the error result. \n",
    "errs = []\n",
    "\n",
    "# create a table that shows the inputs, the expected output, \n",
    "# the neural neytworks prediction and how far away (error)\n",
    "# our model is from the truth. \n",
    "print(f\"Input1\\tInput2\\tExpected\\tPrediction\\tError\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(len(inputs)):\n",
    "    x, y = inputs[i]\n",
    "    exp = outputs[i]\n",
    "    out = output_layer[i]\n",
    "    err = exp - out\n",
    "    errs.append(err)\n",
    "    print(f\"{x}\\t{y}\\t{exp}\\t\\t{out}\\t{err}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the step of calculating the average error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the outputs defined earlier (outputs are the correct results)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get out output layer (predictions)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the errors (error = correct - preditions)\n",
    "error_output_layer = outputs - output_layer\n",
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error average: 0.49880848923713045\n",
      "Error average (rounded) : 0.499\n"
     ]
    }
   ],
   "source": [
    "# In order to get the average error we need to heed a reminder: \n",
    "#    ** we need to use the absolute values **\n",
    "# of the errors. If this is overlooked we will skew the results. \n",
    "\n",
    "error_avg = np.mean(abs(error_output_layer))\n",
    "print(f\"Error average: {error_avg}\")\n",
    "print(f\"Error average (rounded) : {round(error_avg, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.05.02 - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)\n",
    "\n",
    "\n",
    "The idea of gradient descent is to manage out cost function (loss function, error function) to get to the **smallest possible error** in the adjustment of the weights. The directional control of how a weight set should be adjusted is done by calculating the partial derivative as a means of determining the direction of a gradient. \n",
    "\n",
    "If you imagine a x,y axis graph with a curve, the **x-axis is the weight** and the **y-axis is the error value**, we are trying to achieve the lowest point of the curve, which may never be zero by the way, in a multi-dip curve we may have a local minimum and a global minimum across the span of measurements (number of epochs). So the purpose is to calculate the slope of a curve based on the partial derivatives.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Yong_Ma15/publication/267820876/figure/fig1/AS:669428953923612@1536615708709/Schematic-of-the-local-minima-problem-in-FWI-The-data-misfit-has-spurious-local-minima.png)\n",
    "\n",
    "- reminder of the sigmoid function: $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- calculating the partial derivative: $d = y \\cdot (1 -y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothetical example \n",
    "\n",
    "Assuming that `y` = 0.1 \n",
    "\n",
    "- calculating the partial derivative: $d = 0.1 \\cdot (1 -0.1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already declared above but I have brought the definition\n",
    "# down in order to prevent jumping about the notebook in \n",
    "# order to find earlier definitions. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid_derivative(sigmoid_value):\n",
    "        return sigmoid_value * (1 - sigmoid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sigmoid(0.5)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2350037122015945"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sigmoid_derivative(s)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.05-03 - Output layer Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of sequence is: \n",
    "\n",
    "- activation function (sigmoid) $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- Derivative $d = y \\cdot (1 -y)$\n",
    "\n",
    "- Delta $delta _{output} = error \\cdot sigmoid _{derivative}$\n",
    "- Gradient\n",
    "\n",
    "In the notebook above we walked through calculating the neuron calculations, we can now expand on that to walk through the error and the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the process walkthrough for `error`, `partial derivative` & `delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionised the descriptor of the process up until this point.\n",
    "# func accepts the index of the instance\n",
    "def processing_description_level2(i_idx):\n",
    "\n",
    "    print(f\"[Step Definition] Inputs Passing\")\n",
    "    input1 = inputs[i_idx][0]\n",
    "    input2 = inputs[i_idx][1]\n",
    "    print(f\"    [001.001] Input1: {input1}\")\n",
    "    print(f\"    [001.002] Input2: {input2}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] weights0 application\")\n",
    "    for n_idx in range(3):\n",
    "        weights0_0 = weights0[:,n_idx][0]\n",
    "        weights0_1 = weights0[:,n_idx][1]\n",
    "        print(f\"    [002.00{n_idx}] sum function for neuron{n_idx}: ({input1} * {weights0_0}) + ({input2} * {weights0_1}) = {sum_synapse0[i_idx][n_idx]}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Apply sigmoid to get activation function values\")\n",
    "    for n_idx in range(3):\n",
    "        print(f\"    [003.00{n_idx}] Activation value neuron{n_idx}: sigmoid({sum_synapse0[i_idx][n_idx]}) = {hidden_layer[i_idx][n_idx]}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\")\n",
    "    for n_idx in range(3):\n",
    "        print(f\"    [004.001] sum_synpase1 for neuron{n_idx}: {hidden_layer[i_idx][n_idx]} * {float(weights1[n_idx])} = {hidden_layer[i_idx][n_idx] * float(weights1[n_idx])}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Get output as sum of sum_synpase1\")\n",
    "    output_sum_synapse1 = float((hidden_layer[i_idx][0] * weights1[0]) + (hidden_layer[i_idx][1] * weights1[1]) + (hidden_layer[i_idx][2] * weights1[2]))\n",
    "    print(f\"    [005.001] describe ouput calculation: ({hidden_layer[i_idx][0]} * {float(weights1[0])}) + ({hidden_layer[i_idx][1]} * {float(weights1[1])}) + ({hidden_layer[i_idx][2]} * {float(weights1[2])})\")\n",
    "    print(f\"    [005.002] Sum the sum_synpase1 as Output: {output_sum_synapse1}\")\n",
    "\n",
    "    print(f\"\\n[Step Definition] Apply sigmoid to output total to create the prediction\")\n",
    "    nn_prediction = sigmoid(output_sum_synapse1)\n",
    "    print(f\"    [006.001] sigmoid({output_sum_synapse1}): {nn_prediction}\")\n",
    "    print(f\"    [006.002] Neural network prediction: {nn_prediction}\")\n",
    "    \n",
    "    print(f\"\\n[Step Definition] Calculate the error\")\n",
    "    err_val = outputs[i_idx] - nn_prediction\n",
    "    print(f\"    [007.001] Get error value: output[{i_idx}] - {nn_prediction} = {err_val}\")\n",
    "    \n",
    "    print(f\"\\n[Step Definition] Get the partial derivative\")\n",
    "    derivative = sigmoid_derivative(nn_prediction)\n",
    "    print(f\"    [008.001] Get the partial derivative: sigmoid_derivative({nn_prediction}) = {derivative}\")\n",
    "    print(f\"    [008.002] The partial derivative is: {derivative}\")\n",
    "    \n",
    "    print(f\"\\n[Step Definition] Get the delta\")\n",
    "    delta = err_val * derivative\n",
    "    print(f\"    [009.001] Get the delta: (err * derivative) : {err_val} * {derivative} = {err_val - derivative}\")\n",
    "    print(f\"    [009.002] The delta is: {delta}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 0\n",
      "    [001.002] Input2: 0\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (0 * -0.424) + (0 * 0.358) = 0.0\n",
      "    [002.001] sum function for neuron1: (0 * -0.74) + (0 * -0.577) = 0.0\n",
      "    [002.002] sum function for neuron2: (0 * -0.961) + (0 * -0.469) = 0.0\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(0.0) = 0.5\n",
      "    [003.001] Activation value neuron1: sigmoid(0.0) = 0.5\n",
      "    [003.002] Activation value neuron2: sigmoid(0.0) = 0.5\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.5 * -0.017 = -0.0085\n",
      "    [004.001] sum_synpase1 for neuron1: 0.5 * -0.893 = -0.4465\n",
      "    [004.001] sum_synpase1 for neuron2: 0.5 * 0.148 = 0.074\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.5 * -0.017) + (0.5 * -0.893) + (0.5 * 0.148)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.381\n",
      "\n",
      "[Step Definition] Apply sigmoid to output total to create the prediction\n",
      "    [006.001] sigmoid(-0.381): 0.40588573188433286\n",
      "    [006.002] Neural network prediction: 0.40588573188433286\n",
      "\n",
      "[Step Definition] Calculate the error\n",
      "    [007.001] Get error value: output[0] - 0.40588573188433286 = [-0.40588573]\n",
      "\n",
      "[Step Definition] Get the partial derivative\n",
      "    [008.001] Get the partial derivative: sigmoid_derivative(0.40588573188433286) = 0.24114250453705233\n",
      "    [008.002] The partial derivative is: 0.24114250453705233\n",
      "\n",
      "[Step Definition] Get the delta\n",
      "    [009.001] Get the delta: (err * derivative) : [-0.40588573] * 0.24114250453705233 = [-0.64702824]\n",
      "    [009.002] The delta is: [-0.0978763]\n"
     ]
    }
   ],
   "source": [
    "processing_description_level2(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 0\n",
      "    [001.002] Input2: 1\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (0 * -0.424) + (1 * 0.358) = 0.358\n",
      "    [002.001] sum function for neuron1: (0 * -0.74) + (1 * -0.577) = -0.577\n",
      "    [002.002] sum function for neuron2: (0 * -0.961) + (1 * -0.469) = -0.469\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(0.358) = 0.5885562043858291\n",
      "    [003.001] Activation value neuron1: sigmoid(-0.577) = 0.3596231853677901\n",
      "    [003.002] Activation value neuron2: sigmoid(-0.469) = 0.38485295749078957\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.5885562043858291 * -0.017 = -0.010005455474559095\n",
      "    [004.001] sum_synpase1 for neuron1: 0.3596231853677901 * -0.893 = -0.32114350453343654\n",
      "    [004.001] sum_synpase1 for neuron2: 0.38485295749078957 * 0.148 = 0.05695823770863685\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.5885562043858291 * -0.017) + (0.3596231853677901 * -0.893) + (0.38485295749078957 * 0.148)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.2741907222993588\n",
      "\n",
      "[Step Definition] Apply sigmoid to output total to create the prediction\n",
      "    [006.001] sigmoid(-0.2741907222993588): 0.43187856951314224\n",
      "    [006.002] Neural network prediction: 0.43187856951314224\n",
      "\n",
      "[Step Definition] Calculate the error\n",
      "    [007.001] Get error value: output[1] - 0.43187856951314224 = [0.56812143]\n",
      "\n",
      "[Step Definition] Get the partial derivative\n",
      "    [008.001] Get the partial derivative: sigmoid_derivative(0.43187856951314224) = 0.24535947070842423\n",
      "    [008.002] The partial derivative is: 0.24535947070842423\n",
      "\n",
      "[Step Definition] Get the delta\n",
      "    [009.001] Get the delta: (err * derivative) : [0.56812143] * 0.24535947070842423 = [0.32276196]\n",
      "    [009.002] The delta is: [0.13939397]\n"
     ]
    }
   ],
   "source": [
    "processing_description_level2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 1\n",
      "    [001.002] Input2: 0\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (1 * -0.424) + (0 * 0.358) = -0.424\n",
      "    [002.001] sum function for neuron1: (1 * -0.74) + (0 * -0.577) = -0.74\n",
      "    [002.002] sum function for neuron2: (1 * -0.961) + (0 * -0.469) = -0.961\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(-0.424) = 0.39555998258063735\n",
      "    [003.001] Activation value neuron1: sigmoid(-0.74) = 0.323004143761477\n",
      "    [003.002] Activation value neuron2: sigmoid(-0.961) = 0.2766780228949468\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.39555998258063735 * -0.017 = -0.006724519703870836\n",
      "    [004.001] sum_synpase1 for neuron1: 0.323004143761477 * -0.893 = -0.288442700378999\n",
      "    [004.001] sum_synpase1 for neuron2: 0.2766780228949468 * 0.148 = 0.04094834738845213\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.39555998258063735 * -0.017) + (0.323004143761477 * -0.893) + (0.2766780228949468 * 0.148)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.2542188726944177\n",
      "\n",
      "[Step Definition] Apply sigmoid to output total to create the prediction\n",
      "    [006.001] sigmoid(-0.2542188726944177): 0.43678536461116163\n",
      "    [006.002] Neural network prediction: 0.43678536461116163\n",
      "\n",
      "[Step Definition] Calculate the error\n",
      "    [007.001] Get error value: output[2] - 0.43678536461116163 = [0.56321464]\n",
      "\n",
      "[Step Definition] Get the partial derivative\n",
      "    [008.001] Get the partial derivative: sigmoid_derivative(0.43678536461116163) = 0.2460039098726562\n",
      "    [008.002] The partial derivative is: 0.2460039098726562\n",
      "\n",
      "[Step Definition] Get the delta\n",
      "    [009.001] Get the delta: (err * derivative) : [0.56321464] * 0.2460039098726562 = [0.31721073]\n",
      "    [009.002] The delta is: [0.138553]\n"
     ]
    }
   ],
   "source": [
    "processing_description_level2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step Definition] Inputs Passing\n",
      "    [001.001] Input1: 1\n",
      "    [001.002] Input2: 1\n",
      "\n",
      "[Step Definition] weights0 application\n",
      "    [002.000] sum function for neuron0: (1 * -0.424) + (1 * 0.358) = -0.066\n",
      "    [002.001] sum function for neuron1: (1 * -0.74) + (1 * -0.577) = -1.317\n",
      "    [002.002] sum function for neuron2: (1 * -0.961) + (1 * -0.469) = -1.43\n",
      "\n",
      "[Step Definition] Apply sigmoid to get activation function values\n",
      "    [003.000] Activation value neuron0: sigmoid(-0.066) = 0.4835059868921233\n",
      "    [003.001] Activation value neuron1: sigmoid(-1.317) = 0.21131784831127748\n",
      "    [003.002] Activation value neuron2: sigmoid(-1.43) = 0.19309868423321644\n",
      "\n",
      "[Step Definition] Perform neuron calculations (activation * weight1) to generate sum_synapse1\n",
      "    [004.001] sum_synpase1 for neuron0: 0.4835059868921233 * -0.017 = -0.008219601777166097\n",
      "    [004.001] sum_synpase1 for neuron1: 0.21131784831127748 * -0.893 = -0.1887068385419708\n",
      "    [004.001] sum_synpase1 for neuron2: 0.19309868423321644 * 0.148 = 0.02857860526651603\n",
      "\n",
      "[Step Definition] Get output as sum of sum_synpase1\n",
      "    [005.001] describe ouput calculation: (0.4835059868921233 * -0.017) + (0.21131784831127748 * -0.893) + (0.19309868423321644 * 0.148)\n",
      "    [005.002] Sum the sum_synpase1 as Output: -0.16834783505262085\n",
      "\n",
      "[Step Definition] Apply sigmoid to output total to create the prediction\n",
      "    [006.001] sigmoid(-0.16834783505262085): 0.4580121591884929\n",
      "    [006.002] Neural network prediction: 0.4580121591884929\n",
      "\n",
      "[Step Definition] Calculate the error\n",
      "    [007.001] Get error value: output[3] - 0.4580121591884929 = [-0.45801216]\n",
      "\n",
      "[Step Definition] Get the partial derivative\n",
      "    [008.001] Get the partial derivative: sigmoid_derivative(0.4580121591884929) = 0.24823702122398755\n",
      "    [008.002] The partial derivative is: 0.24823702122398755\n",
      "\n",
      "[Step Definition] Get the delta\n",
      "    [009.001] Get the delta: (err * derivative) : [-0.45801216] * 0.24823702122398755 = [-0.70624918]\n",
      "    [009.002] The delta is: [-0.11369557]\n"
     ]
    }
   ],
   "source": [
    "processing_description_level2(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.05-04 - Delta implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the delta is **used to determine the direction of the gradient in order to update weights**. The process takes each instance of a case's dataset and returns a delta value:\n",
    "\n",
    "- To calculate the `derivative_output` we need to get the result of the `sigmoid_derivative(output_layer)`\n",
    "- to calculate the delta output we need to take the `error_output_layer` and multiply by the `derivative_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2411425 ],\n",
       "       [0.24535947],\n",
       "       [0.24600391],\n",
       "       [0.24823702]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_output = sigmoid_derivative(output_layer)\n",
    "derivative_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output = error_output_layer * derivative_output\n",
    "delta_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$delta _{hidden} = sigmoid _{derivative} \\cdot weight \\cdot delta _{output} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\tIter\tActivation\tDerivative\tWeight\tDelta\tPrediction\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "[(0, 0)] [0]\t0.5\t\t0.25\t\t-0.017\t-0.098\t0.0004165\n",
      "[(0, 0)] [1]\t0.5\t\t0.25\t\t-0.893\t-0.098\t0.0218785\n",
      "[(0, 0)] [2]\t0.5\t\t0.25\t\t0.148\t-0.098\t-0.003626\n",
      "\n",
      "[(0, 1)] [0]\t0.58856\t\t0.24216\t\t-0.017\t0.139\t-0.00057222\n",
      "[(0, 1)] [1]\t0.35962\t\t0.23029\t\t-0.893\t0.139\t-0.02858521\n",
      "[(0, 1)] [2]\t0.38485\t\t0.23674\t\t0.148\t0.139\t0.00487022\n",
      "\n",
      "[(1, 0)] [0]\t0.39556\t\t0.23909\t\t-0.017\t0.139\t-0.00056497\n",
      "[(1, 0)] [1]\t0.323\t\t0.21867\t\t-0.893\t0.139\t-0.02714285\n",
      "[(1, 0)] [2]\t0.27668\t\t0.20013\t\t0.148\t0.139\t0.00411707\n",
      "\n",
      "[(1, 1)] [0]\t0.48351\t\t0.24973\t\t-0.017\t-0.114\t0.00048398\n",
      "[(1, 1)] [1]\t0.21132\t\t0.16666\t\t-0.893\t-0.114\t0.01696632\n",
      "[(1, 1)] [2]\t0.1931\t\t0.15581\t\t0.148\t-0.114\t-0.00262883\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs\\tIter\\tActivation\\tDerivative\\tWeight\\tDelta\\tPrediction\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(len(hidden_layer)):\n",
    "    x,y = inputs[i]\n",
    "    arr = np.array(hidden_layer[i])\n",
    "    print(\"\")\n",
    "    for j in range(len(arr)):  \n",
    "        synapse = round(hidden_layer[i][j], 5)\n",
    "        sd_val = round(sigmoid_derivative(hidden_layer[i][j]), 5)\n",
    "        weight = float(weights1[j])\n",
    "        do = round(float(delta_output[i]), 3)\n",
    "        print(f\"[{x,y}] [{j}]\\t{synapse}\\t\\t{sd_val}\\t\\t{weight}\\t{do}\\t{round(float(sd_val * weight * do),8)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rationalise the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017],\n",
       "       [-0.893],\n",
       "       [ 0.148]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delta_output_weight_multiplier = delta_output.dot(weights1)\n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017, -0.893,  0.148]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the T denotes transposed, where cols become rows and rows become cols\n",
    "# this is basically a total reshaping allowing us to continue with the \n",
    "# weights array as a usable multiplier because if we attempt to calc\n",
    "# delta_output.dot(weights1) we have a shape mismatch. \n",
    "weights1T = weights1.T\n",
    "weights1T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights1     : (3, 1)\n",
      "Shape of weights1T    : (1, 3)\n",
      "Shape of delta_output : (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# check shapes \n",
    "print(f\"Shape of weights1     : {weights1.shape}\")\n",
    "print(f\"Shape of weights1T    : {weights1T.shape}\")\n",
    "print(f\"Shape of delta_output : {delta_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0016639 ,  0.08740354, -0.01448569],\n",
       "       [-0.0023697 , -0.12447882,  0.02063031],\n",
       "       [-0.0023554 , -0.12372783,  0.02050584],\n",
       "       [ 0.00193282,  0.10153015, -0.01682694]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output_weight_multiplier = delta_output.dot(weights1T)\n",
    "delta_output_weight_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a reminder of our functions, for the purposes of not having to traverse back up the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of a sigmoid function. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid_derivative(sigmoid):\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00041597,  0.02185088, -0.00362142],\n",
       "       [-0.00057384, -0.02866677,  0.00488404],\n",
       "       [-0.00056316, -0.02705587,  0.00410378],\n",
       "       [ 0.00048268,  0.01692128, -0.00262183]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_hidden_layer = delta_output_weight_multiplier * sigmoid_derivative(hidden_layer)\n",
    "delta_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.05.05 - Backpropagation (adjusting the weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have worked on the `feed forward` principle by applying weights to the input layer that lead to hidden layer calculations. Then feeding those results forward until we have an output layer total and `prediction`. Overall the process works from `left to right`. Backproagation, on the other hand, is the reverse of this flow in the sense that we will recalculate the weights updates based on results, weights and activation_function results as the input. Then apply them from `right to left`. We will use the formula: $weight_{n + 1} = weight_{n} + (input \\cdot delta \\cdot learning\\_rate)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a neural network the user, or programmer, determines the value of the learning rate. This rate defines the speed of the algorithm or how fast it will learn. Where the learning rate is: \n",
    "- High : convergence is fast but the risk is to lose the global minimum. \n",
    "- Low : convergence is slow but the risk of losing the global minimum is greatly reduced. \n",
    "\n",
    "Note: convergence means the neural network has reached the best result, or global minimum. To maximise a neural network's capability with its efficiency and avoiding setting the learning rate so high that it loses the global minimum many libraries will implement a dynamic learning rate that reduces the rate as the number of epochs increases, so a learning rate starts off higher and reduces as it nears the global minimum which is deemed the best of both worlds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the learning rate as 0.3\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE ALONG SAMPLE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "------------------------------------------------------------\n",
      "inputs:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "weights0:\n",
      " [[-0.424 -0.74  -0.961]\n",
      " [ 0.358 -0.577 -0.469]]\n",
      "\n",
      "sum_synapse0:\n",
      " [[ 0.     0.     0.   ]\n",
      " [ 0.358 -0.577 -0.469]\n",
      " [-0.424 -0.74  -0.961]\n",
      " [-0.066 -1.317 -1.43 ]]\n",
      "\n",
      "hidden_layer:\n",
      " [[0.5        0.5        0.5       ]\n",
      " [0.5885562  0.35962319 0.38485296]\n",
      " [0.39555998 0.32300414 0.27667802]\n",
      " [0.48350599 0.21131785 0.19309868]]\n",
      "\n",
      "weights1:\n",
      " [[-0.017]\n",
      " [-0.893]\n",
      " [ 0.148]]\n",
      "\n",
      "sum_synapse1:\n",
      " [[-0.381     ]\n",
      " [-0.27419072]\n",
      " [-0.25421887]\n",
      " [-0.16834784]]\n",
      "\n",
      "output_layer:\n",
      " [[0.40588573]\n",
      " [0.43187857]\n",
      " [0.43678536]\n",
      " [0.45801216]]\n",
      "\n",
      "derivative_outut:\n",
      " [[0.2411425 ]\n",
      " [0.24535947]\n",
      " [0.24600391]\n",
      " [0.24823702]]\n",
      "\n",
      "error_output_layer:\n",
      " [[-0.40588573]\n",
      " [ 0.56812143]\n",
      " [ 0.56321464]\n",
      " [-0.45801216]]\n",
      "\n",
      "delta_output:\n",
      " [[-0.0978763 ]\n",
      " [ 0.13939397]\n",
      " [ 0.138553  ]\n",
      " [-0.11369557]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets print off a summary of our strutures and values at this stage\n",
    "print(\"Summary\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"inputs:\\n {inputs}\\n\")\n",
    "print(f\"weights0:\\n {weights0}\\n\")\n",
    "print(f\"sum_synapse0:\\n {sum_synapse0}\\n\")\n",
    "print(f\"hidden_layer:\\n {hidden_layer}\\n\")\n",
    "print(f\"weights1:\\n {weights1}\\n\")\n",
    "print(f\"sum_synapse1:\\n {sum_synapse1}\\n\")\n",
    "print(f\"output_layer:\\n {output_layer}\\n\")\n",
    "print(f\"derivative_outut:\\n {derivative_output}\\n\")\n",
    "print(f\"error_output_layer:\\n {error_output_layer}\\n\")\n",
    "print(f\"delta_output:\\n {delta_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5885562 , 0.39555998, 0.48350599],\n",
       "       [0.5       , 0.35962319, 0.32300414, 0.21131785],\n",
       "       [0.5       , 0.38485296, 0.27667802, 0.19309868]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose the hidden layer so we can \n",
    "# work by neuron index across all instances\n",
    "# with the multiplier. \n",
    "hidden_layerT = hidden_layer.T\n",
    "hidden_layerT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03293657],\n",
       "       [0.02191844],\n",
       "       [0.02108814]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the deltas needed for the updated weights \n",
    "# calculations to be done. \n",
    "input_delta1_multiplier = hidden_layerT.dot(delta_output)\n",
    "input_delta1_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00711903],\n",
       "       [-0.88642447],\n",
       "       [ 0.15432644]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the weights from the \n",
    "# hidden layer to the output layer \n",
    "weights1 = weights1 + (input_delta1_multiplier * learning_rate) \n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00041597,  0.02185088, -0.00362142],\n",
       "       [-0.00057384, -0.02866677,  0.00488404],\n",
       "       [-0.00056316, -0.02705587,  0.00410378],\n",
       "       [ 0.00048268,  0.01692128, -0.00262183]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0004,  0.0219, -0.0036],\n",
       "       [-0.0006, -0.0287,  0.0049],\n",
       "       [-0.0006, -0.0271,  0.0041],\n",
       "       [ 0.0005,  0.0169, -0.0026]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round_(delta_hidden_layer, 4)\n",
    "#delta_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.06 - Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
