{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi layered perceptron](https://upload.wikimedia.org/wikipedia/commons/c/c2/MultiLayerNeuralNetworkBigger_english.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi-Layered Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.01 - Introduction to Multi-layer networks\n",
    "- 3.02 - Hidden layer activation\n",
    "- 3.03 - Multilayer Perceptron Implementation steps\n",
    "- 3.04 - Multilayered Perceptron basic Algorithm\n",
    "    - 3.04.01 - Error Functions (cost function, loss function)\n",
    "    - 3.04.02 - Gradient Descent\n",
    "    - 3.04.03 - Output Layer Delta\n",
    "    - 3.04.04 - Delta implementation in Python\n",
    "    - 3.04.05 - Backpropagation\n",
    "- 3.05 - Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.01 - Introduction to Multi-layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the depiction at the top of the workbook the main concepts of a multilayer perceptron. The basic principal is that there is at least 1 hidden layer. Each neuron in the hidden layer should have its own `sum function` & `activation function`. The elements are all connected! which means in the case of the first hidden layer that each neuron is connected to each of the inputs. In the case of subsequent hidden layers each neuron is connected to each neuron of the preceding layer, again each having its own `sum` and `activation` functions.  \n",
    "\n",
    "Finally our structure will conclude and this is called the output layer. This is fed the results of each of the neurons in the final hidden layer and this result is `the prediction` of our neural network.\n",
    "\n",
    "**key point$^1$**: a `sum function` is the result of multiplying an input value by the associated weight. In a single layer perceptron that means, a single sum and activation function for the inputs. In a multilayer perceptron where we can have `_n_ layers` that means the sum can be made of the multiplier by the inputs and the weights in the case of the first hidden layer and for subsequent layers it can be the sum of the neuron in that preceding layer multiplied by another weight between hidden layers.   \n",
    "\n",
    "**key point$^2$**: an `activation function` is a decision fork of evaluating a sum and deciding of that neuron is fired or not. In the single layer perceptron we seen a `step function` type of activation function. A step can have values of `0` or `1`. Another type of activation function is the `sigmoid function`. What is different here is that the result or activation can be between `0` and `1` and not stepped. That ability to touch all points between `0` and `1` means we need to work out exactly where on the line that value belongs. \n",
    "  \n",
    "**key point$^3$**: If we need to return negative values we can use the `hyperbolic tangent function`:\n",
    "- $y = \\frac{e^{x} - e^{-x} }{e^{x} + e^{-x}}$\n",
    "- evaluating the equation asks to replace the `x` with the value under evaluation and the return will be graded between `-1` & `1`. \n",
    "\n",
    "**Theory to remember**: The irrational number `e` is also known as `Eulerâ€™s number`. It is approximately 2.718281, and is the base of the natural logarithm, ln (this means that, if $x = \\ln y = \\log_e y$, then $e^x = y$. In a `sigmoid function` We apply the following equation: $y = \\frac{1}{1 + e^{-x}}$ to determine:\n",
    "- if `x` is high, the value lies closer to, or equal to 1. \n",
    "- if `x` is low , the value lies closer to, or equal to 0.  \n",
    "\n",
    "**Let's see what this looks like in python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-1)\t is:  0.2689414213699951\n",
      "sigmoid(0)\t is:  0.5\n",
      "sigmoid(1)\t is:  0.7310585786300049\n",
      "sigmoid(3)\t is:  0.9525741268224334\n",
      "sigmoid(5)\t is:  0.9933071490757153\n",
      "sigmoid(30.5)\t is:  0.9999999999999432\n",
      "sigmoid(-25.5)\t is:  8.423463754397692e-12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# definition of a sigmoid function. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))\n",
    "\n",
    "\n",
    "# sample testing the sigmoid function with a range of values. \n",
    "values = [-1, 0, 1, 3, 5, 30.5, -25.5]\n",
    "\n",
    "for val in values:\n",
    "    print(f\"sigmoid({val})\\t is:  {sigmoid(val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.02 - Hidden Layer activation (using binary `xor` operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static.javatpoint.com/tutorial/coa/images/logic-gates5.png)\n",
    "\n",
    "We will use the 'XOR' operator as our case for the multi-layer study. The following truth table used as reference. We will focus on the `feed-forward` process from the input layer to the hidden layer. We can declare the following upfront: \n",
    "- We have 2 inputs (x,y) as we have had all along.\n",
    "- We have 3 neurons in our hidden layer, with individual sum and activation functions, of course. \n",
    "\n",
    "\n",
    "**Let's see that in simple python code**\n",
    "\n",
    "- task 1 - define the inputs of the xor truth table \n",
    "- task 2 - define some weights (demonstration purposes here)\n",
    "- task 3 - perform the calculation loop to find:\n",
    "    - product of _(input * weight)_\n",
    "    - sigmoid of product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "index\tx1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "--------------------------------------------------------------------------------\n",
      "0\t0\t0\t-0.424\t0.358\t\t0.0\t0.5 \n",
      "0\t0\t0\t-0.74\t-0.577\t\t-0.0\t0.5 \n",
      "0\t0\t0\t-0.961\t-0.469\t\t-0.0\t0.5 \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "index\tx1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "--------------------------------------------------------------------------------\n",
      "1\t0\t1\t-0.424\t0.358\t\t0.358\t0.5885562043858291 \n",
      "1\t0\t1\t-0.74\t-0.577\t\t-0.577\t0.3596231853677901 \n",
      "1\t0\t1\t-0.961\t-0.469\t\t-0.469\t0.38485295749078957 \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "index\tx1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "--------------------------------------------------------------------------------\n",
      "2\t1\t0\t-0.424\t0.358\t\t-0.424\t0.39555998258063735 \n",
      "2\t1\t0\t-0.74\t-0.577\t\t-0.74\t0.323004143761477 \n",
      "2\t1\t0\t-0.961\t-0.469\t\t-0.961\t0.2766780228949468 \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "index\tx1\tx2\tw1\tw2\t\tProduct\tSigmoid \n",
      "--------------------------------------------------------------------------------\n",
      "3\t1\t1\t-0.424\t0.358\t\t-0.066\t0.4835059868921233 \n",
      "3\t1\t1\t-0.74\t-0.577\t\t-1.317\t0.21131784831127748 \n",
      "3\t1\t1\t-0.961\t-0.469\t\t-1.43\t0.19309868423321644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To show a table of inputs, weights and products\n",
    "def table_border():\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# declare the inputs for a xor truth table     \n",
    "inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
    "\n",
    "# define some weights. These are random numbers for demo purposes. \n",
    "weights0 = [(-0.424, 0.358), (-0.740, -0.577), (-0.961, -0.469)]\n",
    "\n",
    "# create a table of outcomes \n",
    "for idx, i in enumerate(inputs):\n",
    "    x1,x2 = i\n",
    "    table_border()\n",
    "    print(f\"index\\tx1\\tx2\\tw1\\tw2\\t\\tProduct\\tSigmoid \")\n",
    "    table_border()\n",
    "    for w1,w2 in weights0:\n",
    "        product = (x1 * w1) + (x2 * w2)\n",
    "        sig = sigmoid(product)\n",
    "        print(f\"{idx}\\t{x1}\\t{x2}\\t{w1}\\t{w2}\\t\\t{product}\\t{sig} \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the results, or `products` of the `inputs` * `weights` above, we call the sigmoid function to get the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.03 - Multilayer Perceptron Implementation steps \n",
    "\n",
    "We have covered the theory of passing from the inputs to the hidden layer, the calculation of the inputs and weights to generate a sum and the sigmoid to get an activation value. We can now move to present that in slightly more robust python code and utilizing the `numpy` library because it's much more performant, an industry standard and well, pretty awesome too. Here are the tasks.\n",
    "\n",
    "1. create the inputs \n",
    "2. create the outputs \n",
    "3. create the np.array(weights_for_each_input)\n",
    "4. create the np.array(weights_for_each_hidden_layer_to_output)\n",
    "5. create the epochs threshold. \n",
    "6. create the sum_synapse0 as np.dot(inputs, weights_for_each_input)\n",
    "7. create the hidden_layer results as sigmoid(sum_synapse0) _see Euler's number_ \n",
    "8. apply the sum function to each of the hidden layer results (sigmoid) and activation application as sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "9. define the error_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the inputs data \n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = np.array([[0], [1], [1], [0]])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights0 are inputX weights, inputY weghts respectively \n",
    "# these are the weights that are used in the sum function to \n",
    "# geberate a product value. \n",
    "weights0 = np.array([[-0.424, -0.740, -0.961], \n",
    "                     [0.358, -0.577, -0.469]])\n",
    "\n",
    "\n",
    "# weights1 are hardcoded in the class lecture of this example. \n",
    "# These are the weights we see used between the hidden layer\n",
    "# and the output layer at the end of our opoeration. The figures\n",
    "# are for demonstration purposes so don't get hung up on what \n",
    "# these particular numbers mean. They're just demo weights. \n",
    "weights1 = np.array([[-0.017], \n",
    "                     [-0.893], \n",
    "                     [0.148]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the epochs limit to control the amount of times we'll allow the algorithm to run. Epoch thresholds are used to prevent infinite loops in search of a perfect prediction because in a lot of cases in machine learning we will not achieve a 100% perfect algorithm. I guess we can take some clues from the fact we use ML to make `predictions` of outcomes rather than determine factual outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the epochs limit.\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start the sum of the communication between the input layer and the hidden layer. This is basically a matrix multiplication exercise. \n",
    "\n",
    "In the proof of concept above in the intro section we are doing this with `for loops` and gather in the values of `x1`, `x2`, `w1` & `w2` which creates a results of: _for each input_layer * each weights0_. Here we use the numpy method `np.dot()` as it is far more highly optimised than using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ],\n",
       "       [ 0.358, -0.577, -0.469],\n",
       "       [-0.424, -0.74 , -0.961],\n",
       "       [-0.066, -1.317, -1.43 ]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = inputs\n",
    "\n",
    "# starts the sum of the communication between the input\n",
    "# layer and the hidden layer. This is basically a matrix\n",
    "# multiplication exercise. Creating the results of: \n",
    "#     \"for each input_layer * each weights0\"  \n",
    "# Important note: using the np.dot is more highly \n",
    "# optimised than using a for loop.\n",
    "\n",
    "# sum_synapse0 is what we called product above\n",
    "# the name sum_synapse0 is more descriptive \n",
    "# because it is the sum of the synapse at level 0\n",
    "# which is the first layer, or the input to hidden\n",
    "# layer. \n",
    "sum_synapse0 = np.dot(input_layer, weights0)\n",
    "sum_synapse0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculates the hidden layer values, these are the values\n",
    "# returned from the sigmoid of the sum_synapse0\n",
    "\n",
    "hidden_layer = sigmoid(sum_synapse0)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the results of a the sigmoid from applying the inputs and weights we apply the sum funtion and activation function again to achieve the final outputs. Let's work through a manual calculation process to increase our comprehension before adding more complexity to our python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "------------------------------------------------------------\n",
      "inputs:\n",
      " [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "\n",
      "weights0:\n",
      " [(-0.424, 0.358), (-0.74, -0.577), (-0.961, -0.469)]\n",
      "\n",
      "sum_synapse0:\n",
      " [[ 0.     0.     0.   ]\n",
      " [ 0.358 -0.577 -0.469]\n",
      " [-0.424 -0.74  -0.961]\n",
      " [-0.066 -1.317 -1.43 ]]\n",
      "\n",
      "hidden_layer:\n",
      " [[0.5        0.5        0.5       ]\n",
      " [0.5885562  0.35962319 0.38485296]\n",
      " [0.39555998 0.32300414 0.27667802]\n",
      " [0.48350599 0.21131785 0.19309868]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual calculation of output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.381, 0.40588573188433286)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 0 in our dataset \n",
    "# inputs are (0,0) \n",
    "# sum_synapse0 = inputs * weights0 = (0, 0, 0)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.5, 0.5, 0.5)\n",
    "\n",
    "arr = np.array([0.5, 0.5, 0.5])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction\n",
    "\n",
    "# in our case it's 0.40588573188433286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.27419072598999994, 0.43187856860760854)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 1 in our dataset \n",
    "# inputs are (0,1) \n",
    "# sum_synapse0 = inputs * weights0 = (0.358, -0.577, -0.469)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.5885562 , 0.35962319, 0.38485296)\n",
    "\n",
    "\n",
    "arr = np.array([0.5885562 , 0.35962319, 0.38485296])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,1) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.25421886972, 0.43678536534288)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 2 in our dataset \n",
    "# inputs are (1,0) \n",
    "# sum_synapse0 = inputs * weights0 = (-0.424, -0.74 , -0.96)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.39555998, 0.32300414, 0.27667802)\n",
    "\n",
    "arr = np.array([0.39555998, 0.32300414, 0.27667802])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (1,0) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.16834783724, 0.4580121586455045)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example 3 in our dataset \n",
    "# inputs are (1,1) \n",
    "# sum_synapse0 = inputs * weights0 = (-0.066, -1.317, -1.43)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.48350599, 0.21131785, 0.19309868)\n",
    "\n",
    "arr = np.array([0.48350599, 0.21131785, 0.19309868])\n",
    "neuron_calculations = float(arr.dot(weights1))\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (1,1) from the dataset\n",
    "prediction = sigmoid(neuron_calculations)\n",
    "\n",
    "neuron_calculations, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create the sum_synapse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.381     ],\n",
       "       [-0.27419072],\n",
       "       [-0.25421887],\n",
       "       [-0.16834784]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the sum_synapse1 values. \n",
    "# These are the values that are generated\n",
    "# from the hidden layer to the output\n",
    "# layer and are considered as the final \n",
    "# results of the neural network for \n",
    "# each of the items in our dataset. \n",
    "sum_synapse1 = np.dot(hidden_layer, weights1)\n",
    "sum_synapse1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create the output layer or as it's \n",
    "# often called the prediction of the \n",
    "# neural network.\n",
    "output_layer  = sigmoid(sum_synapse1)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.04 - Multi-layered Perceptron Basic Algorithm\n",
    "\n",
    "- 3.04.01. Cost function (loss function)\n",
    "- 3.04.02. Gradient descent\n",
    "- 3.04.03. Derivative \n",
    "- 3.04.04. Delta\n",
    "- 3.04.05. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.04.01 - Error Functions (_loss function, cost function_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the error by comparing the results of the predictions with the outputs of the dataset. The error function is often referred to as the loss function in ML terminology. the simplest formula is `error = correct - prediction`\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1JqnDqu0T9k9-g87C_6dEHLoWItFK8D7J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1\tInput2\tExpected\tPrediction\tError\n",
      "------------------------------------------------------------\n",
      "0\t0\t[0]\t\t[0.40588573]\t[-0.40588573]\n",
      "0\t1\t[1]\t\t[0.43187857]\t[0.56812143]\n",
      "1\t0\t[1]\t\t[0.43678536]\t[0.56321464]\n",
      "1\t1\t[0]\t\t[0.45801216]\t[-0.45801216]\n"
     ]
    }
   ],
   "source": [
    "# create a list to store the alculated error rate of our predictions \n",
    "# whilst remembering that the formula: error = (correct - prediction)\n",
    "# is applied to get the error result. \n",
    "errs = []\n",
    "\n",
    "# create a table that shows the inputs, the expected output, \n",
    "# the neural neytworks prediction and how far away (error)\n",
    "# our model is from the truth. \n",
    "print(f\"Input1\\tInput2\\tExpected\\tPrediction\\tError\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(len(inputs)):\n",
    "    x, y = inputs[i]\n",
    "    exp = outputs[i]\n",
    "    out = output_layer[i]\n",
    "    err = exp - out\n",
    "    errs.append(err)\n",
    "    print(f\"{x}\\t{y}\\t{exp}\\t\\t{out}\\t{err}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the step of calculating the average error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab the outputs defined earlier (outputs are the correct results)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get out output layer (predictions)\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the errors (error = correct - preditions)\n",
    "error_output_layer = outputs - output_layer\n",
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error average: 0.49880848923713045\n",
      "Error average (rounded) : 0.499\n"
     ]
    }
   ],
   "source": [
    "# Get the average error. As a reminder we need to use the absolute values \n",
    "# of the errors. If this is overlooked we will skew the results. \n",
    "error_avg = np.mean(abs(error_output_layer))\n",
    "print(f\"Error average: {error_avg}\")\n",
    "print(f\"Error average (rounded) : {round(error_avg, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.04.02 - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)\n",
    "\n",
    "\n",
    "The idea of gradient descent is to manage out cost function (loss function, error function) to get to the **smallest possible error** in the adjustment of the weights. The directional control of how a weight set should be adjusted is done by calculating the partial derivative as a means of determining the direction of a gradient. \n",
    "\n",
    "If you imagine a x,y axis graph with a curve, the **x-axis is the weight** and the **y-axis is the error value**, we are trying to achieve the lowest point of the curve, which may never be zero by the way, in a multi-dip curve we may have a local minimum and a global minimum across the span of measurements (number of epochs). So the purpose is to calculate the slope of a curve based on the partial derivatives.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Yong_Ma15/publication/267820876/figure/fig1/AS:669428953923612@1536615708709/Schematic-of-the-local-minima-problem-in-FWI-The-data-misfit-has-spurious-local-minima.png)\n",
    "\n",
    "- reminder of the sigmoid function: $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- calculating the partial derivative: $d = y \\cdot (1 -y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothetical example \n",
    "\n",
    "Assuming that `y` = 0.1 \n",
    "\n",
    "- calculating the partial derivative: $d = 0.1 \\cdot (1 -0.1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid_derivative(sigmoid):\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224593312018546"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sigmoid(0.5)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2350037122015945"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sigmoid_derivative(s)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.04-03 - Output layer Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of sequence is: \n",
    "\n",
    "- activation function (sigmoid) $y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "- Derivative $d = y \\cdot (1 -y)$\n",
    "\n",
    "- Delta $delta _{output} = error \\cdot sigmoid _{derivative}$\n",
    "- Gradient\n",
    "\n",
    "In the notebook above we walked through calculating the neuron calculations, we can now expand on that to walk through the error and the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walkthrough for dataset example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) : [0.5, 0.5, 0.5]\n",
      "[Step 02.01] hidden_layer[0.5] * hidden_layer_weight[-0.017] : -0.0085 \n",
      "[Step 02.02] hidden_layer[0.5] * hidden_layer_weight[-0.893] : -0.4465 \n",
      "[Step 02.03] hidden_layer[0.5] * hidden_layer_weight[0.148] : 0.074 \n",
      "[step 02.04] Apply sum to calculated figures :  -0.381\n",
      "[Step 03.00] Calculate sigmoid_sum (sigmoid(-0.381)) :  0.40588573188433286\n",
      "[Step 04.00] Calculate Error : [0] - 0.40588573188433286 :  [-0.40588573]\n",
      "[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid(0.40588573188433286))) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [-0.40588573] * 0.24114250453705233 :  [-0.0978763]\n"
     ]
    }
   ],
   "source": [
    "# for example 0 in our dataset \n",
    "# inputs are (0,0) \n",
    "# sum_synapse0 = inputs * weights0 = (0, 0, 0)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.5, 0.5, 0.5)\n",
    "\n",
    "\n",
    "ex0_hidden_layer = [0.5, 0.5, 0.5]\n",
    "ex0_weights1 = [-0.017, -0.893, 0.148]\n",
    "print(\"[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) :\", ex0_hidden_layer)\n",
    "\n",
    "# Start output layer sequence steps \n",
    "# 1. Multiply by the weights between hidden layer and the output layer. \n",
    "# 2. With final result (prediction)\n",
    "prediction = ex0_hidden_layer[0] * ex0_weights1[0] + ex0_hidden_layer[1] * ex0_weights1[1] + ex0_hidden_layer[2] * ex0_weights1[2]\n",
    "print(f\"[Step 02.01] hidden_layer[{ex0_hidden_layer[0]}] * hidden_layer_weight[{ex0_weights1[0]}] : {ex0_hidden_layer[0] * ex0_weights1[0]} \")\n",
    "print(f\"[Step 02.02] hidden_layer[{ex0_hidden_layer[1]}] * hidden_layer_weight[{ex0_weights1[1]}] : {ex0_hidden_layer[1] * ex0_weights1[1]} \")\n",
    "print(f\"[Step 02.03] hidden_layer[{ex0_hidden_layer[2]}] * hidden_layer_weight[{ex0_weights1[2]}] : {ex0_hidden_layer[2] * ex0_weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", prediction)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(prediction)\n",
    "print(f\"[Step 03.00] Calculate sigmoid_sum (sigmoid({prediction})) : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[0] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Calculate Error : {outputs[0]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid({sigmoid_sum}))) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walkthrough for dataset example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) : [0.5885562, 0.35962319, 0.38485296]\n",
      "[Step 02.01] hidden_layer[0.5885562] * hidden_layer_weight[-0.017] : -0.0100054554 \n",
      "[Step 02.02] hidden_layer[0.35962319] * hidden_layer_weight[-0.893] : -0.32114350866999997 \n",
      "[Step 02.03] hidden_layer[0.38485296] * hidden_layer_weight[0.148] : 0.05695823808 \n",
      "[step 02.04] Apply sum to calculated figures :  -0.27419072598999994\n",
      "[Step 03.00] Calculate sigmoid_sum (sigmoid(-0.27419072598999994)) :  0.43187856860760854\n",
      "[Step 04.00] Calculate Error : [1] - 0.43187856860760854 :  [0.56812143]\n",
      "[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid(0.43187856860760854))) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [0.56812143] * 0.24114250453705233 :  [0.13699822]\n"
     ]
    }
   ],
   "source": [
    "# for example 1 in our dataset \n",
    "# inputs are (0,1) \n",
    "# sum_synapse0 = inputs * weights0 = (0.358, -0.577, -0.469)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.358, -0.577, -0.469)\n",
    "\n",
    "\n",
    "ex1_hidden_layer = [0.5885562 , 0.35962319, 0.38485296]\n",
    "ex1_weights1 = [-0.017, -0.893, 0.148]\n",
    "print(\"[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) :\", ex1_hidden_layer)\n",
    "\n",
    "# Start output layer sequence steps \n",
    "# 1. Multiply by the weights between hidden layer and the output layer. \n",
    "# 2. With final result (prediction)\n",
    "prediction = ex1_hidden_layer[0] * ex1_weights1[0] + ex1_hidden_layer[1] * ex1_weights1[1] + ex1_hidden_layer[2] * ex1_weights1[2]\n",
    "print(f\"[Step 02.01] hidden_layer[{ex1_hidden_layer[0]}] * hidden_layer_weight[{ex1_weights1[0]}] : {ex1_hidden_layer[0] * ex1_weights1[0]} \")\n",
    "print(f\"[Step 02.02] hidden_layer[{ex1_hidden_layer[1]}] * hidden_layer_weight[{ex1_weights1[1]}] : {ex1_hidden_layer[1] * ex1_weights1[1]} \")\n",
    "print(f\"[Step 02.03] hidden_layer[{ex1_hidden_layer[2]}] * hidden_layer_weight[{ex1_weights1[2]}] : {ex1_hidden_layer[2] * ex1_weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", prediction)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(prediction)\n",
    "print(f\"[Step 03.00] Calculate sigmoid_sum (sigmoid({prediction})) : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[1] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Calculate Error : {outputs[1]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid({sigmoid_sum}))) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walkthrough for dataset example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) : [0.39555998, 0.32300414, 0.27667802]\n",
      "[Step 02.01] hidden_layer[0.39555998] * hidden_layer_weight[-0.017] : -0.00672451966 \n",
      "[Step 02.02] hidden_layer[0.32300414] * hidden_layer_weight[-0.893] : -0.28844269702 \n",
      "[Step 02.03] hidden_layer[0.27667802] * hidden_layer_weight[0.148] : 0.04094834696 \n",
      "[step 02.04] Apply sum to calculated figures :  -0.25421886972\n",
      "[Step 03.00] Calculate sigmoid_sum (sigmoid(-0.25421886972)) :  0.43678536534288\n",
      "[Step 04.00] Calculate Error : [1] - 0.43678536534288 :  [0.56321463]\n",
      "[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid(0.43678536534288))) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [0.56321463] * 0.24114250453705233 :  [0.13581499]\n"
     ]
    }
   ],
   "source": [
    "# for example 2 in our dataset \n",
    "# inputs are (1,0) \n",
    "# sum_synapse0 = inputs * weights0 = (-0.424, -0.74 , -0.96)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.39555998, 0.32300414, 0.27667802)\n",
    "\n",
    "ex2_hidden_layer = [0.39555998, 0.32300414, 0.27667802]\n",
    "ex2_weights1 = [-0.017, -0.893, 0.148]\n",
    "print(\"[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) :\", ex2_hidden_layer)\n",
    "\n",
    "# Start output layer sequence steps \n",
    "# 1. Multiply by the weights between hidden layer and the output layer. \n",
    "# 2. With final result (prediction)\n",
    "prediction = ex2_hidden_layer[0] * ex2_weights1[0] + ex2_hidden_layer[1] * ex2_weights1[1] + ex2_hidden_layer[2] * ex2_weights1[2]\n",
    "print(f\"[Step 02.01] hidden_layer[{ex2_hidden_layer[0]}] * hidden_layer_weight[{ex2_weights1[0]}] : {ex2_hidden_layer[0] * ex2_weights1[0]} \")\n",
    "print(f\"[Step 02.02] hidden_layer[{ex2_hidden_layer[1]}] * hidden_layer_weight[{ex2_weights1[1]}] : {ex2_hidden_layer[1] * ex2_weights1[1]} \")\n",
    "print(f\"[Step 02.03] hidden_layer[{ex2_hidden_layer[2]}] * hidden_layer_weight[{ex2_weights1[2]}] : {ex2_hidden_layer[2] * ex2_weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", prediction)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(prediction)\n",
    "print(f\"[Step 03.00] Calculate sigmoid_sum (sigmoid({prediction})) : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[2] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Calculate Error : {outputs[2]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid({sigmoid_sum}))) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walkthrough for dataset example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) : [0.48350599, 0.21131785, 0.19309868]\n",
      "[Step 02.01] hidden_layer[0.48350599] * hidden_layer_weight[-0.017] : -0.008219601830000001 \n",
      "[Step 02.02] hidden_layer[0.21131785] * hidden_layer_weight[-0.893] : -0.18870684005 \n",
      "[Step 02.03] hidden_layer[0.19309868] * hidden_layer_weight[0.148] : 0.028578604639999998 \n",
      "[step 02.04] Apply sum to calculated figures :  -0.16834783724\n",
      "[Step 03.00] Calculate sigmoid_sum (sigmoid(-0.16834783724)) :  0.4580121586455045\n",
      "[Step 04.00] Calculate Error : [0] - 0.4580121586455045 :  [-0.45801216]\n",
      "[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid(0.4580121586455045))) :  0.24114250453705233\n",
      "[Step 06.00] Delta : (error * derivative) : [-0.45801216] * 0.24114250453705233 :  [-0.1104462]\n"
     ]
    }
   ],
   "source": [
    "# for example 3 in our dataset \n",
    "# inputs are (1,1) \n",
    "# sum_synapse0 = inputs * weights0 = (-0.066, -1.317, -1.43)\n",
    "# hidden_layer = sigmoid(sum_synapse0) = (0.48350599, 0.21131785, 0.19309868)\n",
    "\n",
    "ex3_hidden_layer = [0.48350599, 0.21131785, 0.19309868]\n",
    "ex3_weights1 = [-0.017, -0.893, 0.148]\n",
    "print(\"[Step 01.00] Calculate hidden layer (np.dot(input_layer, weights0)) :\", ex3_hidden_layer)\n",
    "\n",
    "# Start output layer sequence steps \n",
    "# 1. Multiply by the weights between hidden layer and the output layer. \n",
    "# 2. With final result (prediction)\n",
    "prediction = ex3_hidden_layer[0] * ex3_weights1[0] + ex3_hidden_layer[1] * ex3_weights1[1] + ex3_hidden_layer[2] * ex3_weights1[2]\n",
    "print(f\"[Step 02.01] hidden_layer[{ex3_hidden_layer[0]}] * hidden_layer_weight[{ex3_weights1[0]}] : {ex3_hidden_layer[0] * ex3_weights1[0]} \")\n",
    "print(f\"[Step 02.02] hidden_layer[{ex3_hidden_layer[1]}] * hidden_layer_weight[{ex3_weights1[1]}] : {ex3_hidden_layer[1] * ex3_weights1[1]} \")\n",
    "print(f\"[Step 02.03] hidden_layer[{ex3_hidden_layer[2]}] * hidden_layer_weight[{ex3_weights1[2]}] : {ex3_hidden_layer[2] * ex3_weights1[2]} \")\n",
    "print(f\"[step 02.04] Apply sum to calculated figures : \", prediction)\n",
    "\n",
    "# get the sigmoid. This produces the final result of \n",
    "# the neural network, or the prediction for example (0,0) from the dataset\n",
    "sigmoid_sum = sigmoid(prediction)\n",
    "print(f\"[Step 03.00] Calculate sigmoid_sum (sigmoid({prediction})) : \", sigmoid_sum)\n",
    "\n",
    "error = outputs[3] - sigmoid_sum\n",
    "print(f\"[Step 04.00] Calculate Error : {outputs[3]} - {sigmoid_sum} : \", error)\n",
    "\n",
    "# get the derivative \n",
    "derivative = sigmoid_derivative(result)\n",
    "print(f\"[Step 05.00] Calculate Derivative (sigmoid_derivative(sigmoid({sigmoid_sum}))) : \", derivative)\n",
    "\n",
    "# get the delta \n",
    "delta = error * derivative\n",
    "print(f\"[Step 06.00] Delta : (error * derivative) : {error} * {derivative} : \", delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.04-04 - Delta implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the delta is **used to determine the direction of the gradient in order to update weights**. The process takes each instance of a case's dataset and returns a delta value:\n",
    "\n",
    "- To calculate the `derivative_output` we need to get the result of the `sigmoid_derivative(output_layer)`\n",
    "- to calculate the delta output we need to take the `error_output_layer` and multiply by the `derivative_output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40588573],\n",
       "       [0.43187857],\n",
       "       [0.43678536],\n",
       "       [0.45801216]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2411425 ],\n",
       "       [0.24535947],\n",
       "       [0.24600391],\n",
       "       [0.24823702]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_output = sigmoid_derivative(output_layer)\n",
    "derivative_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output = error_output_layer * derivative_output\n",
    "delta_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$delta _{hidden} = sigmoid _{derivative} \\cdot weight \\cdot delta _{output} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\tIter\tActivation\tDerivative\tWeight\tDelta\tPrediction\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "[(0, 0)] [0]\t0.5\t\t0.25\t\t-0.017\t-0.098\t0.0004165\n",
      "[(0, 0)] [1]\t0.5\t\t0.25\t\t-0.893\t-0.098\t0.0218785\n",
      "[(0, 0)] [2]\t0.5\t\t0.25\t\t0.148\t-0.098\t-0.003626\n",
      "\n",
      "[(0, 1)] [0]\t0.58856\t\t0.24216\t\t-0.017\t0.139\t-0.00057222\n",
      "[(0, 1)] [1]\t0.35962\t\t0.23029\t\t-0.893\t0.139\t-0.02858521\n",
      "[(0, 1)] [2]\t0.38485\t\t0.23674\t\t0.148\t0.139\t0.00487022\n",
      "\n",
      "[(1, 0)] [0]\t0.39556\t\t0.23909\t\t-0.017\t0.139\t-0.00056497\n",
      "[(1, 0)] [1]\t0.323\t\t0.21867\t\t-0.893\t0.139\t-0.02714285\n",
      "[(1, 0)] [2]\t0.27668\t\t0.20013\t\t0.148\t0.139\t0.00411707\n",
      "\n",
      "[(1, 1)] [0]\t0.48351\t\t0.24973\t\t-0.017\t-0.114\t0.00048398\n",
      "[(1, 1)] [1]\t0.21132\t\t0.16666\t\t-0.893\t-0.114\t0.01696632\n",
      "[(1, 1)] [2]\t0.1931\t\t0.15581\t\t0.148\t-0.114\t-0.00262883\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs\\tIter\\tActivation\\tDerivative\\tWeight\\tDelta\\tPrediction\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(len(hidden_layer)):\n",
    "    x,y = inputs[i]\n",
    "    arr = np.array(hidden_layer[i])\n",
    "    print(\"\")\n",
    "    for j in range(len(arr)):  \n",
    "        synapse = round(hidden_layer[i][j], 5)\n",
    "        sd_val = round(sigmoid_derivative(hidden_layer[i][j]), 5)\n",
    "        weight = float(weights1[j])\n",
    "        do = round(float(delta_output[i]), 3)\n",
    "        print(f\"[{x,y}] [{j}]\\t{synapse}\\t\\t{sd_val}\\t\\t{weight}\\t{do}\\t{round(float(sd_val * weight * do),8)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rationalise the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017],\n",
       "       [-0.893],\n",
       "       [ 0.148]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delta_output_weight_multiplier = delta_output.dot(weights1)\n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017, -0.893,  0.148]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the T denotes transposed, where cols become rows and rows become cols\n",
    "# this is basically a total reshaping allowing us to continue with the \n",
    "# weights array as a usable multiplier because if we attempt to calc\n",
    "# delta_output.dot(weights1) we have a shape mismatch. \n",
    "weights1T = weights1.T\n",
    "weights1T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights1     : (3, 1)\n",
      "Shape of weights1T    : (1, 3)\n",
      "Shape of delta_output : (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# check shapes \n",
    "print(f\"Shape of weights1     : {weights1.shape}\")\n",
    "print(f\"Shape of weights1T    : {weights1T.shape}\")\n",
    "print(f\"Shape of delta_output : {delta_output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0016639 ,  0.08740354, -0.01448569],\n",
       "       [-0.0023697 , -0.12447882,  0.02063031],\n",
       "       [-0.0023554 , -0.12372783,  0.02050584],\n",
       "       [ 0.00193282,  0.10153015, -0.01682694]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output_weight_multiplier = delta_output.dot(weights1T)\n",
    "delta_output_weight_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a reminder of our functions, for the purposes of not having to traverse back up the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of a sigmoid function. \n",
    "def sigmoid(sum):\n",
    "    return 1 / (1 + np.exp(-sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sigmoid_derivative(sigmoid):\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00041597,  0.02185088, -0.00362142],\n",
       "       [-0.00057384, -0.02866677,  0.00488404],\n",
       "       [-0.00056316, -0.02705587,  0.00410378],\n",
       "       [ 0.00048268,  0.01692128, -0.00262183]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_hidden_layer = delta_output_weight_multiplier * sigmoid_derivative(hidden_layer)\n",
    "delta_hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.04.05 - Backpropagation (adjusting the weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have worked on the `feed forward` principle by applying weights to the input layer that lead to hidden layer calculations. Then feeding those results forward until we have an output layer total and `prediction`. Overall the process works from `left to right`. Backproagation, on the other hand, is the reverse of this flow in the sense that we will recalculate the weights updates based on results, weights and activation_function results as the input. Then apply them from `right to left`. We will use the formula: $weight_{n + 1} = weight_{n} + (input \\cdot delta \\cdot learning\\_rate)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a neural network the user, or programmer, determines the value of the learning rate. This rate defines the speed of the algorithm or how fast it will learn. Where the learning rate is: \n",
    "- High : convergence is fast but the risk is to lose the global minimum. \n",
    "- Low : convergence is slow but the risk of losing the global minimum is greatly reduced. \n",
    "\n",
    "Note: convergence means the neural network has reached the best result, or global minimum. To maximise a neural network's capability with its efficiency and avoiding setting the learning rate so high that it loses the global minimum many libraries will implement a dynamic learning rate that reduces the rate as the number of epochs increases, so a learning rate starts off higher and reduces as it nears the global minimum which is deemed the best of both worlds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "The method for calculating a weight update is to: \n",
    "- Take the delta_output value by instance\n",
    "- Take the activation value from each hidden layer neuron by instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the learning rate as 0.3\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE ALONG SAMPLE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.017],\n",
       "       [-0.893],\n",
       "       [ 0.148]])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.5885562 , 0.35962319, 0.38485296],\n",
       "       [0.39555998, 0.32300414, 0.27667802],\n",
       "       [0.48350599, 0.21131785, 0.19309868]])"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0978763 ],\n",
       "       [ 0.13939397],\n",
       "       [ 0.138553  ],\n",
       "       [-0.11369557]])"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5885562 , 0.39555998, 0.48350599],\n",
       "       [0.5       , 0.35962319, 0.32300414, 0.21131785],\n",
       "       [0.5       , 0.38485296, 0.27667802, 0.19309868]])"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose the hidden layer so we can \n",
    "# work by neuron index across all instances\n",
    "# with the multiplier. \n",
    "hidden_layerT = hidden_layer.T\n",
    "hidden_layerT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03293657],\n",
       "       [0.02191844],\n",
       "       [0.02108814]])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the deltas needed for the updated weights \n",
    "# calculations to be done. \n",
    "input_delta1_multiplier = hidden_layerT.dot(delta_output)\n",
    "input_delta1_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00711903],\n",
       "       [-0.88642447],\n",
       "       [ 0.15432644]])"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the weights from the \n",
    "# hidden layer to the output layer \n",
    "weights1 = weights1 + (input_delta1_multiplier * learning_rate) \n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03293657],\n",
       "       [0.02191844],\n",
       "       [0.02108814]])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00041597,  0.02185088, -0.00362142],\n",
       "       [-0.00057384, -0.02866677,  0.00488404],\n",
       "       [-0.00056316, -0.02705587,  0.00410378],\n",
       "       [ 0.00048268,  0.01692128, -0.00262183]])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_hidden_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.10 - Implementation of multi-layer Perceptron with Python & numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5885562 , 0.39555998, 0.48350599],\n",
       "       [0.5       , 0.35962319, 0.32300414, 0.21131785],\n",
       "       [0.5       , 0.38485296, 0.27667802, 0.19309868]])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layerT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03293657],\n",
       "       [0.02191844],\n",
       "       [0.02108814]])"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_delta0_multiplier = hidden_layerT.dot(delta_output)\n",
    "input_delta0_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40588573],\n",
       "       [ 0.56812143],\n",
       "       [ 0.56321464],\n",
       "       [-0.45801216]])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.570628955817633"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partial derivative of the error \n",
    "\n",
    "sigmoid_derivative(-0.40588573)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "------------------------------------------------------------\n",
      "inputs:\n",
      " [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "\n",
      "weights0:\n",
      " [(-0.424, 0.358), (-0.74, -0.577), (-0.961, -0.469)]\n",
      "\n",
      "sum_synapse0:\n",
      " [[ 0.     0.     0.   ]\n",
      " [ 0.358 -0.577 -0.469]\n",
      " [-0.424 -0.74  -0.961]\n",
      " [-0.066 -1.317 -1.43 ]]\n",
      "\n",
      "hidden_layer:\n",
      " [[0.5        0.5        0.5       ]\n",
      " [0.5885562  0.35962319 0.38485296]\n",
      " [0.39555998 0.32300414 0.27667802]\n",
      " [0.48350599 0.21131785 0.19309868]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"inputs:\\n {inputs}\\n\")\n",
    "print(f\"weights0:\\n {weights0}\\n\")\n",
    "print(f\"sum_synapse0:\\n {sum_synapse0}\\n\")\n",
    "print(f\"hidden_layer:\\n {hidden_layer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
