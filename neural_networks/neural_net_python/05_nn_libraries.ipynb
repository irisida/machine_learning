{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network libraries with Python\n",
    "\n",
    "In this section we look at some of tge leading libraries in the ML/AI realm for a range of problem types.\n",
    "\n",
    "1. Scikit-learn (sklearn) for clasification and regression\n",
    "2. Tensorflow (image classification problem)\n",
    "3. Pytorch (cancers detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Iris data - Checking volumes and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Train & Test - dataset splitting\n",
    "\n",
    "It is entirely common practice to split an entire dataset into subsets for the purposes of training data and test data. The \n",
    "concept is that we test the model on data it has never seen before to determine a truer outcome than reprocessing data that was used to build the model with. \n",
    "\n",
    "We can define the split ratios but the idea is to create a good (_or best_) balance, where good is considered to be the most training data we can afford while keeping a reasonable test set. In reality this changes from project to project typically hovering in the 80/20 ratio for train/test and some in the 70/30 train/test regions.\n",
    "\n",
    "**note:** It is also common practice to refer to `x` as the inputs and `y` as the expected results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider the whole dataset as 1, therefore defining the test_size\n",
    "# split at 0.2 equates to 20% or as noted above adhereing to the 80/20\n",
    "# split mechanism that is fairly ubiquitous. \n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              x_train         x_test        y_train         x_test\n",
      "--------------------------------------------------------------------------------\n",
      "Shape:       (120, 4)        (30, 4)         (120,)        (30, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'x_train':>21}{'x_test':>15}{'y_train':>15}{'x_test':>15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Shape:{str(x_train.shape):>15}{str(x_test.shape):>15}{str(y_train.shape):>15}{str(x_test.shape):>15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Network training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.09795306\n",
      "Iteration 2, loss = 1.09794742\n",
      "Iteration 3, loss = 1.09794509\n",
      "Iteration 4, loss = 1.09794466\n",
      "Iteration 5, loss = 1.09794231\n",
      "Iteration 6, loss = 1.09794144\n",
      "Iteration 7, loss = 1.09793999\n",
      "Iteration 8, loss = 1.09793896\n",
      "Iteration 9, loss = 1.09793732\n",
      "Iteration 10, loss = 1.09793585\n",
      "Iteration 11, loss = 1.09793555\n",
      "Iteration 12, loss = 1.09793291\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', batch_size=32,\n",
       "              hidden_layer_sizes=(4, 4, 4), learning_rate_init=1e-05,\n",
       "              max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = MLPClassifier(max_iter=1000, \n",
    "                        tol=0.0001, \n",
    "                        activation='logistic', \n",
    "                        solver='adam', \n",
    "                        learning_rate='constant', \n",
    "                        learning_rate_init=0.00001, \n",
    "                        batch_size=32, \n",
    "                        hidden_layer_sizes=(4,4, 4),\n",
    "                        verbose=True)\n",
    "\n",
    "network.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class labels for each output. \n",
    "network.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.35229876, -0.47590459, -0.31507162,  0.40339467],\n",
       "        [ 0.31507802,  0.19751672, -0.0434016 , -0.32454148],\n",
       "        [-0.36927525, -0.30456062, -0.25231454, -0.41259875],\n",
       "        [ 0.33916471, -0.49051799,  0.48527459, -0.34196168]]),\n",
       " array([[ 1.59550068e-01,  4.13056562e-01,  6.75858045e-02,\n",
       "          1.18349535e-01],\n",
       "        [-1.28788351e-01, -4.04301606e-01, -7.60321011e-02,\n",
       "         -4.21431628e-02],\n",
       "        [ 6.68200878e-02, -4.15220112e-01,  4.04244887e-01,\n",
       "         -2.66013521e-04],\n",
       "        [-2.56212758e-01, -2.72813476e-01, -4.32808838e-01,\n",
       "         -2.56135097e-01]]),\n",
       " array([[-0.19334884,  0.43321495, -0.01778518, -0.21422771],\n",
       "        [ 0.35031876, -0.35129638, -0.24517347, -0.29922435],\n",
       "        [ 0.35856304, -0.25681436, -0.13456972,  0.02445175],\n",
       "        [ 0.3102562 , -0.26872211, -0.31788469,  0.17129958]]),\n",
       " array([[-0.49522655,  0.10505073,  0.27288448],\n",
       "        [ 0.51090853, -0.32088139, -0.02799498],\n",
       "        [ 0.25068257,  0.26434059,  0.13232405],\n",
       "        [ 0.44547286,  0.1722969 ,  0.53405841]])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.4 Network evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykit",
   "language": "python",
   "name": "pykit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
